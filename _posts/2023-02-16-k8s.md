# 一：Kubernetes简介：

kubernetges最初源于谷歌内部的Borg,Borg是谷歌内部的大规模集群管理系统，负载对谷歌内部很多核心服务的调度和管理，Borg的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化

Borg主要由BorgMaster、Borglft、Bprgcfg和Scheduler组成

**功能特性：**

• 自动化容器部署与复制

• 随时扩展或收缩容器规模

• 组织容器成组，提供容器间的负载均衡

• 快速更新及回滚容器版本

• 提供弹性伸缩，如果某个容器失效就进行替换

https://kubernetes.io/zh/#官网

https://github.com/kubernetes/kubernetes #github

https://landscape.cncf.io/   CNCF网址

**架构图**



![image-20220703173621149](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220703173621149.png)





## 1.1：kubernetes组件简介

kube-apiserver

https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver

Kubernetes API server提供了K8s各类的资源对象的增删改查及HTTP Rest接口，这些对象包括pods、services、replicationcontrollers等、API Server为REST操作提供服务，并为集群的共享状态提供前端，所有其他组件都通过前端进行交互

RESTful API

是REST风格的网络接口，REST描述的是在网络中client和server的一种交互形式



![image-20220603151807859](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603151807859.png)

### 1.1.1：kubernetes API Server简介

* 改端口默认值为6443，可通过启动参数--secure-port的值来修改默认值
* 默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数--bind-address设置该值
* 改端口用于接收客户端、dashboard等外部HTTPS请求
* 用于基于Tocken文件活客户端证书及HTTP Base的认证
* 用于基于策略的授权

### 1.1.2: kube-scheduler

https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-scheduler

* kubernetes调度器是一个控制面进程，负责将Pods指派到节点上
* 通过调度算法为待调度Pod列表的每个Pod从可用Node列表中选择一个最适合的Node，并将信息写入etcd中
* node节点上的kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载image，并启动容器
* 策略
* LeastRequestedPriority
* 优先从备选节点列表中选择资源消耗最小的节点（CPU+内存）
* CalculateNodelabelPriority
* 优先选择含有指定Label的节点
* BalancedResourceALLocation
* 优先从备选节点列表中选择各项资源使用率最均衡的节点

第一步创建POD

![image-20220603153651958](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603153651958.png)

第二部：过滤掉资源不足的节点

![image-20220603153743355](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603153743355.png)



第三步：在剩余可用的节点中进行删选

![image-20220603153816590](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603153816590.png)



第四步：选中节点

![image-20220603153852907](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603153852907.png)



![image-20220603153939801](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603153939801.png)

### 1.1.3：pod

Pod是k8s进行资源调度的最小单位，每个Pod中运行着一个或多个密切相关的业务容器，这些业务容器共享这个Pause容器的IP和Volume，我们以这个不易死亡的Pause容器作为Pod的根容器，以它的状态表示整个容器组的状态。一个Pod一旦被创建就会放到Etcd中存储，然后由Master调度到一个Node绑定，由这个Node上的Kubelet进行实例化。

每个Pod会被分配一个单独的Pod IP，Pod IP + ContainerPort 组成了一个Endpoint



### 1.1.4：Service

service其功能使应用暴露，Pods 是有生命周期的，也有独立的 IP 地址，随着 Pods 的创建与销毁，一个必不可少的工作就是保证各个应用能够感知这种变化。这就要提到 Service 了，Service 是 YAML 或 JSON 定义的由 Pods 通过某种策略的逻辑组合。更重要的是，Pods 的独立 IP 需要通过 Service 暴露到网络中。

![image-20220703173802123](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220703173802123.png)

### 1.1.5: master

Master节点上面主要由四个模块组成：APIServer、scheduler、controller manager、etcd

• APIServer:APIServer 负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。

• schedule:scheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。

• controller manager:如果说APIServer做的是“前台”的工作的话，那controller manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller manager去保证了。



![image-20220703173937391](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220703173937391.png)

### 1.1.6: etcd

etcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。

### 1.1.7: Node

每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。

runtime指的是容器运行环境，目前Kubernetes支持docker和rkt两种容器。

• kube-proxy:该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。

• kubelet:Kubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上面的所有容器，但是如果容器不是通过Kubernetes创建的，它并不会管理。本质上，它负责使Pod得运行状态与期望的状态一致。

etcd:etcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。





## 1.2：基础集群环境搭建

k8s基础集群环境主要运行kubernetes管理端服务以及node节点上的服务部署及使用。

Kubernetes设计架构：

```
https://www.kubernetes.org.cn/kubernetes%E8%AE%BE%E8%AE%A1%E6%9E%B6%E6%9E%84
```

CNCF云原生容器生态系统概要：

```
http://dockone.io/article/3006
```

### 1.2.1：k8s高可用集群环境规划信息：

安装实际需求，进行规划于部署相应的单master或者多master的高可用k8s运行环境

### 1.2.2：单master

见kubeadm安装k8s

![](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220207223801510.png)

### 1.2.3：多master:

![image-20220207223859815](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220207223859815.png)

### 1.2.4: 服务器统计：

| 类型            | 服务器IP地址       | 备注                               |
| --------------- | ------------------ | ---------------------------------- |
| K8S Master(2台) | 192.168.48.158/159 | k8s控制端，通过一个VIP做主备高可用 |
| Ansible(2台)    | 192.168.48.158/159 | k8s集群部署服务器                  |
| Etcd（2台）     | 192.168.48.158/159 | 保存k8s集群数据的服务器            |
| Node节点（2台） | 192.168.48.160/161 | 真正运行容器的服务器，高可用环境   |
| Hproxy(2台)     | 192.168.48.158/159 | 高可用etcd代理服务器               |
| Harbor（1台)    | 192.168.48.141     | 高可用镜像服务器                   |



### 1.2.5：主机名设置：



| 类型        | 服务器IP       | 主机名      | VIP           |
| ----------- | -------------- | ----------- | ------------- |
| K8S Master1 | 192.168.48.158 | k8s-master1 | 192.168.48.88 |
| K8S Master2 | 192.168.48.159 | k8s-master2 | 192.168.48.88 |
| Habor1      | 192.158.48.141 | k8s-harbor1 |               |
| etcd节点1   | 192.168.48.158 | k8s-etcd1   |               |
| etcd节点2   | 192.168.48.159 | k8s-etcd2   |               |
| Haproxy1    | 192.168.48.158 | k8s-ha1     |               |
| Haproxy2    | 192.168.48.159 | k8s-ha2     |               |
| Node节点1   | 192.168.48.160 | k8s-node1   |               |
| Node节点2   | 192.168.48.161 | k8s-node2   |               |

### 1.2.6: 软件清单：

见当前目录下的kubernetes软件清单

API端口：

```bash
端口：192.168.48.88：6443 #需要配置在负载均衡上实现反向代理，dashboard的端口为8443
操作系统：Ubuntu server 1804
k8s版本：1.13.5
calico: 3.4.4
```

### 1.2.7:基础环境准备

http://releases.ubuntu.com/

系统主机名配置、IP配置、系统参数优化，以及依赖的负载均衡和Harbor部署，主机名登系统配置



### 1.2.8：高可用负载均衡

k8s高可用反向代理

http://blogs.studylinux.net/?p=4579



#### 1.2.8.1：keepalived



```bash
vrrp_instance VI_1 {
	state MASTER
	interface ens33
	virtual_router_id 1
	priority 100
	advert_int 1
	authentication {
		auth_type PASS
		auth_pass 123
}
	virtual_ipaddress {
		192.168.48.99 dev ens33 label ens33:1
}	

}

```

#### 1.2.8.2：haproxy

```bash
maxconn	100000
mode	http
timeout	connect	5s
timeout	client	300s
timeout	server	30s
timeout	check	3s
listen	stats
mode	http
bind	0.0.0.0:9999
stats	enable
log	global
stats	uri	/haproxy-status
stats	auth	admin:admin
listen k8s-api_nodes_6443
        bind 192.168.48.99:6443
        mode tcp
        server 192.168.48.166 192.168.48.166:6443 check inter 2000 fall 3 rise 5
        server 192.168.48.163 192.168.48.163:6443 check inter 2000 fall 3 rise 5


BACKUO没有监听需要修改参数
root@k8s-master2:cat /etc/sysctl.conf
root@k8s-master2:net.ipv4.ip_nonlocal_bind = 1
root@k8s-master2:sysctl -p
root@k8s-master2:systemctl daemon-reload
root@k8s-master2:systemctl restart haproxy
```

## 1.3: Harbor之https

内部镜像将统一保存内部harbor服务器，不在通过互联网在线下载

### 1.3.1：安装harbor

```bash
[root@k8s-habor apps]# tar xvf docker-19.03.15-binary-install.tar.gz
[root@k8s-habor apps]# ./docker-install.sh
[root@k8s-habor apps]# ls
harbor-offline-installer-v2.3.2.tgz

#解压
[root@k8s-habor apps]# tar xvf harbor-offline-installer-v2.3.2.tgz
[root@k8s-habor harbor]# mkdir certs
[root@k8s-habor harbor]# cd certs/
#生成私有key
[root@k8s-habor certs]# openssl genrsa -out harbor-ca.key
Generating RSA private key, 2048 bit long modulus
..............+++
...+++
e is 65537 (0x10001)
#签发证书
[root@k8s-habor certs]# openssl req -x509 -new -nodes -key harbor-ca.key -subj "/CN=harbor.magedu.net" -days 7120 -out harbor-ca.crt

[root@k8s-habor certs]# cd ../
[root@k8s-habor harbor]# cp harbor.yml.tmpl  harbor.yml
[root@k8s-habor harbor]# vim harbor.yml
[root@k8s-habor harbor]# grep -v "#" harbor.yml | grep -v "^$"
hostname: harbor.magedu.net
http:
  port: 80
https:
  port: 443
  certificate: /apps/harbor/certs/harbor-ca.crt
  private_key: /apps/harbor/certs/harbor-ca.key
harbor_admin_password: 123456
database:
  password: root123
  max_idle_conns: 100
  max_open_conns: 900
data_volume: /data

[root@k8s-habor harbor]# ./install.sh --with-trivy

```

### 1.3.2: 测试登录harbor



```bash
1.添加域名解析
[root@k8s-habor harbor]# cat /etc/hosts
192.168.48.167 harbor.magedu.net
2.添加信任仓库
[root@k8s-habor harbor]#cat /lib/systemd/system/docker.service 
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.io   #修改
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --insecure-registry harbor.magedu.net
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

# cat /etc/docker/daemon.conf

{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn",
    "http://hub-mirror.c.163.com"
  ],
  "max-concurrent-downloads": 10,
  "log-driver": "json-file",
  "log-level": "warn",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
    },
  "data-root": "/var/lib/docker",
  "insecure-registries":["https://harbor.magedu.net"]
}



[root@k8s-habor harbor]# systemctl daemon-reload
[root@k8s-habor harbor]# systemctl restart docker
[root@k8s-habor harbor]# docker login harbor.magedu.net
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded



```

#### 1.3.3:  测试push镜像到harbor：

```bash
[root@k8s-habor harbor]# docker pull alpine
[root@k8s-habor harbor]# docker tag alpine harbor.magedu.net/library/alpine
[root@k8s-habor harbor]# docker push harbor.magedu.net/library/alpine
The push refers to repository [harbor.magedu.net/library/alpine]
8d3ac3489996: Pushed 
latest: digest: sha256:e7d88de73db3d3fd9b2d63aa7f447a10fd0220b7cbf39803c803f2af9ba256b3 size: 528
```



## 1.4：ansible部署：

### 1.4.1：基础环境准备：

```bash
root@k8s-master1:~# apt-get install git python3-pip -y
root@k8s-master1:~#pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn  ansible
root@k8s-master1:~# ansible --version


#生成密钥
root@k8s-master1:~# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been sad in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:LWaZ9kyDcjELyVI2aMqA3+dGlKq0pOgLf7fTpKoIuLA root@k8s-master1
The key's randomart image is:
+---[RSA 2048]----+
|.    .+.         |
|o   o+oo         |
| + +.o+ o        |
|  * o.o. O       |
|.+ o +. S +      |
|+ o   o*.= .     |
|*    . +  o      |
|+*  . + .        |
|E.+o.o.o         |
+----[SHA256]-----+

#设置免密登录
#修改ssh配置文件需要root远程登录权限
vim /etc/ssh/sshd_config
找到PermitRootLogin without-password 
修改为PermitRootLogin yes
root@k8s-master1:~# systemctl restart ssh
#安装sshpass命令用于同步公钥到各k8s服务器
root@k8s-master1:~# apt-get install sshpass 
root@k8s-master1:~# cat scp-key.sh
#!/bin/bash
IP="192.168.48.166
192.168.48.163
192.168.48.164
192.168.48.165
192.168.48.167
"
for node in ${IP};do
	sshpass -p 111111 ssh-copy-id ${node} -o StrictHostKeyChecking=no
	if [ $? -eq 0 ];then
		echo "${node} 密钥copy完成"
	else
		echo "${node} 密钥copy失败"
	fi
done
root@k8s-master1:~# bash scp-key.sh

```

### 1.4.1: 部署节点下载部署项目及组件：

实验master1做为部署节点

```bash
root@k8s-master1:~# export release=3.3.4
root@k8s-master1:~# wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown
root@k8s-master1:~# chmod a+x ezdown 
root@k8s-master1:~# vim ezdown
DOCKER_VER=19.03.15
KUBEASZ_VER=3.1.1
K8S_BIN_VER=v1.24.2
root@k8s-master1:~# ./ezdown -D

root@k8s-master1:~# ll /etc/kubeasz/down/
total 1211692
drwxr-xr-x  2 root root      4096 2月   9 22:04 ./
drwxrwxr-x 11 root root      4096 2月   9 21:55 ../
-rw-------  1 root root 399721984 2月   9 21:58 calico_v3.19.2.tar
-rw-------  1 root root  47692288 2月   9 21:59 coredns_1.8.4.tar
-rw-------  1 root root 223074304 2月   9 22:01 dashboard_v2.3.1.tar
-rw-r--r--  1 root root  62436240 2月   2  2021 docker-19.03.15.tgz
-rw-------  1 root root  58150912 2月   9 22:02 flannel_v0.13.0-amd64.tar
-rw-------  1 root root 124833792 2月   9 22:00 k8s-dns-node-cache_1.17.0.tar
-rw-------  1 root root 179037696 2月   9 22:04 kubeasz_3.1.1.tar
-rw-------  1 root root  34566656 2月   9 22:02 metrics-scraper_v1.0.6.tar
-rw-------  1 root root  64775168 2月   9 22:03 metrics-server_v0.5.0.tar
-rw-------  1 root root  45063680 2月   9 22:03 nfs-provisioner_v4.0.1.tar
-rw-------  1 root root    692736 2月   9 22:03 pause_3.5.tar
-rw-------  1 root root    692736 2月   9 22:03 pause.tar
```

### 1.4.2: 生成ansible hosts文件：

```bash
root@k8s-master1:/etc/kubeasz# cd /etc/kubeasz/
root@k8s-master1:/etc/kubeasz# ./ezctl new k8s-01
2022-02-09 22:05:33 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01
2022-02-09 22:05:33 DEBUG set version of common plugins
2022-02-09 22:05:33 DEBUG cluster k8s-01: files successfully created.
2022-02-09 22:05:33 INFO next steps 1: to config '/etc/kubeasz/clusters/k8s-01/hosts'
2022-02-09 22:05:33 INFO next steps 2: to config '/etc/kubeasz/clusters/k8s-01/config.yml'

```

#### 1.4.2.1: 编辑ansible hosts文件：

指定etcd节点、master节点、node节点、VIP、网络组件类型、service IP与Pod IP范围等配置信息

```bash
root@k8s-master1:/etc/kubeasz# cat /etc/kubeasz/clusters/k8s-01/hosts 
# 'etcd' cluster should have odd member(s) (1,3,5,...)
[etcd]
192.168.1.153
192.168.1.155

# master node(s), set unique 'k8s_nodename' for each node
# CAUTION: 'k8s_nodename' must consist of lower case alphanumeric characters, '-' or '.',
# and must start and end with an alphanumeric character
[kube_master]
192.168.1.153 k8s_nodename='master-01'
192.168.1.155 k8s_nodename='master-02'

# work node(s), set unique 'k8s_nodename' for each node
# CAUTION: 'k8s_nodename' must consist of lower case alphanumeric characters, '-' or '.',
# and must start and end with an alphanumeric character
[kube_node]
192.168.1.156 k8s_nodename='node-01'
192.168.1.140 k8s_nodename='node-02'

# [optional] harbor server, a private docker registry
# 'NEW_INSTALL': 'true' to install a harbor server; 'false' to integrate with existed one
[harbor]
#192.168.1.8 NEW_INSTALL=false

# [optional] loadbalance for accessing k8s from outside
[ex_lb]
#192.168.1.6 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443
#192.168.1.7 LB_ROLE=master EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443

# [optional] ntp server for the cluster
[chrony]
#192.168.1.1

[all:vars]
# --------- Main Variables ---------------
# Secure port for apiservers
SECURE_PORT="6443"

# Cluster container-runtime supported: docker, containerd
# if k8s version >= 1.24, docker is not supported
CONTAINER_RUNTIME="containerd"

# Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn
CLUSTER_NETWORK="calico"

# Service proxy mode of kube-proxy: 'iptables' or 'ipvs'
PROXY_MODE="ipvs"

# K8S Service CIDR, not overlap with node(host) networking
SERVICE_CIDR="10.100.0.0/16"

# Cluster CIDR (Pod CIDR), not overlap with node(host) networking
CLUSTER_CIDR="10.200.0.0/16"

# NodePort Range
NODE_PORT_RANGE="30000-34767"

# Cluster DNS Domain
CLUSTER_DNS_DOMAIN="cluster.local"

# -------- Additional Variables (don't change the default value right now) ---
# Binaries Directory
bin_dir="/usr/bin"

# Deploy Directory (kubeasz workspace)
base_dir="/etc/kubeasz"

# Directory for a specific cluster
cluster_dir="{{ base_dir }}/clusters/k8s-01"

# CA and other components cert/key Directory
ca_dir="/etc/kubernetes/ssl"

# Default 'k8s_nodename' is empty
k8s_nodename=''

```

#### 1.4.2.2：编辑config.yml文件：

```bash
root@k8s-master1:/etc/kubeasz# cat /etc/kubeasz/clusters/k8s-01/config.yml 
############################
# prepare
############################
# 可选离线安装系统软件包 (offline|online)
INSTALL_SOURCE: "online"

# 可选进行系统安全加固 github.com/dev-sec/ansible-collection-hardening
OS_HARDEN: false


############################
# role:deploy
############################
# default: ca will expire in 100 years
# default: certs issued by the ca will expire in 50 years
CA_EXPIRY: "876000h"
CERT_EXPIRY: "438000h"

# force to recreate CA and other certs, not suggested to set 'true'
CHANGE_CA: false

# kubeconfig 配置参数
CLUSTER_NAME: "cluster1"
CONTEXT_NAME: "context-{{ CLUSTER_NAME }}"

# k8s version
K8S_VER: "1.24.10"

# set unique 'k8s_nodename' for each node, if not set(default:'') ip add will be used
# CAUTION: 'k8s_nodename' must consist of lower case alphanumeric characters, '-' or '.',
# and must start and end with an alphanumeric character (e.g. 'example.com'),
# regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'
K8S_NODENAME: "{%- if k8s_nodename != '' -%} \
                    {{ k8s_nodename|replace('_', '-')|lower }} \
               {%- else -%} \
                    {{ inventory_hostname }} \
               {%- endif -%}"

############################
# role:etcd
############################
# 设置不同的wal目录，可以避免磁盘io竞争，提高性能
ETCD_DATA_DIR: "/var/lib/etcd"
ETCD_WAL_DIR: ""


############################
# role:runtime [containerd,docker]
############################
# ------------------------------------------- containerd
# [.]启用容器仓库镜像
ENABLE_MIRROR_REGISTRY: true

# [containerd]基础容器镜像
SANDBOX_IMAGE: "harbor.aeotrade.net/base/pauser:3.9"

# [containerd]容器持久化存储目录
CONTAINERD_STORAGE_DIR: "/var/lib/containerd"

# ------------------------------------------- docker
# [docker]容器存储目录
DOCKER_STORAGE_DIR: "/var/lib/docker"

# [docker]开启Restful API
ENABLE_REMOTE_API: false

# [docker]信任的HTTP仓库
INSECURE_REG: '["http://easzlab.io.local:5000"]'


############################
# role:kube-master
############################
# k8s 集群 master 节点证书配置，可以添加多个ip和域名（比如增加公网ip和域名）
MASTER_CERT_HOSTS:
  - "192.168.1.88"
  - "www.aeotrade.net"
  #- "www.test.com"

# node 节点上 pod 网段掩码长度（决定每个节点最多能分配的pod ip地址）
# 如果flannel 使用 --kube-subnet-mgr 参数，那么它将读取该设置为每个节点分配pod网段
# https://github.com/coreos/flannel/issues/847
NODE_CIDR_LEN: 24


############################
# role:kube-node
############################
# Kubelet 根目录
KUBELET_ROOT_DIR: "/var/lib/kubelet"

# node节点最大pod 数
MAX_PODS: 110

# 配置为kube组件（kubelet,kube-proxy,dockerd等）预留的资源量
# 数值设置详见templates/kubelet-config.yaml.j2
KUBE_RESERVED_ENABLED: "no"

# k8s 官方不建议草率开启 system-reserved, 除非你基于长期监控，了解系统的资源占用状况；
# 并且随着系统运行时间，需要适当增加资源预留，数值设置详见templates/kubelet-config.yaml.j2
# 系统预留设置基于 4c/8g 虚机，最小化安装系统服务，如果使用高性能物理机可以适当增加预留
# 另外，集群安装时候apiserver等资源占用会短时较大，建议至少预留1g内存
SYS_RESERVED_ENABLED: "no"


############################
# role:network [flannel,calico,cilium,kube-ovn,kube-router]
############################
# ------------------------------------------- flannel
# [flannel]设置flannel 后端"host-gw","vxlan"等
FLANNEL_BACKEND: "vxlan"
DIRECT_ROUTING: false

# [flannel] 
flannel_ver: "v0.19.2"

# ------------------------------------------- calico
# [calico] IPIP隧道模式可选项有: [Always, CrossSubnet, Never],跨子网可以配置为Always与CrossSubnet(公有云建议使用always比较省事，其他的话需要修改各自公有云的网络配置，具体可以参考各个公有云说明)
# 其次CrossSubnet为隧道+BGP路由混合模式可以提升网络性能，同子网配置为Never即可.
CALICO_IPV4POOL_IPIP: "Always"

# [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现
IP_AUTODETECTION_METHOD: "can-reach={{ groups['kube_master'][0] }}"

# [calico]设置calico 网络 backend: brid, vxlan, none
CALICO_NETWORKING_BACKEND: "brid"

# [calico]设置calico 是否使用route reflectors
# 如果集群规模超过50个节点，建议启用该特性
CALICO_RR_ENABLED: false

# CALICO_RR_NODES 配置route reflectors的节点，如果未设置默认使用集群master节点 
# CALICO_RR_NODES: ["192.168.1.1", "192.168.1.2"]
CALICO_RR_NODES: []

# [calico]更新支持calico 版本: ["3.19", "3.23"]
calico_ver: "v3.24.5"

# [calico]calico 主版本
calico_ver_main: "{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}"

# ------------------------------------------- cilium
# [cilium]镜像版本
cilium_ver: "1.12.4"
cilium_connectivity_check: true
cilium_hubble_enabled: false
cilium_hubble_ui_enabled: false

# ------------------------------------------- kube-ovn
# [kube-ovn]选择 OVN DB and OVN Control Plane 节点，默认为第一个master节点
OVN_DB_NODE: "{{ groups['kube_master'][0] }}"

# [kube-ovn]离线镜像tar包
kube_ovn_ver: "v1.5.3"

# ------------------------------------------- kube-router
# [kube-router]公有云上存在限制，一般需要始终开启 ipinip；自有环境可以设置为 "subnet"
OVERLAY_TYPE: "full"

# [kube-router]NetworkPolicy 支持开关
FIREWALL_ENABLE: true

# [kube-router]kube-router 镜像版本
kube_router_ver: "v0.3.1"
busybox_ver: "1.28.4"


############################
# role:cluster-addon
############################
# coredns 自动安装
dns_install: "no"
corednsVer: "1.9.3"
ENABLE_LOCAL_DNS_CACHE: true
dnsNodeCacheVer: "1.22.13"
# 设置 local dns cache 地址
LOCAL_DNS_CACHE: "10.100.0.2"

# metric server 自动安装
metricsserver_install: "no"
metricsVer: "v0.5.2"

# dashboard 自动安装
dashboard_install: "no"
dashboardVer: "v2.7.0"
dashboardMetricsScraperVer: "v1.0.8"

# prometheus 自动安装
prom_install: "no"
prom_namespace: "monitor"
prom_chart_ver: "39.11.0"

# nfs-provisioner 自动安装
nfs_provisioner_install: "no"
nfs_provisioner_namespace: "kube-system"
nfs_provisioner_ver: "v4.0.2"
nfs_storage_class: "managed-nfs-storage"
nfs_server: "192.168.1.10"
nfs_path: "/data/nfs"

# network-check 自动安装
network_check_enabled: false 
network_check_schedule: "*/5 * * * *"

############################
# role:harbor
############################
# harbor version，完整版本号
HARBOR_VER: "v2.6.3"
HARBOR_DOMAIN: "harbor.easzlab.io.local"
HARBOR_PATH: /var/data
HARBOR_TLS_PORT: 8443
HARBOR_REGISTRY: "{{ HARBOR_DOMAIN }}:{{ HARBOR_TLS_PORT }}"

# if set 'false', you need to put certs named harbor.pem and harbor-key.pem in directory 'down'
HARBOR_SELF_SIGNED_CERT: true

# install extra component
HARBOR_WITH_NOTARY: false
HARBOR_WITH_TRIVY: false
HARBOR_WITH_CHARTMUSEUM: true


```

### 1.4.2:  部署k8s集群

通过ansible脚本初始化环境及部署K8s高可用集群

#### 1.4.2.1：步骤1-基础环境初始化

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl  setup help
Usage: ezctl setup <cluster> <step>
available steps:
    01  prepare            to prepare CA/certs & kubeconfig & other system settings 
    02  etcd               to setup the etcd cluster
    03  container-runtime  to setup the container runtime(docker or containerd)
    04  kube-master        to setup the master nodes
    05  kube-node          to setup the worker nodes
    06  network            to setup the network plugin
    07  cluster-addon      to setup other useful plugins
    90  all                to run 01~07 all at once
    10  ex-lb              to install external loadbalance for accessing k8s from outside
    11  harbor             to install a new harbor server or to integrate with an existed one

examples: ./ezctl setup test-k8s 01  (or ./ezctl setup test-k8s prepare)
	  ./ezctl setup test-k8s 02  (or ./ezctl setup test-k8s etcd)
          ./ezctl setup test-k8s all
          ./ezctl setup test-k8s 04 -t restart_master
root@k8s-master1:/etc/kubeasz# vim playbooks/01.prepare.yml #系统基础初始化主机配置
root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 01 #准备CA和基础系统设置

```

#### 1.4.2.2：步骤2-部署etcd集群：

可更改启动脚本路径及版本等自定义配置

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 02 #部署etcd集群
```

验证各etcd节点服务状态

```bash
root@k8s-master1:/etc/kubeasz# systemctl status etcd
```

#### 1.4.2.3：部署docker:

如果是containerd:  使用私有的仓库需要修改地址，不是无需修改

```bash
# pwd
/etc/kubeasz/roles/containerd/templates
# vim config.toml.j2
147行
...
       [plugins."io.containerd.grpc.v1.cri".registry.mirrors."harbor.aeotrade.net"]
          endpoint = ["https://harbor.aeotrade.net"]
        [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.aeotrade.net".tls]
          insecure_skip_verify = true
        [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.aeotrade.net".auth]
          username = "admin"
          password = "123456"

```



```bash
root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 03  #部署containerd
```

#### 1.4.2.4: 同步harbor客户端正式：

```bash
#在每个服务器上都创建目录
root@k8s-node1:~# mkdir /etc/docker/certs.d/harbor.magedu.net -p
[root@k8s-habor certs]# pwd
/apps/harbor/certs
[root@k8s-habor certs]# cp harbor-ca.crt /etc/docker/certs.d/harbor.magedu.net/
[root@k8s-habor certs]# scp /etc/docker/certs.d/harbor.magedu.net/harbor-ca.crt 192.168.48.164:/etc/docker/certs.d/harbor.magedu.net/harbor-ca.crt
....
....
....
#每个服务器上解析harbor
192.168.48.167 harbor.magedu.net >> /etc/hosts

#master测试登录harbor出现访问不了
root@k8s-master1:~#/etc/kubeasz# docker login harbor.magedu.net
Username: admin
Password: 
Error response from daemon: Get "https://harbor.lcy.net/v2/": dial tcp 192.168.48.164:443: connect: connection refused
root@s1:/etc/kubeasz# vim /etc/docker/daemon.json
.....
"insecure-registries":["harbor.magedu.net"]
....
root@k8s-master1:~#/etc/kubeasz# systemctl daemon-reload
root@k8s-master1:~#/etc/kubeasz# systemctl restart docker
root@k8s-master1:/etc/kubeasz# docker login harbor.lcy.net
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
```

#### 1.4.2.5: 部署master节点：

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 04

#验证服务器
root@k8s-master1:/etc/kubeasz# kubectl get node
NAME             STATUS                     ROLES    AGE     VERSION
192.168.48.163   Ready,SchedulingDisabled   master   7m35s   v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   7m33s   v1.22.2

```

#### 1.4.2.6： 部署node节点：

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 05

#验证服务器

root@k8s-master1:/etc/kubeasz# kubectl get node
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.163   Ready,SchedulingDisabled   master   11m   v1.22.2
192.168.48.164   Ready                      node     38s   v1.22.2
192.168.48.165   Ready                      node     39s   v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   11m   v1.22.2

```

#### 1.4.2.7: 部署网络服务

网络组件可以使用calico或者flannel

##### 1.4.2.7.1: 使用calico网络组件：

```bash
root@k8s-master1:/etc/kubeasz# vim clusters/k8s-01/config.yml
# ------------------------------------------- calico
# [calico]设置 CALICO_IPV4POOL_IPIP=“off”,可以提高网络性能，条件限制详见 docs/setup/calico.md
CALICO_IPV4POOL_IPIP: "Always"

# [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现
IP_AUTODETECTION_METHOD: "can-reach={{ groups['kube_master'][0] }}"

# [calico]设置calico 网络 backend: brid, vxlan, none
CALICO_NETWORKING_BACKEND: "brid"

# [calico]更新支持calico 版本: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x]
calico_ver: "v3.19.2"

root@k8s-master1:/etc/kubeasz# grep image roles/calico/templates/calico-v3.15.yaml.j2 
          image: calico/cni:v3.15.3
          image: calico/pod2daemon-flexvol:v3.15.3
          image: calico/node:v3.15.3
          image: calico/kube-controllers:v3.15.3
          
          root@k8s-master1:/etc/kubeasz# docker login harbor.magedu.net
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

root@k8s-master1:/etc/kubeasz# docker pull calico/cni:v3.15.3
root@k8s-master1:/etc/kubeasz# docker tag calico/cni:v3.15.3 harbor.magedu.net/baseimages/calico-cni:v3.15.3
root@k8s-master1:/etc/kubeasz# docker push harbor.magedu.net/baseimages/calico-cni:v3.15.3

root@k8s-master1:/etc/kubeasz# docker pull calico/pod2daemon-flexvol:v3.15.3
root@k8s-master1:/etc/kubeasz#docker tag calico/pod2daemon-flexvol:v3.15.3 harbor.magedu.net/baseimages/calico-pod2daemon-flexvol:v3.15.3
root@k8s-master1:/etc/kubeasz# docker push harbor.magedu.net/baseimages/calico-pod2daemon-flexvol:v3.15.3

root@k8s-master1:/etc/kubeasz# docker pull calico/node:v3.15.3
root@k8s-master1:/etc/kubeasz# docker tag calico/node:v3.15.3 harbor.magedu.net/baseimages/calico-node:v3.15.3
root@k8s-master1:/etc/kubeasz# docker push harbor.magedu.net/baseimages/calico-node:v3.15.3

root@k8s-master1:/etc/kubeasz# docker pull calico/kube-controllers:v3.15.3
root@k8s-master1:/etc/kubeasz# docker tag calico/kube-controllers:v3.15.3 harbor.magedu.net/baseimages/calico-kube-controllers:v3.15.3
root@k8s-master1:/etc/kubeasz# docker push harbor.magedu.net/baseimages/calico-kube-controllers:v3.15.3

#修改镜像地址：
root@k8s-master1:/etc/kubeasz#  vim roles/calico/templates/calico-v3.15.yaml.j2

root@k8s-master1:/etc/kubeasz# grep image roles/calico/templates/calico-v3.15.yaml.j2 
          image: harbor.magedu.net/baseimages/calico-cni:v3.15.3
          image: harbor.magedu.net/baseimages/calico-pod2daemon-flexvol:v3.15.3
          image: harbor.magedu.net/baseimages/calico-node:v3.15.3 
          image: harbor.magedu.net/baseimages/calico-kube-controllers:v3.15.3


root@k8s-master1:/etc/kubeasz# ./ezctl setup k8s-01 06
```

#### 1.4.2.7.2: 验证网络组件calico状态:

```bash
root@k8s-master1:/etc/kubeasz# calicoctl node status
Calico process is running.

IPv4 BGP status
+----------------+-------------------+-------+----------+-------------+
|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+----------------+-------------------+-------+----------+-------------+
| 192.168.48.163 | node-to-node mesh | up    | 09:41:41 | Established |
| 192.168.48.165 | node-to-node mesh | up    | 09:41:41 | Established |
| 192.168.48.164 | node-to-node mesh | up    | 10:19:09 | Established |
+----------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.

```

#### 1.4.2.7.3: 创建容器测试网络通信：

```bash
root@k8s-master2:~# docker pull harbor.magedu.net/library/alpine
#创建Pod测试跨主机网络通信是否正常
root@k8s-master2:~# kubectl run net-test1 --image=harbor.magedu.net/library/alpine sleep 36000
pod/net-test1 created
root@k8s-master2:~# kubectl run net-test2 --image=harbor.magedu.net/library/alpine sleep 36000
pod/net-test2 created
root@k8s-master2:~# kubectl run net-test3 --image=harbor.magedu.net/library/alpine sleep 36000
pod/net-test3 created


root@k8s-master1:~# kubectl get po -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP               NODE             NOMINATED NODE   REA
net-test1   1/1     Running   0          12s   10.200.36.69     192.168.48.164   <none>           <no
net-test2   1/1     Running   0          15m   10.200.169.129   192.168.48.165   <none>           <no
net-test3   1/1     Running   0          15s   10.200.36.68     192.168.48.164   <none>           <n
```

# 二：资源清单

## 2.1：k8s存在哪些资源

```bash
				名称空间级别
工作负载型资源（Workload):Pod、Replicaset、Deployment、StatefulSet、DaemonSet、Job、CronJob（ReplicationController 在v.11版本被废弃）

服务发现及负载型资源（ServiceDiscover LoadBalance）： Service、Ingress…..

配置与存储型资源： Volume(存储卷)、CSI(容器存储接口，可以扩展各种各样的第三方存储卷)特殊类型的存储卷：ConfigMap（当配置中心来使用的资源类型)、Secret(保存敏感数据)、DownwardAPI(把外部环境中的信息输出给容器)

集群级资源： Namespace、Node、Role、ClusterRole、RoleBinding、ClusterBinding

元数据型资源：HPA、PodTemplate、LimitRange

```

## 2.2: 常用字段的解释

| 参数名                  | 字段类型 | 说明                                                         |
| ----------------------- | -------- | ------------------------------------------------------------ |
| version                 | String   | 这里是指 的是K8S API的版本。目前基本上是v1，可以用Kubectl api-versions命令查询 |
| kind                    | String   | 这里指的是yaml文件定义的资源类型和校色，比如：POD            |
| metadata                | Object   | 元数据对象，固定值就写metadata                               |
| Metadata.name           | string   | 元数据对象的名字，这里由我们编写，比如命名pod的名字          |
| Matadata.namespace      | string   | 元数据对象的命名空间，有我们自身定义                         |
| spec                    | Object   | 详细定义对象，固定值就写spec                                 |
| Spec.containers[]       | list     | 这里是spec对象的容器列表定义，是个列表                       |
| Spec.containers[].name  | string   | 这里定义容器的名字                                           |
| Spec.containers[].image | string   | 这里定义要用到的镜像名称                                     |

**主要对象**

| 参数名                                      | 字段类型 | 说明                                                         |
| ------------------------------------------- | -------- | ------------------------------------------------------------ |
| Spec.containers[].name                      | string   | 定义容器的名字                                               |
| Spec.containers[].image                     | string   | 定义耀荣到的镜像名称                                         |
| Spec.containers[].imagePullPolicy           | string   | 定义镜像拉取策略，有Always、Never、IfNotPressent三个值可选（1）Always：以实是每次都尝试重新拉取镜像（2）Nerver:表示仅使用本地镜像（IfNotPresent）:如果本地有镜像就使用本地镜像，没有就拉取在线镜像。上面三个值都没设置的话默认是Always. |
| Spec.containers[].command[]                 | List     | 指定容器启动命令，因为是数组可以指定多个，不定的则使用镜像打包使用的启动命令 |
| Spec.containers[].args[]                    | List     | 指定容器启动命令参数，因为是数组可以指定多个                 |
| Spec.containers[].workingDir                | String   | 指定容器的工作目录                                           |
| Spec.containers[].volumeMounts[]            | LIst     | 指定容器内部的存储卷配置                                     |
| Spec.containers[].volumeMounts[].name       | String   | 指定可以被容器挂载的存储卷的名称                             |
| Spec.containers[].volumeMount[].mountPath   | string   | 指定可以被容器挂载的存储卷的路径                             |
| Spec.containers[].volumeMounts[].readOnly   | string   | 设置存储卷路径的读写模式，true或者false，默认为读写模式      |
| Spec.containers[].ports[]                   | List     | 指定容器需要用到的端口列表                                   |
| Spec.containers[]ports[].name               | String   | 指定端口名称                                                 |
| Spec.containers[].ports[].containerPoer     | string   | 指定容器需要监听的端口号                                     |
| Spec.containers[].ports[].hostport          | string   | 指定容器所在需要监听的端口号，默认跟上面containersport相同，注意设置了hostPort同一台主机无法启动该容器的相同副本（因为主机的端口号不能相同，这样会冲突） |
| Spec.containers[].ports[].protocol          | string   | 指定端口协议，支持TCP和UDP，默认值为TCP                      |
| Spec.containers[].env[]                     | List     | 指定容器运行前需要设置的环境变  量列表                       |
| Spec.containers[].env[].name                | string   | 指定环境变量名称                                             |
| Spec.containers[].env[].value               | string   | 指定环境变量值                                               |
| Spec.containers[].resources                 | Object   | 指定资源限制和资源请求的值（这里开始就是设置容器的资源上限） |
| Spec.containers[].resources.limits          | Object   | 指定设置容器运行时资源的运行上限                             |
| Spec.containers[].limits.cpu                | string   | 指定CPU的限制，单位为CORE数，将用于docker run --cpu-shares参数（这里前面文章Pod资源） |
| Spec.containers[].limits.memory             | string   | 指定MEM内存的限制，单位为MIB、GIB                            |
| Spec.containers[].resources.requests        | Object   | 指定容器启动和调度时的限制设置                               |
| Spec.containers[].resources.requests.cpu    | string   | cpu请求，单位为core数，容器启动时初始化可以数量              |
| Spec.containers[].resources.requests.memory | string   | 内存请求，单位为MIB、GiB，容器启动的初始化可用数量           |
| Spec.restartPolicy                          | string   | 定义Pod的重启策略，可选值为Always.OnFailure,默认值为Always. 1、Always:Pod一旦终止运行，则无论容器是如何终止的，kubelet服务都将重启它。2、OnFailure:只有Pod以非零退出码终止时，kubelet才会重启该容器，如果容器正常结束，（退出码为0），则kubelet将不会重启它 3、NeverPod终止后，kubelet将退出码报告给Master,不会重启该Pod |
| Spec.nodeSelector                           | Object   | 定义Node的Label过滤标签，以key:value格式指定                 |
| Spec.imagePullSecrets                       | Object   | 定义pull镜像时使用secret名称，以name:secretkey格式指定       |
| Spec.hostNetwork                            | Boolean  | 定义是否使用主机网络模式，默认值为false，设置trye表示使用宿主机网络，不适应docker网桥，同时设置了true将无法在同一台宿主机上启动第二个副本 |

## 2.3：pod生命周期



首先Kubectl 向API接口发送指令，API接口通过etcd存储调度kubelet，kubelet去操作对应的cri,cri完成容器环境初始化，初始化过程先启动一个Pause基础容器，这是谷歌提供的容器，他是负责网络，存储卷共享的，这是它进行initC 或者多个initC初始化 ，initC初始化完成以后，如果是自动退出了，正常码是0，如果是非0就代表这个程序没有正常的成功退出。

多个初始化都做完以后就进行MainC运行主容器，在当运行的时候可以启动一条命令或者启动一个脚本，MAINC 结束之后会执行一个stop命令，也就是结束后怎么办，这个时候readiness和liveness参与，rediness多少秒进行探测,这个程序没有结束POD是不会runing状态，只有这个探测结束之后才会出现啊running状态



![image-20220703175848626](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220703175848626.png)



### 2.3.1：init容器

Pod能够具有多个容器，应用运行在容器里面，但是它也有可能有一个或多个先于应用容器启动时Init容器

 

Init容器与普通的容器非常像，除了如下两点：

- Init容器总是运行到成功完成为止
- 每个Init容器都必须在下一个Init容器启动之前成功完成

如果 Pod的 Init容器失败，kubernetes会不断地重启该Pod,直到Init容器成功为止，然而，如果Pod对应的restartPolicy为Never,它不会重新启动

```bash
vim pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  containers:
  - name: web
    image: busybox
  command:
  - "sh"
  - "sleep 3600"
  initContainers:
  - name: init-web
    image: busybox
    command: ["sh","-c","until nslookup mydb; do echo waiting for mydb; sleep 2; done;"]



vim server.yaml

apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort:80

```

### 2.3.2: 容器的作用

因为Init容器具有于应用程序容器分离的单独镜像，所有它们的启动相关代码具有如下优势：

- 他们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用得工具的
- 它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要FROM另一个镜像，只需要在安装过程中使用类似sed、awk、python、dig这样的工具
- 应用 程序镜像可以分离出创建和部署的角色，而没有必要联合他们构建一个单独的镜像。
- Init容器使用Linux Namespace 所以相对应用程序来说具有不同的文件系统视图。因此，他们能够具有访问Secret的权限，而应用程序则不能
- 他们必须在应用程序容器启动之前运行完成，而应用程序容器并运行的，所以Init容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足一组先决条件

# 三：集群管理：

集群管理主要是添加master、添加node、删除master与删除node等节点管理及监控：

当前集群状态：



```bash
root@k8s-master1:~# kubectl get no
NAME             STATUS                     ROLES    AGE     VERSION
192.168.48.163   Ready,SchedulingDisabled   master   5h36m   v1.22.2
192.168.48.164   Ready                      node     51m     v1.22.2
192.168.48.165   Ready                      node     5h26m   v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   5h36m   v1.22.2

```



## 3.1: 删除node节点：

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl help
root@k8s-master1:/etc/kubeasz# ./ezctl del-node k8s-01 192.168.48.164

```

验证node节点信息：

```bash
root@k8s-master1:/etc/kubeasz# kubectl get no
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.163   Ready,SchedulingDisabled   master   8h    v1.22.2
192.168.48.165   Ready                      node     8h    v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   8h    v1.22.2

#删除node节点，ansible会在hosts清单中删除nodeip
root@k8s-master1:/etc/kubeasz# vim clusters/k8s-01/hosts
```

![image-20220210215631940](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220210215631940.png)

### 3.1.1：添加node节点：

```bash
root@k8s-master1:/etc/kubeasz# ./ezctl help
root@k8s-master1:/etc/kubeasz# ./ezctl add-node k8s-01 192.168.48.164
```

验证node节点信息：

```bash
root@k8s-master1:/etc/kubeasz# kubectl get no
NAME             STATUS                     ROLES    AGE    VERSION
192.168.48.163   Ready,SchedulingDisabled   master   8h     v1.22.2
192.168.48.164   Ready                      node     8m8s   v1.22.2
192.168.48.165   Ready                      node     8h     v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   8h     v1.22.2

```

### 3.1.2：删除master节点：

```
root@k8s-master1:/etc/kubeasz# ./ezctl help
root@k8s-master1:/etc/kubeasz# ./ezctl del-master k8s-01 192.168.48.163
```

验证当前节点：

```
root@k8s-master1:/etc/kubeasz# kubectl get no
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.164   Ready                      node     14m   v1.22.2
192.168.48.165   Ready                      node     8h    v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   8h    v1.22.2

```

### 3.1.3：添加master节点

```
root@k8s-master1:/etc/kubeasz# ./ezctl help
root@k8s-master1:/etc/kubeasz# ./ezctl add-master k8s-01 192.168.48.163
```

验证当前节点：

```
root@k8s-master1:/etc/kubeasz# kubectl get no
NAME             STATUS                     ROLES    AGE     VERSION
192.168.48.163   Ready,SchedulingDisabled   master   3m47s   v1.22.2
192.168.48.164   Ready                      node     23m     v1.22.2
192.168.48.165   Ready                      node     8h      v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   8h      v1.22.2

```

## 3.2：验证node节点路由：

```
root@k8s-master1:/etc/kubeasz# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.48.2    0.0.0.0         UG    20100  0        0 ens33
10.200.36.64    192.168.48.164  255.255.255.192 UG    0      0        0 tunl0
10.200.159.128  0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.169.128  192.168.48.165  255.255.255.192 UG    0      0        0 tunl0
10.200.224.0    192.168.48.163  255.255.255.192 UG    0      0        0 tunl0
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 ens33
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.48.0    0.0.0.0         255.255.255.0   U     100    0        0 ens33
192.168.48.99   0.0.0.0         255.255.255.255 UH    100    0        0 ens33

```



## 3.3：DNS服务：

```
目前在kubernetes中常用的dns组件有kube-dns和coredns两个，kube-dns和coredns用于解析k8s集群中service name所以对应得到ip地址，在k8s版本1.17.x和之前的版本都可以使用kube-dns，但是k8s在1.18之后不在支持kube-dns
```

## 3.4：部署coredns:

k8s 1.18版本以后将不再支持kube-dns

https://github.com/coredns/coredns

```bash
root@k8s-master1:~# cat coredns-v1.8.6.yaml 
# __MACHINE_GENERATED_WARNING__

apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get

- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes magedu.local. in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153
        forward . 8.8.8.8 {
            max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: k8s-app
                    operator: In
                    values: ["kube-dns"]
              topologyKey: kubernetes.io/hostname
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      containers:
      - name: coredns
        #image: k8s.gcr.io/coredns/coredns:v1.8.0
        image: coredns/coredns:1.8.6 
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.100.0.2 
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
    
root@k8s-master1:~# kubectl apply -f coredns-v1.8.6.yaml
```

### 3.4.1: 测试域名解析：

```bash
root@k8s-master1:~# kubectl exec -it net-test2 sh
/ # ping www.baidu.com
PING www.baidu.com (103.235.46.39): 56 data bytes
64 bytes from 103.235.46.39: seq=0 ttl=127 time=240.299 ms
64 bytes from 103.235.46.39: seq=1 ttl=127 time=256.892 ms
64 bytes from 103.235.46.39: seq=2 ttl=127 time=248.252 ms
64 bytes from 103.235.46.39: seq=3 ttl=127 time=249.436 ms

/ # ping kubernetes.default.svc.magedu.local
PING kubernetes.default.svc.magedu.local (10.100.0.1): 56 data bytes
64 bytes from 10.100.0.1: seq=0 ttl=64 time=0.218 ms
64 bytes from 10.100.0.1: seq=1 ttl=64 time=0.174 ms
64 bytes from 10.100.0.1: seq=2 ttl=64 time=0.128 ms
^C
--- kubernetes.default.svc.magedu.local ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.128/0.173/0.218 ms

```

### 3.4.1.: 查看所有资源对象

```bash
root@s3:~# kubectl api-resources
```

### 3.4.2：kubectl 自动补全

```bash
source <(kubectl completion bash) # 在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "source <(kubectl completion bash)" >> ~/.bashrc # 在您的 bash shell 中永久的添加自动补全
```



## 3.5: dashboard:

部署kubernetes的web管理界面dashboard

https://github.com/kubernetes/dashboard



### 1.5.1: 部署dashboard:

![image-20220212222105746](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212222105746.png)

![image-20220212222044116](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212222044116.png)

```bash
root@k8s-master1:~# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.0/aio/deploy/recommended.yaml

#添加如下配置：
vim recommended.yaml
```

![image-20220212223523015](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212223523015.png)

```bash
root@k8s-master1:~# cat admin-user.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

root@k8s-master1:~# kubectl apply -f recommended.yaml
root@k8s-master1:~# kubectl apply -f admin-user.yaml
```

![image-20220212225740870](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212225740870.png)

### 1.5.1.1: tocken登录dashboard:

```
root@k8s-master1:~# kubectl get secret -A | grep admin
kubernetes-dashboard   admin-user-token-9sxcb                           kubernetes.io/service-account-token   3      11s
root@k8s-master1:~# kubectl -n kubernetes-dashboard describe secret admin-user-token-9sxcb
Name:         admin-user-token-9sxcb
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 25b40ed3-5cd1-4474-a16f-4b3cbac933d1

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InJNSVBzV3lvb0hodHRJemN4UGgxX0U2VThYTzJLZ3puLXZxdDg1ZWIxRkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWQycHJ4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjNjY0MDA3Yi03OWRmLTQyMzQtOGNhNy00OWVkZTA0OWMwNDEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.PXjeyUy_uYBqLzLdxIV1kLEpcj811gMecXt4ncnD9bs6nsnykopKVBbb4l-0Nc6najxZB2oNCPvK8Uck9okGIWtLM4fULEkiu5ZDw478il8sBcyWu-h5-WqeIF3bVfEWZkr-J3ueGAneb9GsiLGosleXD998PgSp_jk7B-dMvm_VGJZlkmiDXvUt6RxPd6V87K_iIp65DObDgxWTOBz1okyCpG4zaXfRGuTYso8Lflq1r3iCHDHiHwpeBmm4axr6D2gmAZIVJOYSVlsS9TR-cfbpfHX-7UEbQNoNDnHk4c9Nsa6GYKvp9OixGVbaDI1ECc9O2vy_Tl9tk_ProCbcxg

ca.crt:     1350 bytes
namespace:  20 bytes


```

![image-20220212230609064](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212230609064.png)

![image-20220212232055276](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220212232055276.png)

### 1.5.1.2: 这是token登录会话保持时间

```bash
  root@k8s-master1:~# vim recommended.yaml
  spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.3.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            - --token-ttl=43200
  root@k8s-master1:~# kubectl apply -f recommended.yaml
```

#### 1.5.1.3：kubeconfig文件登录dashboard

```bash
root@s3:~# kubectl get secret -A | grep admin
kubernetes-dashboard   admin-user-token-fm4cq                           kubernetes.io/service-account-token   3      14h
#获取token地址
root@s3:~#  kubectl -n kubernetes-dashboard describe secret admin-user-token-fm4cq
.....
....
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InQ2U2Njd0ZXTk9OYlR1alM4S3R3X29nV1dYdFhMLWdPLVJWaVNxWXE1ZWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWZtNGNxIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4MzUxZThlYi1hZTY2LTQxODUtYThlNS05N2NkMDA4N2E2YWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.ULzIw8Io2SzeEy6cuD7Sci01fZ2L3ocKfwFCLenQgNlolvNKbz98c64m4V5IKcenTxiIFMoNGle13x6I406vrxtOwsM3ySsUfowVqI1uImYvyog8wc5FD1mmPLWRASibDhNwVmyssAqFhOwQ4cVnbv_ZOrkY-S4SPAjwrudccGlBwRzcmYii8hkYx4XDkVTbVL15z8SUH1n4lBOtVFO-GOALeMSQw68Lvb4Z6hfVpVlpaWljj6VzBpyJFNKr6anxLjBE2EdbwinlJzl4L083tdO9tffG8-6hgovfNJ7ob-ENncp9n64w2ytvO7dojcDx2_v1cZPW9lrPe12MsTqvMA
#制作kubeconfig文件
root@s3:~# cp /root/.kube/config /opt/kubeconfig
#添加token地址到admin用户最下面
root@s3:~# vim /opt/kubeconfig
.....
...
#注前面四个空格
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InQ2U2Njd0ZXTk9OYlR1alM4S3R3X29nV1dYdFhMLWdPLVJWaVNxWXE1ZWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWZtNGNxIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4MzUxZThlYi1hZTY2LTQxODUtYThlNS05N2NkMDA4N2E2YWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.ULzIw8Io2SzeEy6cuD7Sci01fZ2L3ocKfwFCLenQgNlolvNKbz98c64m4V5IKcenTxiIFMoNGle13x6I406vrxtOwsM3ySsUfowVqI1uImYvyog8wc5FD1mmPLWRASibDhNwVmyssAqFhOwQ4cVnbv_ZOrkY-S4SPAjwrudccGlBwRzcmYii8hkYx4XDkVTbVL15z8SUH1n4lBOtVFO-GOALeMSQw68Lvb4Z6hfVpVlpaWljj6VzBpyJFNKr6anxLjBE2EdbwinlJzl4L083tdO9tffG8-6hgovfNJ7ob-ENncp9n64w2ytvO7dojcDx2_v1cZPW9lrPe12MsTqvMA
   #上传到本机
root@s3:~# sz /opt/kubeconfig 
```



![image-20220603142358703](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603142358703.png)





### 1.5.2: kuboard

kuboard-kubernetes多集群管理界面

```bash
docker run -d --restart=unless-stopped --name=kuboard -p 80:80/tcp -p 10081:10081/tcp -e KUBOARD_ENDPOINT="http://192.168.48.145:80" -e KUBOARD_AGENT_SERVER_TCP_PORT="10081" -v /root/kuboard-data:/data swr.cn-east-2.myhuaweicloud.com/kuboard/kuboard:v3


在浏览器输入ip：80 既可以访问kuboard的界面登录方式：
用户名：admin
密码：Kuboard123
```

![image-20220213230530584](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213230530584.png)



![image-20220213230551527](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213230551527.png)

![image-20220213230632354](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213230632354.png)



```
#在mster服务器上执行以上命令，把里面内容复制过来
root@k8s-master1:~# cat ~/.kube/config
```

![image-20220213231126801](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213231126801.png)

![image-20220213231056804](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213231056804.png)

![image-20220213231223813](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220213231223813.png)





## 3.6：集群升级

### 3.6.1：升级前准备

https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#v1225

```bash
#验证当前节点版本
root@k8s-master1:~# kubectl get no
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.163   Ready,SchedulingDisabled   master   24h   v1.22.2
192.168.48.164   Ready                      node     23h   v1.22.2
192.168.48.165   Ready                      node     23h   v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   23h   v1.22.2

#下载包到服务器中
root@k8s-master1:/usr/local/src# ll
total 468444
drwxr-xr-x  2 root root      4096 2月  13 21:53 ./
drwxr-xr-x 10 root root      4096 9月  16 04:17 ../
-rw-r--r--  1 root root  29011907 1月   1 13:54 kubernetes-v1.22.5-client-linux-amd64.tar.gz
-rw-r--r--  1 root root 118916964 1月   1 13:54 kubernetes-v1.22.5-node-linux-amd64.tar.gz
-rw-r--r--  1 root root 331170423 1月   1 13:54 kubernetes-v1.22.5-server-linux-amd64.tar.gz
-rw-r--r--  1 root root    565584 1月   1 13:54 kubernetes-v1.22.5.tar.gz

#解压
root@k8s-master1:/usr/local/src# tar -xf kubernetes-v1.22.5-client-linux-amd64.tar.gz 
root@k8s-master1:/usr/local/src# tar -xf kubernetes-v1.22.5-node-linux-amd64.tar.gz 
root@k8s-master1:/usr/local/src# tar -xf kubernetes-v1.22.5-server-linux-amd64.tar.gz 
root@k8s-master1:/usr/local/src# tar -xf kubernetes-v1.22.5.tar.gz

```

### 3.6.2：master升级

```bash
#注释haproxymaster1,不被调度
listen k8s-api_nodes_6443
        bind 192.168.48.99:6443
        mode tcp
     #   server 192.168.48.166 192.168.48.166:6443 check inter 2000 fall 3 rise 5
        server 192.168.48.163 192.168.48.163:6443 check inter 2000 fall 3 rise 5
        
#node节点注销掉master
root@k8s-node1:~# vi /etc/kube-lb/conf/kube-lb.conf 
stream {
    upstream backend {
#        server 192.168.48.166:6443    max_fails=2 fail_timeout=3s;
        server 192.168.48.163:6443    max_fails=2 fail_timeout=3s;
    }
root@k8s-node1:~# systemctl restart kube-lb
#组件停掉master
root@k8s-master1:/usr/local/src# systemctl stop kube-proxy.service kube-scheduler.service kubelet kube-apiserver  kube-controller-manager
root@k8s-master1:/usr/local/src# cd kubernetes/server/bin/

#拷贝组件到bin目录下面
root@k8s-master1:/usr/local/src/kubernetes/server/bin# cp kube-apiserver kube-controller-manager kubelet kubectl kube-proxy kube-scheduler /usr/bin/

#取消注释
root@k8s-master1:~# vim /etc/haproxy/haproxy.cfg
listen k8s-api_nodes_6443
        bind 192.168.48.99:6443
        mode tcp
        server 192.168.48.166 192.168.48.166:6443 check inter 2000 fall 3 rise 5
        server 192.168.48.163 192.168.48.163:6443 check inter 2000 fall 3 rise 5
        
  root@k8s-master1:~# systemctl restart haproxy
  
  stream {
    upstream backend {
        server 192.168.48.166:6443    max_fails=2 fail_timeout=3s;
        server 192.168.48.163:6443    max_fails=2 fail_timeout=3s;
    }


root@k8s-node1:~# systemctl reload kube-lb.service


#启动组件
root@k8s-master1:/usr/local/src/kubernetes/server/bin# systemctl start kube-proxy.service kube-scheduler.service kubelet kube-apiserver  kube-controller-manager
```

#### 3.6.2.1：验证master

```bash
root@k8s-master1:~# kubectl get no
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.163   Ready,SchedulingDisabled   master   25h   v1.22.5
192.168.48.164   Ready                      node     25h   v1.22.2
192.168.48.165   Ready                      node     25h   v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   25h   v1.22.5


```

### 3.6.3：node升级

1.node上有业务pod需要驱逐node上的pod

```bash
root@k8s-master1:~ kubectl drain 192.168.48.165 --ignore-daemonsets --force
root@k8s-node1:~# systemctl stop kubelet kube-proxy
root@k8s-master1:/usr/local/src/kubernetes/server/bin# scp kubectl kubelet kube-proxy 192.168.48.165:/usr/bin/
kubectl                                                           100%   45MB  81.9MB/s   00:00    
kubelet                                                           100%  116MB  53.1MB/s   00:02    
kube-proxy                                                        100%   41MB  53.4MB/s   00:00  

root@k8s-node1:~# systemctl start kubelet kube-proxy
```

#### 3.6.3.1：验证node

```
root@k8s-master1:/usr/local/src/kubernetes/server/bin# kubectl get no
NAME             STATUS                     ROLES    AGE   VERSION
192.168.48.163   Ready,SchedulingDisabled   master   25h   v1.22.5
192.168.48.164   Ready                      node     25h   v1.22.5
192.168.48.165   Ready                      node     25h   v1.22.5
192.168.48.166   Ready,SchedulingDisabled   master   25h   v1.22.5

```

# 四：etcd

### 4.1 etcd简介

etcd是CoreOS团队于2013年6月发起得开源项目，它的目标是构建一个高可用得分布式键值（key-value）数据库，etcd内部采用raft协议作为一致性算法,etcd基于Go语言实现。

官方网站：https://etcd.io/

github地址：https://github.com/etcd-io/etcd

https://etcd.io/docs/v3.4/op-guide/hardware/ #官方硬件推荐



Etcd具有下面这些属性：

完全复制：集群中得每个节点都可以使用完整得存档

高可以用性：Etcd可用于避免硬件的单点故障或网络问题

一致性：每次读取都会返回跨多主机的最新写入

简单：包括一定定义良好、面向用户的API（gRPC）

安全：实现了带有可选的客户端证书身份验证的自动化TLS

快速：每秒10000次写入的基准速度

可靠：使用Raft算法实现了存储的合理分布Etcd的工作原理

```
root@k8s-master1:~# cat /etc/systemd/system/etcd.service 
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd #数据保存目录
ExecStart=/usr/bin/etcd \	#二进制文件路径
  --name=etcd-192.168.48.166 \	#当前node名称
  --cert-file=/etc/kubernetes/ssl/etcd.pem \
  --key-file=/etc/kubernetes/ssl/etcd-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \
  --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls=https://192.168.48.166:2380 \#通告自己的集群端口
  --listen-peer-urls=https://192.168.48.166:2380 \ #集群之间通信端口
  --listen-client-urls=https://192.168.48.166:2379,http://127.0.0.1:2379 \ #客户端访问地址
  --advertise-client-urls=https://192.168.48.166:2379 \ 通告自己的客户端端口
  --initial-cluster-token=etcd-cluster-0 \ #创建集群使用的token，一个集群内的节点保持一致
  --initial-cluster=etcd-192.168.48.166=https://192.168.48.166:2380,etcd-192.168.48.163=https://192.168.48.163:2380 \ #集群索引的节点信息
  --initial-cluster-state=new \ #新建集群的时候值为new，如果已经存在的集群为existing
  --data-dir=/var/lib/etcd \ #数据目录路径
  --wal-dir= \
  --snapshot-count=50000 \
  --auto-compaction-retention=1 \
  --auto-compaction-mode=periodic \
  --max-request-bytes=10485760 \
  --quota-backend-bytes=8589934592
Restart=always
RestartSec=15
LimitNOFILE=65536
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target

```

### 2.1.1 查看成员信息

etcd有多个不同的API访问版本,v1版本已经废弃，etcd v2和v3本质上是共享一套raft协议代码的两个独立应用，接口不一样，存储不一样，数据互相隔离，也就是说如果从Etcd v2升级到Etcd v3，原来的v2的数据还是只能用v2的接口访问，v3的接口创建的数据也只能访问通过v3接口访问。

```
root@k8s-master1:~# ETCDCTL_API=3 etcdctl --help
root@k8s-master1:~# ETCDCTL_API=3 etcdctl member --help
NAME:
	member - Membership related commands

USAGE:
	etcdctl member <subcommand> [flags]

API VERSION:
	3.5


COMMANDS:
	add	Adds a member into the cluster
	list	Lists all members in the cluster
	promote	Promotes a non-voting member in the cluster
	remove	Removes a member from the cluster
	update	Updates a member in the cluster

OPTIONS:
  -h, --help[=false]	help for member
  root@k8s-master1:~# ETCDCTL_API=3 etcdctl member list

```

### 2.1.2 验证当前etcd所有成员状态：

```
root@k8s-master1:/etc/kubeasz# export NODE_IPS="192.168.48.166 192.168.48.163"
root@k8s-master1:~# for ip in ${NODE_IPS};do ETCDCTL_API=3 /usr/bin/etcdctl --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem endpoint health; done
```

![image-20220221222739890](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220221222739890.png)



### 2.1.3 以表格方式显示节点详细状态：

```
root@k8s-master1:/etc/kubeasz# export NODE_IPS="192.168.48.166 192.168.48.163"
root@k8s-master1:~# for ip in ${NODE_IPS};do ETCDCTL_API=3 /usr/bin/etcdctl --write-out=table endpoint status --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem; done

```

![image-20220221230333260](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220221230333260.png)



### 2.1.4 查看etcd数据信息：

```
root@k8s-master1:~# ETCDCTL_API=3 etcdctl get / --prefix --keys-only #以路径的方式所有key信息
pod信息
root@k8s-master1:~# ETCDCTL_API=3 etcdctl get / --prefix --keys-only  | grep pod
namespace信息
root@k8s-master1:~# ETCDCTL_API=3 etcdctl get / --prefix --keys-only  | grep namespace

控制器信息：
root@k8s-master1:~# ETCDCTL_API=3 etcdctl get / --prefix --keys-only  | grep deployemnt

calico组件信息;
root@k8s-master1:~# ETCDCTL_API=3 etcdctl get / --prefix --keys-only  | grep calico
```



### 2.1.5 etcd增删改查数据：



```
#添加数据
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl put /name "tom"
OK
#查询数据
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl get /name
/name
tom
#改动数据，#直接覆盖就是更新数据
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl put /name "jack"
OK

#验证改动
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl get /name
/name
jack

#删除数据
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl del /name
1

root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl get /name
```

### 2.1.6 etcd数据watch机制：

基于不断监控看数据，发生变化就主动触发通知客户端，Etcd v3 的机制支持watch某个固定的key，也支持watch一个范围

#在etcd 弄得上watch-个key，没有此key也可也执行watch，后期可以在创建：

```
root@k8s-master1:~# ETCDCTL_API=3 /usr/bin/etcdctl watch /data

#在etcd2 node2修改数据，验证etcd node1是否能够发现数据变化
root@k8s-master2:~# ETCDCTL_API=3 /usr/bin/etcdctl put /data "data v1"
OK
root@k8s-master2:~# ETCDCTL_API=3 /usr/bin/etcdctl put /data "data v2"
OK
#验证etcd node1
```

![image-20220221232425528](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220221232425528.png)



### 2.1.7 V3 API版本数据备份于恢复

WAL是write ahead log的缩写，顾名思义，也就是在执行真正的写操作之前先写一个日志，预写日志

wal：存放预写日志，最大作用是记录了整个数据变化的全部历程，在etcd中，所有数据的修改在提交前，都要写入到WAL中

备份数据：

```
#etcd数据目录
root@k8s-master2:~/back# cd /var/lib/etcd/member/
root@k8s-master1:/var/lib/etcd/member/wal# pwd
/var/lib/etcd/member/wal	#预写时日志，写数据先写日志，用于恢复数据
root@k8s-master2:/var/lib/etcd/member# ls
snap  wal

#创建目录用于备份
root@k8s-master2:~# mkdir back
root@k8s-master2:~# cd back/
#备份当前目录下面
root@k8s-master1:~# etcdctl snapshot save /back/n60.bak
#恢复到一个新的不存在得目录中
root@k8s-master2:~/back# etcdctl snapshot restore snapshot.db -data-dir=/opt/etcd-testdir
Deprecated: Use `etcdutl snapshot restore` instead

#自动备份数据
root@k8s-master2:~/back# mkdir /data/etcd-backup-dir -p
root@k8s-master2:~/back# cat script.sh 
#!/bin/bash
DATE=`date +%Y-%m-%d_%H-%M-%S`
/usr/bin/etcdctl snapshot save /data/etcd-backup-dir/etcd-snapshot-${DATE}.db
```

### 2.1.8 集群数据自动备份与恢复

```
#备份集群
root@k8s-master1:/etc/kubeasz# ./ezctl backup k8s-cluster1
#备份目录
root@k8s-master1:/etc/kubeasz# cd clusters/k8s-cluster1/
root@k8s-master1:/etc/kubeasz/clusters/k8s-cluster1# cd backup/
root@k8s-master1:/etc/kubeasz/clusters/k8s-cluster1/backup# ls
snapshot_202202122226.db  snapshot.db
#当前系统得时间
#查看当前pod
root@k8s-master1:~# kubectl get pod
NAME        READY   STATUS        RESTARTS   AGE
net-test1   1/1     Terminating   0          10d
net-test2   1/1     Running       0          10d
net-test3   1/1     Terminating   0          10d
#删除pod
root@k8s-master1:~# kubectl delete po net-test2
pod "net-test2" deleted

#恢复数据
root@k8s-master1:/etc/kubeasz# ./ezctl restore k8s-cluster1
#验证
root@k8s-master1:/etc/kubeasz# kubectl get po
NAME        READY   STATUS              RESTARTS   AGE
net-test1   1/1     Terminating         0          10d
net-test2   1/1     Running             1          10d
net-test3   1/1     Terminating         0          10d

```

## 4.2：Velrio结合minio实现kubernetes业务数据备份与恢复

### 4.2.1：Velero简介

Velero是vmware开源的一个云原生的灾难恢复和迁移工具，它本身也是开源的，采用Go语言编写，开源安全的备份、恢复和迁移Kubernetes集群资源数据

https://velero.io/

Velero是西班牙语意思是帆船，非常符合kubernetes社区的名命风格，Velero的开发公司风格，Velero的开发公司Heptio，已被Vmware收购

Vrlro支持标准的k8s集群，既开源是私有云平台也可以是公有云，除了灾备之外它还能做资源转移，支持把容器从一个集群迁移到另一个集群

Velero的工作方式就是把Kubernetes中的数据备份到对象存储以实现高可用和持久化，默认的备份保存时间为720小时，并在需要的时候进行下载和恢复

#### 4.2.2：Velero与etcd快照备份的区别

etcd快照是全局备份，在即使一个资源对象需要恢复，也需要做全局恢复到备份的状态，既会影响其它namespace中pod运行服务

velero可以有针对性的备份，比如按照namespace单独备份，只备份单独的资源对象等，在恢复的时候只恢复单独的namespace或资源对象，而不影响其它namespace中pod运行服务

velero支持ceph、oss等对象存储，etcd快照是一个为本地文件

velero支持任务计划实现周期备份，但etcd快照也可以基于cronjob实现

#### 4.2.3：Velero备份流程

Velero客户端调用kubernetes API Server创建Backup任务

Backup控制器基于watch机制通过API Server获取到备份任务

Backup控制器开始执行备份动作，其会通过请求API Server获取需要备份的数据

Backup控制器获取到的数据备份到指定的对象存储server端



![image-20220802153341684](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220802153341684.png)



### 4.2.4：部署minio

```bash
root@s5:~# docker search minio
root@s5:~# docker pull minio/minio
root@s5:~# mkdir -p /data/minio
#创建minio容器，如果不之地那个，则默认用户名与密码为minioadmin/minioadmin,可以通过环境变量自定义，密码为8位数
root@s5:~# docker run --name minio \
> -p 9000:9000 \
> -p 9999:9999 \
> -d --restart=always \
> -e 'MINIO_ROOT_USER=admin' \
> -e 'MINIO_ROOT_PASSWORD=12345678' \
> -v /data/minio/data:/data \
> minio/minio:latest server /data \
> --console-address '0.0.0.0:9999'
```

web界面



![image-20220801213509869](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220801213509869.png)



创建bucker

![image-20220801213645267](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220801213645267.png)



验证bucket



![image-20220801213758822](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220801213758822.png)



## 4.3: 部署velero

在master节点部署velero

```bash
root@s3:/usr/local/src# wget https://github.com/vmware-tanzu/velero/releases/download/v1.8.1/velero-v1.8.1-linux-amd64.tar.gz
root@s3:/usr/local/src# tar -xf velero-v1.8.1-linux-amd64.tar.gz
root@s3:/usr/local/src# cp velero-v1.8.1-linux-amd64/velero /usr/bin/
root@s3:/usr/local/src# velero --help

配置velero认证环境
root@s3:/usr/local/src# mkdir /data/velero -p
#认证文件
root@s3:/data/velero# cat velero-auth.txt 
[default]
aws_access_key_id = admin
aws_secret_access_key = 12345678

#准备user-csr文件
root@s3:/data/velero# cat awsuser-csr.json 
{
  "CN": "awsuser",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

root@s3:/data/velero# cp -r /etc/kubeasz/bin/cfssl* /usr/bin/
root@s3:/data/velero# chmod a+x /usr/bin/cfssl*
#执行证书签发
root@s3:/data/velero# /usr/bin/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubeasz/clusters/kubernetes-cluster/ssl/ca-config.json --profile=kubernetes ./awsuser-csr.json | cfssljson -bare awsuser

#验证证书
root@s3:/data/velero# ll awsuser*
-rw-r--r-- 1 root root  997 8月   1 22:02 awsuser.csr
-rw-r--r-- 1 root root  220 8月   1 21:57 awsuser-csr.json
-rw------- 1 root root 1679 8月   1 22:02 awsuser-key.pem
-rw-r--r-- 1 root root 1387 8月   1 22:02 awsuser.pem

#分发证书到api-server证书路径
root@s3:/data/velero# cp awsuser-key.pem /etc/kubernetes/ssl/
root@s3:/data/velero# cp awsuser.pem /etc/kubernetes/ssl/

#生成集群认证config文件
root@s3:/data/velero# export KUBE_APISERVER="https://192.168.48.166:6443"
root@s3:/data/velero# kubectl config set-cluster cluster1 --certificate-authority=/etc/kubernetes/ssl/ca.pem  --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=./awsuser.kubeconfig

#设置客户端证书认证
root@s3:/data/velero# kubectl config set-credentials awsuser --client-certificate=/etc/kubernetes/ssl/awsuser.pem --client-key=/etc/kubernetes/ssl/awsuser-key.pem --embed-certs=true --kubeconfig=./awsuser.kubeconfig

#设置上下文参数
root@s3:/data/velero# kubectl config set-context cluster1 --cluster=cluster1 --user=awsuser --namespace=velero-system --kubeconfig=./awsuser.kubeconfig

#设置默认上下文
root@s3:/data/velero# kubectl config use-context cluster1 --kubeconfig=awsuser.kubeconfig 
Switched to context "cluster1".
#k8s集群中创建awsuser账户
root@s3:/data/velero# kubectl create clusterrolebinding awsuser --clusterrole=cluster-admin --user=awsuser
clusterrolebinding.rbac.authorization.k8s.io/awsuser created
#创建namespace
root@s3:/data/velero# kubectl create ns velero-system
namespace/velero-system created
#执行安装
root@s3:/data/velero# velero --kubeconfig ./awsuser.kubeconfig install --provider aws --plugins velero/velero-plugin-for-aws:v1.3.1 --bucket webdata --secret-file ./velero-auth.txt --use-volume-snapshots=false --namespace velero-system --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://192.168.48.164:9000

--provider aws #提供者aws 
--plugins velero/velero-plugin-for-aws:v1.3.1 #谁用那个插件
 --bucket webdata #存储名字
 --use-volume-snapshots=false#是否使用快照 false
root@s3:/data/velero# kubectl describe po velero-f7c9588d7-6vlsm  -n velero-system


root@s3:/data/velero# kubectl get po  -n velero-system 
NAME                     READY   STATUS    RESTARTS   AGE
velero-f7c9588d7-6vlsm   1/1     Running   0          10m

```

### 4.3.1：对default ns进行备份

```bash
#对default ns进行备份
root@s3:/data/velero# pwd
/data/velero

root@s3:/data/velero# DATE=`date +%Y%m%d%H%M%S`
root@s3:/data/velero# velero backup create default-backup-${DATE} --include-cluster-resources=true --include-namespaces default --kubeconfig=./awsuser.kubeconfig --namespace velero-system
 --include-cluster-resources=true #导入集群资源
 --include-namespaces default #备份的名称空间
#验证备份
root@s3:/data/velero# velero backup describe default-backup-20220801223535 --kubeconfig=./awsuser.kubeconfig --namespace velero-system
```

### 4.3.2：minio验证备份数据



![image-20220801224500322](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220801224500322.png)



### 4.3.3：删除pod并验证数据恢复

```bash
root@s3:/data/velero# kubectl delete pod livenessprobe-pod-58c898d449-dqpvv
pod "livenessprobe-pod-58c898d449-dqpvv" deleted

root@s3:/data/velero# velero restore create --from-backup default-backup-20220801223535 --wait --kubeconfig=./awsuser.kubeconfig --namespace velero-system
 
#验证恢复后的pod是否存在
root@s3:/data/velero# kubectl get po
NAME                                 READY   STATUS    RESTARTS   AGE
livenessprobe-pod-58c898d449-dqpvv   1/1     Running   0          67s
```

## 4.4：n60 ns进行备份

```bash
root@s3:/data/ingress# kubectl get po -n n60
NAME                          READY   STATUS    RESTARTS       AGE
tomcat-app1-8f98c555c-stsx8   1/1     Running   3 (130m ago)   6d12h
tomcat-app2-c796fd57f-n44q7   1/1     Running   0              61s
root@s3:/data/velero# DATE=`date +%Y%m%d%H%M%S`

root@s3:/data/velero# velero backup create n60-ns-backup-${DATE} --include-cluster-resources=true --include-namespaces n60 --kubeconfig=/root/.kube/config --namespace velero-system

```

minio验证备份



![image-20220801230356032](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220801230356032.png)

### 4.4.5：删除数据并恢复

```bash
root@s3:/data/ingress# kubectl delete -f .
root@s3:/data/ingress# kubectl get po -n n60
No resources found in n60 namespace.

root@s3:/data/velero# velero restore create --from-backup n60-ns-backup-20220801223535 --wait --kubeconfig=./awsuser.kubeconfig  --namespace velero-system

root@s3:/data/velero# kubectl get po -n n60
NAME                          READY   STATUS    RESTARTS   AGE
tomcat-app1-8f98c555c-stsx8   1/1     Running   0          80s
tomcat-app2-c796fd57f-n44q7   1/1     Running   0          80s

```

### 4.4.6: 备份指定资源对象

备份指定namespace中的pod或特定资源

```bash
root@s3:/data/velero# velero backup create pod-backup-${DATE} --include-cluster-resources=true --ordered-resources 'pods=n60/tomcat-app1-8f98c555c-stsx8' --namespace velero-system --include-namespaces=n60

```

#### 4.4.6.1：删除pod与恢复

```bash
#删除pod
root@s3:/data/ingress# kubectl delete -f tomcat-app1.yaml 
deployment.apps "tomcat-app1" deleted
root@s3:/data/velero# velero restore create --from-backup pod-backup-20220801223535 --wait --kubeconfig=./awsuser.kubeconfig  --namespace velero-system
root@s3:/data/velero# kubectl get po -n n60
NAME                          READY   STATUS    RESTARTS   AGE
tomcat-app1-8f98c555c-stsx8   1/1     Running   0          70s
tomcat-app2-c796fd57f-n44q7   1/1     Running   0          44m

```

### 4.4.7：备份所有名称空间

```bash
root@s3:~# cat back.sh 
#!/bin/bash
NS_NAME=`kubectl get ns | awk '{if (NR>2){print}}' | awk '{print $1}'`
DATE=`date +%Y%m%d%H%M%S`
cd /data/velero/

for i in $NS_NAME;do
  velero backup create ${i}-ns-backup${DATE} \
--include-cluster-resources=true \ 
--include-namespaces ${i} \
--kubeconfig=/root/.kube/config \
--namespace velero-system 
done
```



## 2.1: yaml文件详解

上下级关系

列表

键值对

```bash
kind: Deployment  #类型，是deployment控制器，kubectl explain  Deployment
apiVersion: extensions/v1beta1  #API版本，# kubectl explain  Deployment.apiVersion
metadata: #pod的元数据信息，kubectl explain  Deployment.metadata
  labels: #自定义pod的标签，# kubectl explain  Deployment.metadata.labels
    app: linux36-nginx-deployment-label #标签名称为app值为linux36-nginx-deployment-label，后面会用到此标签 
  name: linux36-nginx-deployment #pod的名称
  namespace: linux36 #pod的namespace，默认是defaule
spec: #定义deployment中容器的详细信息，kubectl explain  Deployment.spec
  replicas: 1 #创建出的pod的副本数，即多少个pod，默认值为1
  selector: #定义标签选择器
    matchLabels: #定义匹配的标签，必须要设置
      app: linux36-nginx-selector #匹配的目标标签，
  template: #定义模板，必须定义，模板是起到描述要创建的pod的作用
    metadata: #定义模板元数据
      labels: #定义模板label，Deployment.spec.template.metadata.labels
        app: linux36-nginx-selector #定义标签，等于Deployment.spec.selector.matchLabels
    spec: #定义pod信息
      containers:#定义pod中容器列表，可以多个至少一个，pod不能动态增减容器
      - name: linux36-nginx-container #容器名称
        image: harbor.magedu.net/linux36/nginx-web1:v1 #镜像地址
        #command: ["/apps/tomcat/bin/run_tomcat.sh"] #容器启动执行的命令或脚本
        #imagePullPolicy: IfNotPresent
        imagePullPolicy: Always #拉取镜像策略
        ports: #定义容器端口列表
        - containerPort: 80 #定义一个端口
          protocol: TCP #端口协议
          name: http #端口名称
        - containerPort: 443 #定义一个端口
          protocol: TCP #端口协议
          name: https #端口名称
        env: #配置环境变量
        - name: "password" #变量名称。必须要用引号引起来
          value: "123456" #当前变量的值
        - name: "age" #另一个变量名称
          value: "18" #另一个变量的值
        resources: #对资源的请求设置和限制设置
          limits: #资源限制设置，上限
            cpu: 500m  #cpu的限制，单位为core数，可以写0.5或者500m等CPU压缩值
            memory: 2Gi #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数
          requests: #资源请求的设置
            cpu: 200m #cpu请求数，容器启动的初始可用数量,可以写0.5或者500m等CPU压缩值
            memory: 512Mi #内存请求大小，容器启动的初始可用数量，用于调度pod时候使用
    
    ---
kind: Service #类型为service
apiVersion: v1 #service API版本， service.apiVersion
metadata: #定义service元数据，service.metadata
  labels: #自定义标签，service.metadata.labels
    app: linux36-nginx #定义service标签的内容
  name: linux36-nginx-spec #定义service的名称，此名称会被DNS解析
  namespace: linux36 #该service隶属于的namespaces名称，即把service创建到哪个namespace里面
spec: #定义service的详细信息，service.spec
  type: NodePort #service的类型，定义服务的访问方式，默认为ClusterIP， service.spec.type
  ports: #定义访问端口， service.spec.ports
  - name: http #定义一个端口名称
    port: 80 #service 80端口
    protocol: TCP #协议类型
    targetPort: 80 #目标pod的端口
    nodePort: 30001 #node节点暴露的端口
  - name: https #SSL 端口
    port: 443 #service 443端口
    protocol: TCP #端口协议
    targetPort: 443 #目标pod端口
    nodePort: 30043 #node节点暴露的SSL端口
  selector: #service的标签选择器，定义要访问的目标pod
app: linux36-nginx #将流量路到选择的pod上，须等于Deployment.spec.selector.matchLabels
```

### 2.1.1 k8s案例

容器优势

* 提高资源利用率、节约部署IT成本
* 提高部署效率，基于kubernetes实现快速部署交付、秒级启动
* 实现横向狂容、灰度部署、回滚等。
* 可根据业务负载进行弹性扩展
* 容器将环境打包在镜像内，保证了测试于生产环境的一致性



### 2.1.2：镜像分层构建

![image-20220306200007861](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220306200007861.png)



### 2.1.3：Nginx+Tomcat+NFS实现动静分离

### 2.1.4：示例图

![image-20220306204217881](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220306204217881.png)



### 2.1.5：centos基础镜像构建制作

```bash
root@s3:/data/image# cat Dockerfile 
FROM centos:7.7.1908
RUN yum -y install epel-release 
RUN yum -y install vim wget lrzsz tree gcc gcc-c++ zlib zlib-devel openssl openssl-devel net-tools unzip ntpdate nfs-utils telnet
RUN rm -rf /etc/localtime && ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
```

#### 2.1.5.1: build-command脚本

基于脚本实现自动build及上传到harbor功能

```bash
root@s3:/data/image# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/centos:7.7.1908 .

sleep 1
docker push harbor.lcy.net/base/centos:7.7.1908

root@s3:/data/image# bash docker-command.sh
```

### 2.1.6: Nginx业务镜像制作

基于Nginx基础镜像，制作N个不同服务的Nginx业务镜像

#### 2.1.6.1：Dockerfile文件内容

```bash
root@s3:/data/image/nginx# cat Dockerfile 
FROM harbor.lcy.net/base/centos:7.7.1908

ADD nginx-1.18.0.tar.gz /usr/local/src/

RUN useradd nginx -u 1000

WORKDIR /usr/local/src/nginx-1.18.0

RUN  ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --with-stream --with-stream_ssl_module --with-stream_realip_module && make && make install

RUN  mkdir /usr/local/nginx/web/portal -p

ADD portal.tar.gz /usr/local/nginx/web/portal/

ADD nginx.conf /usr/local/nginx/conf/nginx.conf

ADD index.html /usr/local/nginx/html/index.html

RUN chown -R nginx.nginx /usr/local/nginx

EXPOSE 80 443

CMD ["/usr/local/nginx/sbin/nginx"]

```

#### 2.1.6.2: build-command脚本

```bash
root@s3:/data/image/nginx# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/nginx-web1:v1 .
sleep 1
docker push harbor.lcy.net/base/nginx-web1:v1

root@s3:/data/image/nginx# bash docker-command.sh
```

#### 2.1.6.3：文件内容

```bash
root@s3:/data/image/nginx# cat nginx.conf 
user  nginx nginx;
worker_processes  auto;


#pid        logs/nginx.pid;eee
daemon off;

events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;



    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html;
            index  index.html index.htm;
        }

        location /portal {
            root   /usr/local/nginx/web;
            index  index.html index.htm;
        }



    }

}

root@s3:/data/image/nginx# cat index.html 
web app1
```

#### 2.1.6.4: 测试nginx业务镜像可以启动为容器

```bash
root@s3:/data/image/nginx# docker run  --rm --name nginx -p 80:80 harbor.lcy.net/base/nginx-web1:v1
```

#### 2.1.6.5: 访问nginx业务web页面

![image-20220603182810935](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603182810935.png)



![image-20220603182829958](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603182829958.png)

#### 2.1.6.7： nginx业务yaml构建

```
#创建pnginx
root@s3:/data/deployment/nginx# cat nginx.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx-label
  namespace: bjsw
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      containers:
      - name: nginx-contaeiners
        image: harbor.lcy.net/base/nginx-web1:v1
        imagePullPolicy: IfNotPresent
        ports:
         - containerPort: 80
           protocol: TCP
           name: http
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: bjsw
  labels:
    app: nginx-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30004
  selector:
    app: nginx-label

    
root@s3:/data/deployment/nginx# kubectl apply -f nginx.yml
#查看Pod
root@s3:/data/deployment/nginx# kubectl get po -n bjsw
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-6fb5d459bf-dtpnb   1/1     Running   0          11m

#查看service
root@s3:/data/deployment/nginx# kubectl get svc -n bjsw
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
nginx-service   NodePort   10.100.21.103   <none>        80:30004/TCP   11m

#查看ep
root@s3:/data/deployment/nginx# kubectl get ep -n bjsw
NAME            ENDPOINTS           AGE
nginx-service   10.200.152.199:80   12m

```

#### 2.1.6.8： 验证

![image-20220603190604608](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603190604608.png)

### 2.1.7运行tomcat

基于基础的Centos镜像，制作公司基础镜像--jdk镜像--tomcat基础镜像--tomcat业务镜像

#### 2.1.7.1：JDK基础镜像制作



```bash
root@s3:/data/image/jdk# cat Dockerfile 
FROM harbor.lcy.net/base/centos:7.7.1908

ADD jdk-8u191-linux-x64.tar.gz /usr/local/src/

RUN ln -sv /usr/local/src/jdk1.8.0_191 /usr/local/jdk
RUN  useradd tomcat -u 1000 

ADD profile /etc/profile

ENV JAVA_HOME /usr/local/jdk
ENV JRE_HOME $JAVA_HOME/jre
ENV CLASSPATH $JAVA_HOME/lib/:$JRE_HOME/lib/
ENV PATH $PATH:$JAVA_HOME/bin
```

#### 2.1.7.2: profile文件内容

```bash
root@s3:/data/image/jdk# ls
docker-command.sh  Dockerfile  jdk1.8.0_191  jdk-8u191-linux-x64.tar.gz  profile
root@s3:/data/image/jdk# cat profile 
# /etc/profile: system-wide .profile file for the Bourne shell (sh(1))
# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).

if [ "${PS1-}" ]; then
  if [ "${BASH-}" ] && [ "$BASH" != "/bin/sh" ]; then
    # The file bash.bashrc already sets the default PS1.
    # PS1='\h:\w\$ '
    if [ -f /etc/bash.bashrc ]; then
      . /etc/bash.bashrc
    fi
  else
    if [ "`id -u`" -eq 0 ]; then
      PS1='# '
    else
      PS1='$ '
    fi
  fi
fi

if [ -d /etc/profile.d ]; then
  for i in /etc/profile.d/*.sh; do
    if [ -r $i ]; then
      . $i
    fi
  done
  unset i
fi
export LANG=en_US.UTF-8
JAVA_HOME=/usr/local/jdk
CLASSPATH=.:$JAVA_HOME/bin/tools.jar
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME        CLASSPATH PATH

```

#### 2.1.7.3: build-command脚本

```bash
root@s3:/data/image/jdk# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/jdk:v8 .
sleep 1
docker push harbor.lcy.net/base/jdk:v8

root@s3:/data/image/jdk# bash docker-command.sh
```



#### 2.1.7.4: 验证jdk镜像启动为容器后的java环境

```bash
root@s3:/data/image/jdk# docker run -it --rm --name tomcat harbor.lcy.net/base/jdk:v8 bash
[root@d7edc404c313 jdk]# java -version
java version "1.8.0_191"
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)
```



### 2.1.8: tomcat基础镜像制作

#### 2.1.8.1：Dockerfile文件内容

```bash
root@s3:/data/image/tomcat# cat Dockerfile 
FROM harbor.lcy.net/base/jdk:v8

RUN mkdir /apps

ADD apache-tomcat-9.0.62.tar.gz /apps

RUN ln -sv /apps/apache-tomcat-9.0.62 /apps/tomcat && chown -R tomcat.tomcat /apps

```

#### 2.1.8.2：build-command脚本

```bash
root@s3:/data/image/tomcat# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/tomcat:8.0 .
sleep 1
docker push harbor.lcy.net/base/tomcat:8.0
```



#### 2.1.8.3: 测试访问tomcat基本镜像启动为容器

```bash
root@s3:/data/image/tomcat# dockworker-it --rm --name tomcat -p 8080:8080 harbor.lcy.net/base/tomcat:8
[root@7f5c9e00aa50 ]# cd /apps/tomcat/bin
[root@7f5c9e00aa50 bin]# ./startup.sh 
Using CATALINA_BASE:   /apps/tomcat
Using CATALINA_HOME:   /apps/tomcat
Using CATALINA_TMPDIR: /apps/tomcat/temp
Using JRE_HOME:        /usr/local/jdk/jre
Using CLASSPATH:       /apps/tomcat/bin/bootstrap.jar:/apps/tomcat/bin/tomcat-juli.jar
Using CATALINA_OPTS:   
Tomcat started.

```



![image-20220603221805650](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603221805650.png)



### 2.1.9: tomcat业务镜像app1制作

#### 2.1.9.1：Dockerfile文件内容

```bash
root@s3:/data/image/web# cat Dockerfile 
FROM harbor.lcy.net/base/tomcat:8.0

RUN mkdir /apps/tomcat/webapps/hmccmfront

ADD hmccmfront.tar /apps/tomcat/webapps/hmccmfront

ADD run_tomcat.sh /apps/tomcat/bin/run_tomcat.sh

RUN chmod +x  /apps/tomcat/bin/run_tomcat.sh && chown -R tomcat.tomcat /apps

EXPOSE 8080

CMD ["/apps/tomcat/bin/run_tomcat.sh"]
```



#### 2.1.9.2: run_tomcat.sh脚本内容

```bash
root@s3:/data/image/web# cat run_tomcat.sh 
#!/bin/bash
su - tomcat -c "/apps/tomcat/bin/catalina.sh start"
tail -f /etc/hosts
```

#### 2.1.9.3: build-command脚本

```bash
root@s3:/data/image/web# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/tomcat-app:v1 .
sleep 1
docker push harbor.lcy.net/base/tomcat-app:v1

root@s3:/data/image/web# bash docker-command.sh
```



#### 2.1.9.4: 测试tomcat业务镜像启动为容器

```bash
root@s3:/data/image/web# docker run  --rm --name tomcat -p 8080:8080 harbor.lcy.net/base/tomcat-app:v1
Tomcat started.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.2	0cbf1e8e82d5

```





#### 2.1.9.5: 访问tomcat业务镜像web页面



![image-20220603231239474](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220603231239474.png)



### 2.1.10 在k8s环境运行tomcat

#### 2.1.10.1 tomcat YAML文件内容

```bash
root@s3:/data/deployment/tomcat# cat tomcat.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deploy
  namespace: bjsw
  labels:
    app: tomcat-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat-label
  template:
    metadata:
      labels:
        app: tomcat-label
    spec:
      containers:
      - name: tomcat-containers
        image: harbor.lcy.net/base/tomcat-app:v1
        ports:
        - containerPort: 8080
          name: http
---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
  namespace: bjsw
  labels:
    app: tomcat-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 30007
  selector:
    app: tomcat-label

root@s3:/data/deployment/tomcat# kubectl apply -f tomcat.yaml 
deployment.apps/tomcat-deploy created
service/tomcat-service created


root@s3:~# kubectl get po -n bjsw -o wide
NAME                                READY   STATUS    RESTARTS   AGE     IP               NODE             NOMINATED NODE   READINESS GATES
nginx-deployment-6fb5d459bf-dtpnb   1/1     Running   0          5h8m    10.200.152.199   192.168.48.169   <none>           <none>
tomcat-deploy-599785f75b-m598z      1/1     Running   0          4m20s   10.200.78.131    192.168.48.170   <none>           <none>
root@s3:~# 
root@s3:~# kubectl get svc -n bjsw
NAME             TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
nginx-service    NodePort   10.100.21.103    <none>        80:30004/TCP     5h8m
tomcat-service   NodePort   10.100.253.221   <none>        8080:30007/TCP   4m8s

```

#### 2.1.10.2 测试访问tomcat业务pod的nodeport

![image-20220604000425079](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220604000425079.png)



### 2.1.11: k8s中nginx+tomcat实现动静分离

实现一个通用的Nignx+tomcat动静分离web架构，既用户访问的静态页面和图片在由nginx直接响应，而动态请求基于location转发致tomcat

#### 2.1.11.1：nginx业务镜像配置

#### 2.1.11.2： nginx配置文件

```bash
root@s3:/data/image/nginx# cat nginx.conf 
user  nginx nginx;
worker_processes  auto;


#pid        logs/nginx.pid;
daemon off;

events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;
upstream tomcat-service {
	server	tomcat-service.bjsw.svc.magedu.local:8080;
	}


    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html;
            index  index.html index.htm;
        }

        location /portal {
            root   /usr/local/nginx/web;
            index  index.html index.htm;
        }

	location /hmccmfront {
 	 proxy_pass http://tomcat-service;
	 proxy_set_header Host $host;
	 proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
         proxy_set_header X-Real-IP $remote_addr;
	}

    }

}

```

#### 2.1.11.3: build-command文件内容

```bash
root@s3:/data/image/nginx# cat docker-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/nginx-web1:v2 .
sleep 1
docker push harbor.lcy.net/base/nginx-web1:v2

root@s3:/data/image/nginx#bash docker-command.sh 
```

#### 2.1.11.4: 删除并重新构建nginx pod

```bash
root@s3:/data/deployment/nginx# kubectl delete -f nginx.yml 
deployment.apps "nginx-deployment" deleted
service "nginx-service" deleted

root@s3:/data/deployment/nginx# cat nginx.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx-label
  namespace: bjsw
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      containers:
      - name: nginx-contaeiners
        image: harbor.lcy.net/base/nginx-web1:v2
        imagePullPolicy: IfNotPresent
        ports:
         - containerPort: 80
           protocol: TCP
           name: http
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: bjsw
  labels:
    app: nginx-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30004
  selector:
    app: nginx-label

root@s3:/data/deployment/nginx# kubectl apply -f nginx.yml 
deployment.apps/nginx-deployment created
service/nginx-service created

```

#### 2.1.11.5: 获取当前po

```bash
root@s3:/data/image/nginx# kubectl get po -n bjsw
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-78464fc9d8-fvm5n   1/1     Running   0          11m
tomcat-deploy-599785f75b-m598z      1/1     Running   0          14h

```

#### 2.1.11.6: web访问测试

验证能否访问nignx portal图片

![image-20220604141917530](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220604141917530.png)

验证能否通过nginx访问到tomcat hmccfront项目



![image-20220604142003870](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220604142003870.png)







## 2.2 Volume数据卷

Why: 数据和镜像解耦，以及容器间的数据共享

What: K8s抽象出的一个对象用来保存数据，做存储用

常用的几种卷：

* emptyDir: 本地临时卷

* hostPath: 本地卷

* nfs：共享卷

* configmap：配置文件

  https://kubernetes.io/zh/docs/concepts/storage/volumes/



### 2.2.3 emptyDir

当Pod被分配给节点时，首先创建emptyDir卷，并且只要该Pod在该节点上运行，该卷就会存在，正如卷的名字所述，它最初时空的，pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管可以挂载到每个容器中的相同或不通路径上，当出于任何原因从节点中删除Pod时，emptyDir中的数据将永久删除

```
root@k8s-master1:~/pod/volume# cat emptydir.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: emptydir-deployment
  namespace: n60
  labels:
    app: emptydir-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: emptydir-label
  template:
    metadata:
      labels:
        app: emptydir-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /cache
          name: emptydir-volume
      volumes:
      - name: emptydir-volume
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: emptydir-service
  namespace: n60
  labels:
    app: emptydir-label
spec:
  type: NodePort
  ports:
  - name: htpp
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: emptydir-label
root@k8s-master1:~/pod/volume# kubectl apply -f emptydir.yaml 
deployment.apps/emptydir-deployment unchanged
service/emptydir-service created

root@k8s-master1:~/pod/volume# kubectl get pod -o wide -n n60
NAME                                   READY   STATUS    RESTARTS   AGE     IP             NODE             NOMINATED NODE   READINESS GATES
emptydir-deployment-6546bbb7ff-z974q   1/1     Running   0          2m38s   10.200.36.87   192.168.48.164   <none>           <none>

#emptydir路径
root@k8s-node1:/var/lib/kubelet/pods/ac454c9d-c579-4154-bf7e-9b3897f01966/volumes/kubernetes.io~empty-dir# ls
emptydir-volume

#进入容器
root@k8s-master1:~/pod/volume# kubectl exec -it emptydir-deployment-6546bbb7ff-z974q /bin/bash -n n60

root@emptydir-deployment-6546bbb7ff-z974q:/# cd /cache
root@emptydir-deployment-6546bbb7ff-z974q:/cache# touch n60.log
root@emptydir-deployment-6546bbb7ff-z974q:/cache# echo "web app1" > n60.log 
#宿主机验证 
#pwd
/var/lib/kubelet/pods/1c46c085-da33-4126-bb7b-49bf7837f392/volumes/kubernetes.io~empty-dir/emptydir-volume/
mpty-dir/emptydir-volume# cat n60.log 
web app1

#容器删除  临时目录不存在
root@k8s-master1:~/pod/volume# kubectl delete -f emptydir.yaml 
deployment.apps "emptydir-deployment" deleted
service "emptydir-service" deleted

```

### 2.2.4 hostPath

hostPath卷将主机节点的文件相同中的文件或目录挂载到集群中，pod删除的时候，卷不会被删除

```
root@k8s-master1:~/pod/volume# cat hostpath.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostpath-deployment
  namespace: n60
  labels:
    app: hostpath-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hostpath-label
  template:
    metadata:
      labels:
        app: hostpath-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /data
          name: hostpath-volume
      volumes:
      - name: hostpath-volume
        hostPath:
          path: /tmp/n60
---
apiVersion: v1
kind: Service
metadata:
  name: hostpath-service
  namespace: n60
  labels:
    app: hostpath-service
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: hostpath-label

root@k8s-master1:~/pod/volume# kubectl apply -f hostpath.yaml 
deployment.apps/hostpath-deployment created
service/hostpath-service created

root@k8s-master1:~/pod/volume# kubectl get po -o wide -n n60
NAME                                   READY   STATUS    RESTARTS   AGE   IP             NODE             NOMINATED NODE   READINESS GATES
hostpath-deployment-75b9d7d467-r45b6   1/1     Running   0          37s   10.200.36.88   192.168.48.164   <none>           <none>

#node节点查看
root@k8s-node1:/tmp/n60# pwd
/tmp/n60

#进入容器写入数据
root@k8s-master1:~/pod/volume# kubectl exec -it hostpath-deployment-75b9d7d467-r45b6 /bin/bash -n n60
root@hostpath-deployment-75b9d7d467-r45b6:/# cd /data/
root@hostpath-deployment-75b9d7d467-r45b6:/data# ls
root@hostpath-deployment-75b9d7d467-r45b6:/data# touch n60.txt
root@hostpath-deployment-75b9d7d467-r45b6:/data# echo "web app1" > n60.txt 

#宿主机验证
root@k8s-node1:/tmp/n60# cat n60.txt 
web app1

#删除容器
root@k8s-master1:~/pod/volume# kubectl delete -f hostpath.yaml 
deployment.apps "hostpath-deployment" deleted
service "hostpath-service" deleted

#宿主机验证 数据还在
root@k8s-node1:/tmp/n60# cat n60.txt 
web app1

```

### 2.2.5 nfs等共享存储

nfs卷允许将现有的nfs（网络文件相同）共享挂载到你的容器中，不像emptydir,当删除pod时。nfs卷的内容被保留，卷仅仅时被卸载，这意味着nfs卷可以与填充数据，并且可以在pod之间切换数据，可以被多个写入者同时挂载

* 创建多个pod测试挂载同一个nfs
* 创建多个pod测试每个pod挂载多个nfs

```
#nfs服务器安装nfs
[root@k8s-habor ~]# yum -y install nfs-utils
[root@k8s-habor ~]# mkdir /data/k8sdata -p
[root@k8s-habor ~]# vim /etc/exports
#添加以下内容
/data/k8sdata *(rw,no_root_squash)
[root@k8s-habor ~]# systemctl restart nfs-server

验证：
root@k8s-node1:~# showmount -e 192.168.48.167
Export list for 192.168.48.167:
/data/k8sdata *

root@k8s-master1:~/pod/volume# cat nfs.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-deployment
  namespace: n60
  labels:
    app: nfs-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-label
  template:
    metadata:
      labels:
        app: nfs-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /usr/share/nginx/html/mysite
          name: nfs-volume
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.48.167
          path: /data/k8sdata
---
apiVersion: v1
kind: Service
metadata:
  name: nfs-server
  namespace: n60
  labels:
    app: nfs-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: nfs-label

root@k8s-master1:~/pod/volume# kubectl apply -f nfs.yaml 
deployment.apps/nfs-deployment created
service/nfs-server created

#进入容器
root@k8s-master1:~/pod/volume# kubectl exec -it nfs-deployment-6f679df68b-x4w2j bash -n n60

#查看挂载目录
root@nfs-deployment-6f679df68b-x4w2j:/# df -h
Filesystem                    Size  Used Avail Use% Mounted on
overlay                        49G   11G   37G  23% /
tmpfs                          64M     0   64M   0% /dev
tmpfs                         914M     0  914M   0% /sys/fs/cgroup
/dev/sda1                      49G   11G   37G  23% /etc/hosts
shm                            64M     0   64M   0% /dev/shm
192.168.48.167:/data/k8sdata   17G  6.0G   12G  35% /usr/share/nginx/html/mysite
tmpfs                         1.5G   12K  1.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                         914M     0  914M   0% /proc/acpi
tmpfs                         914M     0  914M   0% /proc/scsi
tmpfs                         914M     0  914M   0% /sys/firmware


#存储服务器上放入图片 
[root@k8s-habor k8sdata]# ll
total 408
-rw-r--r-- 1 root root 413718 Sep  6  2020 1.jpg

#浏览器验证
```

http://192.168.48.99:30005/mysite/1.jpg

![image-20220304215757433](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220304215757433.png)



#### 2.2.5.1 多个pod挂载nfs

dashboard操作

![image-20220304220205328](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220304220205328.png)

#多个Pod都会挂载到同一个nfs数据目录中

```
#进入刚创建的pod查看
```

![image-20220304220501189](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220304220501189.png)



#### 2.2.5.2 pod挂载多个nfs

```
root@k8s-master1:~/pod/volume# cat nfs1.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs1-deployment
  namespace: n60
  labels:
    app: nfs1-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs1-label
  template:
    metadata:
      labels:
        app: nfs1-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nfs-volume
          mountPath: /usr/share/nginx/html/mysite
        - name: nfs-statics
          mountPath: /data/nginx/statics
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.48.167
          path: /data/k8sdata
      - name: nfs-statics
        nfs:
          server: 192.168.48.148
          path: /data/k8sdata/statics
---
apiVersion: v1
kind: Service
metadata:
  name: nfs1-server
  namespace: n60
  labels:
    app: nfs1-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: nfs1-label
root@k8s-master1:~/pod/volume# kubectl apply -f nfs1.yaml 
deployment.apps/nfs1-deployment created
service/nfs1-server created

```

![image-20220304223143226](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220304223143226.png)



```
#存储放入图片
[root@haproxy1 statics]# ls
1.jpg
[root@haproxy1 statics]# pwd
/data/k8sdata/statics

#进入容器 配置nginx
oot@nfs1-deployment-55cb9dc8bf-7t9mk:/etc/nginx/conf.d# pwd
/etc/nginx/conf.d
root@nfs1-deployment-55cb9dc8bf-7t9mk:/etc/nginx/conf.d# vim default.conf
#添加以下内容
  location /statics {
         index index.html index.htm;
        root /data/nginx;

}

#检查语法有没有报错
root@nfs1-deployment-55cb9dc8bf-7t9mk:/etc/nginx/conf.d# nginx -t 
root@nfs1-deployment-55cb9dc8bf-7t9mk:/etc/nginx/conf.d# nginx -s reload

#验证
```

![image-20220304224401307](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220304224401307.png)



### 2.2.6 Configmap

应用部署的一个最佳实践，就是将应用所需的配置信息与程序进行分离，kubernetes 提供了一种的集群配置管理方案,即 ConfigMap，就是将一些环境变量或者配置文件定义为 configmap，放在 kubernetes 中，可以让其他 pod 调用。

- 生成为容器内的环境变量

- 设置容器启动命令的启动参数（需设置为环境变量）

- 以 volume 的形式挂载为容器内部的文件或目录

- ConfigMap 必须在pod之前创建

- ConfigMap 也可以定于属于某个 NameSpace，只有处于相同NameSpace的pod可以应用它

- ConfigMap 中的配额管理还未实现

- 如果是volume的形式挂载到容器内部，只能挂载到某个目录下，该目录下原有的文件会被覆盖掉

- 静态pod不能用configmap(静态 pod 不受API server 管理)

#### 2.2.6.1: 命令行帮助

```bash
root@s3:~# kubectl create configmap --help

```

#### 2.2.6.2: 命令行创建

```bash
root@s3:~# kubectl create configmap nginx --from-literal=nginx=80 --from-literal=tomcat=8080
configmap/nginx created
root@s3:~# kubectl describe configmap nginx
Name:         nginx
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
nginx:
----
80
tomcat:
----
8080

BinaryData
====

root@s3:~# kubectl get cm
NAME               DATA   AGE
nginx              2      114s
```

#### 2.2.6.3: 文件创建

注：指定文件名当键，文件内容当值

```
root@s3:~# kubectl create configmap nginx-www --from-file=configmap/www.conf 
configmap/nginx-www created
root@s3:~# kubectl get cm nginx-www -o yaml
apiVersion: v1
data:
  www.conf: "server {\n\tlisten 80;\n\tserver_name www.lcy.com\n\troot /root/configmap;\n\n\t}\n"
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-12T12:45:04Z"
  name: nginx-www
  namespace: default
  resourceVersion: "213843"
  uid: e13a9f1f-cac4-4177-bbf6-ce8365594088

```

#### 2.2.6.4: 使用ConfigMap(使用env的形式)

```bash
root@s3:/apps# cat configmap.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: ikubernetes/myapp:v1
    ports:
    - name: http
      containerPort: 80
    env:
    - name: nginx-env
      valueFrom:
        configMapKeyRef:
          name: nginx
          key: nginx
    - name: tomcat-env
      valueFrom:
        configMapKeyRef:
          name: nginx
          key: tomcat


#进入容器验证
root@s3:/apps# kubectl exec -it -u root nginx-pod -- /bin/sh
/ # env
....
tomcat-env=8080
nginx-env=80
....
```

#### 2.2.6.5 configmap环境变量

####  

```bash
root@k8s-master1:~/pod/volume# cat configmap-env.yml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-configmap
data:
  username: user1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deployment
  labels:
    app: ng-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        env:
        - name: MY_USERNAME
          valueFrom:
            configMapKeyRef:
              name: ng-configmap
              key: username
root@k8s-master1:~/pod/volume# kubectl apply -f configmap-env.yml 
configmap/ng-configmap configured
deployment.apps/ng-deployment created
#进入容器验证
```

![image-20220305172024928](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305172024928.png)





#### 2.2.6.6: 使用ConfigMap(使用volumeMount的形式)

```bash
root@k8s-master1:~/pod/volume# cat configmapp.yml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-configmap
  namespace: n60
data:
  default: |
    server {
      listen    80;
      server_name 192.168.48.99;
      index       index.html;

     location / {
        root /data/nginx/html;
        if (!-e $request_filename) {
                rewrite ^/(.*) /index.html last;
        }
     } 
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deployment
  namespace: n60
  labels:
    app: ng-label
spec:
  replicas:
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /etc/nginx/conf.d
          name: ng-configmap
        - name: ng-nfs
          mountPath: /data/nginx/html
      volumes:
      - name: ng-nfs
        nfs:
          server: 192.168.48.167
          path: /data/k8sdata
      - name: ng-configmap
        configMap:
          name: ng-configmap
          items:
             - key: default
               path: mysite.conf
---
apiVersion: v1
kind: Service
metadata:
  name: ng-service
  namespace: n60
  labels:
    app: ng-label
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30005
    targetPort: 80
  selector:
    app: ng-label

deployment.apps/ng-deployment created
service/ng-service created

#进入容器验证
```

![image-20220305163645856](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305163645856.png)

![image-20220305163831973](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305163831973.png)



#### 2.2.6.7: secret

secret：一个 secret volume 用于为 Pod 提供加密的信息，可以将定义在 Kubernetes 中的 secret 直接挂载为文件让 Pod 访问。Secret volume 是通过 tmfs（内存文件系统）实现的，所以这种类型的 volume 不会持久化

Secret的功能类似于ConfigMap给pod提供额外的配置信息，但是Secret是一种包含少量敏感信息例如密码，令牌或密钥的对象

Secret的名称必须是合法的DNS子域名

每个Secret的大小最多为1MIB主要是为了避免用户创建非常大的Secret进而导致API服务器和kubelet内存耗尽，不过创建很多小的Secret也可能耗尽内存，可以使用资源配额来约束每个名字空间中的Secret的个数

在通过yaml文件创建secret时，可以设置data或stringData字段，data和stringData字段都是可选的，data字段中所有键值都必须时base64编码的字符串，如果不希望执行这种base64字符串的转换操作，也可以选择设置stringData字段，其中可以使用任何非加密的字符串作为取值

Pod可以用三种方式的任意一种来使用Secret

作为挂载到一个或多个容器上的卷中的文件（crt文件、key文件）

作为容器的环境变量

由kubelet在为pod拉取镜像时使用（与镜像仓库的认证）



#### 2.2.6.8: Secret简介类型：

kubernetes默认支持多种不同类型的secret，用于一不同的使用常见，不同类型的secret的配置参数也不一样

| Secret类型                          | 使用场景                              |
| ----------------------------------- | ------------------------------------- |
| Opaque                              | 用户定义的任意数据                    |
| kubernetes.io/service-account-token | ServiceAccount令牌                    |
| kubernetes.io/dockercfg             | ~/.dockercfg 文件的序列化形式         |
| kubernetes.io/dockerconfigjson      | ~/.docker.config.json文件的序列化形式 |
| kubernetes.io/basic-auth            | 用于基本身份认证的凭据                |
| kubernetes.io/ssh-auth              | 用于SSH身份证的凭据                   |
| kubernetes.io/tls                   | 用于TLS环境，保存crt证书和key证书     |
| bootstrap.kubernetes.io/token       | 启动引导令牌数据                      |



```bash
root@s3:/apps# kubectl create secret --help
Create a secret using specified subcommand.

Available Commands:
  docker-registry Create a secret for use with a Docker registry #docker私有仓库使用
  generic         Create a secret from a local file, directory, or literal value #其他使用
  tls             Create a TLS secret  #放入证书私钥使用

Usage:
  kubectl create secret [flags] [options]
```

#### 2.2.6.9：Secret类型-Opaque格式:

Opaque格式-data类型数据-先使用base64加密

```bash
echo admin | base64
echo 123456 | base64
#创建secret
root@s3:/data/secret# cat data.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret-data
  namespace: n60
type: Opaque
data:
  password: MTIzNDU2
  user: YWRtaW4=
```

Opaque格式straingData类型数据-不用事先加密-上传到k8s会加密

```bash
root@s3:/data/secret# cat stringData.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret-stringdata
  namespace: n60
type: Opaque
stringData:
  superuser: 'admin'
  password: '123456'
```

```bash
root@s3:/data/secret# cat nginx.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-daemon
  namespace: n60
spec:
  containers:
  - name: http
    image: ikubernetes/myapp:v1
    volumeMounts:
    - name: auth-secret
      mountPath: /data/auth
  volumes:
  - name: auth-secret
    secret:
      secretName: mysecret-data

root@s3:/data/secret# kubectl exec -it pod-daemon sh -n n60

/ # cat /data/auth/password 
123456/ # 
/ # cat /data/auth/user 
admin/ # 

```

#### 2.2.6.10：Secret的挂载流程

```bash
root@s3:/data/secret# etcdctl get / --keys-only --prefix | grep n60
/registry/secrets/n60/mysecret-data
root@s3:/data/secret# etcdctl get /registry/secrets/n60/mysecret-data

root@s1:~# cat /var/lib/kubelet/pods/758b1bb6-bbc3-4217-ba2c-51e63d78f77a/volumes/kubernetes.io~secret/auth-secret/user
admin

root@s1:~# cat /var/lib/kubelet/pods/758b1bb6-bbc3-4217-ba2c-51e63d78f77a/volumes/kubernetes.io~secret/auth-secret/password
123456

```

#### 2.2.6.11 Secret类型-kubernetes.io/tls-为nginx提供证书示例：

```bash
#自签名证书
root@s3:~# mkdir certs
root@s3:~# cd certs/
root@s3:~/certs# openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 3560 -nodes -subj '/CN=www.ca.com'
root@s3:~/certs# openssl req -new -newkey rsa:4096 -keyout server.key -out server.csr -nodes -subj '/CN=www.mysite.com
root@s3:~/certs# openssl x509 -req -sha256 -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt
root@s3:~/certs# kubectl create secret tls myserver-tls-key --cert=./server.crt --key=./server.key -n n60




root@s3:~/certs# cat nginx.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-configmap
  namespace: n60
data:
  default: |
    server {
      listen    80;
      server_name www.mysite.com;
      listen 443 ssl;
      ssl_certificate /etc/nginx/conf.d/certs/tls.crt;
      ssl_certificate_key /etc/nginx/conf.d/certs/tls.key;

     location / {
        root /usr/share/nginx/html;
        index index.html;
        if ($scheme = http) {
                rewrite / https://www.mysite.com permanent;
        }
        
        if (!-e $request_filename) {
            rewrite ^/(.*) /index.html last;
        }
     } 
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myserver-myapp
  namespace: n60
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-pod
        image: nginx:1.20.2-alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d/myserver
        - name: myserver-tls-key
          mountPath: /etc/nginx/conf.d/certs
      volumes:
      - name: nginx-config
        configMap:
          name: ng-configmap
          items:
          - key: default
            path: mysite.conf
      - name: myserver-tls-key
        secret:
          secretName: myserver-tls-key
---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp
  namespace: n60
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30018
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    nodePort: 30019
    protocol: TCP
  selector:
    app: myapp
root@s3:~/certs# kubectl apply -f nginx.yaml 
configmap/ng-configmap unchanged
deployment.apps/myserver-myapp created
service/myserver-myapp unchanged

root@s3:~/certs# kubectl exec -it myserver-myapp-fdf4f5dcf-jdmvp -n n60 sh
/etc/nginx # cd /etc/nginx/
/etc/nginx # vi nginx.conf
#最下面添加
    include /etc/nginx/conf.d/myserver/*.conf;
    
 /etc/nginx/conf.d/myserver # nginx -t
 /etc/nginx/conf.d/myserver # nginx -s reload
 
 /etc/nginx/conf.d/myserver # netstat -tlnp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      1/nginx: master pro
tcp        0      0 :::80                   :::*                    LISTEN      1/nginx: master pro



root@s3:~/certs# curl -lvk https://10.100.247.135:443
* Rebuilt URL to: https://10.100.247.135:443/
*   Trying 10.100.247.135...
* TCP_NODELAY set
* Connected to 10.100.247.135 (10.100.247.135) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: CN=www.mysite.com
*  start date: Aug  3 15:51:47 2022 GMT
*  expire date: Jul 31 15:51:47 2032 GMT
*  issuer: CN=www.ca.com
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
> GET / HTTP/1.1
> Host: 10.100.247.135
> User-Agent: curl/7.58.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< Server: nginx/1.20.2
< Date: Thu, 04 Aug 2022 13:25:04 GMT
< Content-Type: text/html
< Content-Length: 612
< Last-Modified: Tue, 16 Nov 2021 15:04:23 GMT
< Connection: keep-alive
< ETag: "6193c877-264"
< Accept-Ranges: bytes
< 

```

#### 2.2.6.12: 浏览器验证

https://192.168.48.166:30019



https://www.mysite.com

![image-20220804215001530](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220804215001530.png)

#### 2.2.6.13：命令行创建secret

```bash
root@s3:/apps# kubectl create secret generic mysql-password --from-literal=password=Mysql@123
secret/mysql-password created
root@s3:/apps# kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-2vfhh   kubernetes.io/service-account-token   3      10d
mysql-password        Opaque                                1      20s
root@s3:/apps# kubectl describe secret mysql-password
Name:         mysql-password
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
password:  9 bytes
```

#### 2.2.6.14: secret使用

```bash
root@s3:/apps# cat secret.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: secret-pod1
spec:
  containers:
  - name: secret
    image: ikubernetes/myapp:v1
    ports:
    - name: http
      containerPort: 80
    env:
    - name: secret
      valueFrom:
        secretKeyRef:
          name: mysql-password
          key: password

#进入容器验证
root@s3:/apps# kubectl exec -it secret-pod1 -- /bin/sh
/ # env
....
secret=Mysql@123
....
```

#### 2.2.6.15: secret解密

```bash
root@s3:/apps# kubectl get secret mysql-password -o yaml
apiVersion: v1
data:
  password: TXlzcWxAMTIz
kind: Secret
metadata:
  creationTimestamp: "2022-06-12T15:24:49Z"
  name: mysql-password
  namespace: default
  resourceVersion: "226990"
  uid: 138c5f90-e33b-45df-86fe-cfc6a5e89b22
type: Opaque
root@s3:/apps# echo TXlzcWxAMTIz | base64 -d
Mysql@123root@s3:/apps# 

```



### 2.2.7: Persistent Volume 持久卷

默认情况下容器中的磁盘文件时非持久化的，对于运行在容器中的应用来说面临两个问题，第一：当容器

容器挂掉kubelet将重新启动它时，文件将会丢失，第二：当pod中同时运行多个容器，容器之间需要共享文件时，kubernetes的volumes解决了这两个问题

https://kubernetes.io/zh/docs/concepts/storage #官方文档



```
PersistenVolume (PV)是集群中已由管理员配置中的一段网络存储，集群中的存储资源就像一个node节点是一个集群资源，pv是诸如卷之类的插件，但是具有独立使用pv的任何单个pod的生命周期，该API对象捕获存储的实现节点，既NFS，iscsi或云提供商特定的存储系统，pv是由管理员添加的一个存储的描述，是一个全局资源既不隶属于任何namespace，包含存储的类型，存储大小和访问模式等，它的生命周期独立于pod，例如当使用它的pod销毁时对pv没有任何影响

PersistentVolumeClaim (pvc)是用户存储的请求，它类似于pod，pod消耗节点资源,pvc消耗存储资源，就像pod可以请求特定级别的资源（cpu和内存），pvc是namespace中的资源，可以设置特定的空间大小和访问模式

kubernetes 从1.0版本开始支持persisentvolume和persistentvolumeclaim

pv是对底层网络存储的抽象，既将网络存储定义为一种存储资源，将一个整体的存储资源拆分为多分后给不同的业务使用
pvc是对pv资源的申请调度，就像pod消费节点资源一样，pod是通过pvc将数据保存致pv，pv在保存致存储
```





![image-20220312152635195](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220312152635195.png)

PersistentVolume参数：

```bash
#kubectl explain PersistentVolume
	accessModes: 访问模式 #kubectl explain PersistentVolume,spec.accessMode
		ReadWriteOnce - pv只能被单个节点以读写权限挂载，RWO
	    ReadOnlyMany - pv可以被多个节点挂载但是权限只是读的，ROX
	    ReadWriteMany - pv可以被多个节点是读写方式挂载使用，RWX
	    
	    
PersistentVolumeReclaimPolicy #删除机制既删除存储卷时候，已经创建好的存储卷由以下删除操作
#kubernetes explain PersistentVolume.spec.persistentVolumeReclaimPolicy
Retain - 删除pv后保存原装，最后需要管理员手动删除
Recycle - 空间回收，及删除存储卷上的所有数据（包括目录和隐藏文件），目前支持NFS和hostPath
Delete - 自动删除存储卷

VolumeMode #卷类型 kubectl explain PersistentVolume.spec.volumeMode
 定义存储卷使用的文件系统是块设备还是文件系统，默认为文件系统
 
 mountOptions #附加的挂载选项列表，实现更精细的权限控制
 ro #等
```

PersistentVolumeClaime创建参数：

```bash
kubectl explain PersistentVolumeClaim.

accessModes : PVC访问模式 #kubectl explain PersistentVolumeClaim.spec.volumeMode
   ReadWriteOnce - PVC只能被单个节点以读写权限挂载，RWO
   ReadOnlyMany - PVC以可以被多个节点挂载但是权限只是读的,ROX
   ReadWriteMany - PVC可以被多个节点是读写方式挂载使用，RWX
   
 resources: #定义pvc创建存储卷的空间大小
 
 selectro: #标签选择器，选择要绑定的PV
   matchLabels #匹配标签名称
   matchExpressions #基于正则表达式匹配
   
  volumeName #要绑定的Pv名称
  
 volumeMode #卷类型
    定义Pvc使用的文件系统时块设备还是文件系统，默认为文件系统
```



回收策略：

```bash
- ---pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。

 保留策略：允许人工处理保留的数据。
 删除策略：将删除pv和外部关联的存储资源，需要插件支持。
 回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。

 注：目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略
```

下面是NF类型的PV的yaml定义内容，生命了需要2G的存储空间

```bash
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistent-pv1
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /test/pv1
    server: 192.168.48.164
```

PV 的 accessModes 属性有以下类型：

- ReadWriteOnce：读写权限、并且只能被单个 Node 挂载。
- ReadOnlyMany：只读权限、允许被多个 Node 挂载。
- ReadWriteMany：读写权限、允许被多个 Node 挂载。

如果 Pod 想申请使用 PV 资源，则首先需要定义一个 PersistentVolumeClaim（PVC）对象

```bash
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: persistentvolumeClaim-pvc1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

```

然后在 Pod 的 volume 定义中引用上述 PVC 即可

```
volumes:
  - name: mypd
    persistentVolumeClaim:
      claimName: persistentvolumeClaim-pvc1
```

PV 是有状态的对象，它有以下几种状态：

- Available：空闲状态。
- Bound：已经绑定到某个 PVC 上。
- Released：对应的 PVC 已经删除，但资源还没有被集群收回。
- Failed：PV 自动回收失败。

#### 2.2.7.1: 创建PV

##### 2.2.7.1.1：准备nfs服务器

在nfs服务器上先建立存储卷对应的目录

```bash
root@s5:~# mkdir /volumes
root@s5:~# cd /volumes/
root@s5:/volumes# mkdir v{1,2,3,4,5}
root@s5:/volumes# ls
v1  v2  v3  v4  v5
root@s5:/volumes#  echo "<h1>NFS stor 01</h1>" > v1/index.html
root@s5:/volumes# echo "<h1>NFS stor 01</h1>" > v2/index.html
root@s5:/volumes# echo "<h1>NFS stor 01</h1>" > v3/index.html
root@s5:/volumes# echo "<h1>NFS stor 01</h1>" > v4/index.html
root@s5:/volumes# echo "<h1>NFS stor 01</h1>" > v5/index.html
```

修改nfs的配置

```bash
root@s5:/volumes# vim /etc/exports 
/volumes/v1 *(rw,no_root_squash)
/volumes/v2 *(rw,no_root_squash)
/volumes/v3 *(rw,no_root_squash)
/volumes/v4 *(rw,no_root_squash)
/volumes/v5 *(rw,no_root_squash)

```

配置生效

```
root@s5:/volumes# showmount -e
Export list for s5:
/volumes/v5 *
/volumes/v4 *
/volumes/v3 *
/volumes/v2 *
/volumes/v1 *

```

##### 2.2.7.1.2： master上创建pv

```bash
root@s3:/data/volume# cat persistentvolume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-pv1
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /volumes/v1
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-pv2
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /volumes/v2
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-pv3
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /volumes/v3
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-pv4
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /volumes/v4
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-pv5
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /volumes/v5
    server: 192.168.48.164

root@s3:/data/volume# kubectl apply -f persistentvolume.yaml 
persistentvolume/persistentvolume-pv1 unchanged
persistentvolume/persistentvolume-pv2 unchanged
persistentvolume/persistentvolume-pv3 created
persistentvolume/persistentvolume-pv4 created
persistentvolume/persistentvolume-pv5 created

```

查询验证

```bash
root@s3:/data/volume# kubectl get pv
NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
persistentvolume-pv1   2Gi        RWO            Retain           Available                                   4m2s
persistentvolume-pv2   2Gi        RWO            Retain           Available                                   2m39s
persistentvolume-pv3   2Gi        RWO            Retain           Available                                   98s
persistentvolume-pv4   2Gi        RWO            Retain           Available                                   98s
persistentvolume-pv5   2Gi        RWO            Retain           Available                                   98s

```

##### 2.2.7.1.3: 创建PVC、绑定PV

```bash
root@s3:/data/volume# cat persistentvolumeclaim.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: persistentvolumeclaim
spec:
  resources:
    requests:
      storage: 2Gi
  accessModes:
    - ReadWriteOnce
  volumeName: persistentvolume-pv1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  volumes:
  - name: html
    persistentVolumeClaim:
      claimName: persistentvolumeclaim
root@s3:/data/volume# kubectl apply -f persistentvolumeclaim.yaml 
persistentvolumeclaim/persistentvolumeclaim created
pod/pod created
```

查询验证：PVC已经绑定到PV01

```bash
root@s3:/data/volume# kubectl get pvc
NAME                    STATUS   VOLUME                 CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim   Bound    persistentvolume-pv1   2Gi        RWO                           38s
root@s3:/data/volume# kubectl get pv
NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                           STORAGECLASS   REASON   AGE
persistentvolume-pv1   2Gi        RWO            Retain           Bound       default/persistentvolumeclaim                           22m
persistentvolume-pv2   2Gi        RWO            Retain           Available                                                           21m
persistentvolume-pv3   2Gi        RWO            Retain           Available                                                           20m
persistentvolume-pv4   2Gi        RWO            Retain           Available                                                           20m
persistentvolume-pv5   2Gi        RWO            Retain           Available                                                           20m

```

查询业务验证

```bash
root@s3:/data/volume# kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS     AGE     IP               NODE             NOMINATED NODE   READINESS GATES
pod                                       1/1     Running   0            2m11s 
root@s3:/data/volume# curl 10.200.78.135
<h1>NFS stor 01</h1>

```

### 2.2.8: 实战案例之Redis

#### 2.2.8.1 构建redis镜像

#### 2.2.8.2 Docker文件内容

```bash
root@s3:/data/image/redis# cat Dockerfile 
FROM harbor.lcy.net/base/centos:7.7.1908

ADD redis-4.0.14.tar.gz /usr/local/src

RUN ln -sv  /usr/local/src/redis-4.0.14 /usr/local/redis && cd /usr/local/redis && make && cp src/redis-cli /usr/sbin/ && cp src/redis-server /usr/sbin/ && mkdir -pv /data/redis-data

ADD redis.conf /usr/local/redis/redis.conf
ADD run_redis.sh /usr/local/redis/run_redis.sh

EXPOSE 6379

CMD ["/usr/local/redis/run_redis.sh"]

```

#### 2.2.8.3 配置文件内容

```bash
root@s3:/data/image/redis# cat run_redis.sh 
#!/bin/bash

/usr/sbin/redis-server /usr/local/redis/redis.conf

tail -f  /etc/hosts


root@s3:~/data/image/redis# grep -v "#" redis.conf | grep -v "^$"
bind 0.0.0.0
protected-mode yes
port 6379
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes
supervised no
pidfile /var/run/redis_6379.pid
loglevel notice
logfile ""
databases 16
always-show-logo yes
save 900 1
save 5 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error no
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /data/redis-data
slave-serve-stale-data yes
slave-read-only yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-disable-tcp-nodelay no
slave-priority 100
requirepass 123456
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
slave-lazy-flush no
appendonly no
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble no
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events ""
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
aof-rewrite-incremental-fsync yes
```

#### 2.2.8.4: build-command文件内容

```bash
root@s3:/data/image/redis# cat build-command.sh 
#!/bin/bash
docker build -t harbor.lcy.net/base/redis:6.0 .
sleep 1
docker push harbor.lcy.net/base/redis:6.0


root@s3:~/data/image/redis# chmod a+x run_redis.sh
root@s3::~/data/image/redis# bash build-command.sh
```

#### 2.2.8.5: 运行redis服务

基于PV/PVC保存数据，实现K8S中运行Redis服务

#### 2.2.8.6： 创建PV于PVC

```bash
root@s3:/data/deployment/redis# cat persistentvolume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: persistentvolume-1
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /k8sdata/redis-1


root@s3:/data/deployment/redis# cat persistentvolumeclaim.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: persistentvolumeclaim
spec:
  volumeName: persistentvolume-1
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

```

#### 2.2.8.7: 验证PV于PVC

```bash
root@s3:~# kubectl get pv
NAME                 CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS   REASON   AGE
persistentvolume-1   2Gi        RWO            Retain           Bound    default/persistentvolumeclaim                           48m
root@s3:~# kubectl get pvc
NAME                    STATUS   VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim   Bound    persistentvolume-1   2Gi        RWO                           48m

```

#### 2.2.8.8: 运行redis服务

```bash
root@s3:/data/deployment/redis# cat redis.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-deploy
  labels:
    app: redis-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-label
  template:
    metadata:
      labels:
        app: redis-label
    spec:
      containers:
      - name: redis-containers
        image: harbor.lcy.net/base/redis:6.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6379
        volumeMounts:
        - mountPath: "/data/redis-data/"
          name: redis-persistent
      volumes:
      - name: redis-persistent
        persistentVolumeClaim:
          claimName: persistentvolumeclaim
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    nodePort: 30008
  selector:
    app: redis-label
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10000

```

#### 2.2.8.9: 外部客户端访问redis

![image-20220608230847429](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220608230847429.png)

#### 2.2.8.10: 验证pvc存储数据

验证nfs服务器redis的快照数据

![image-20220608232812631](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220608232812631.png)



进入到容器里验证rdis数据



![image-20220608232907785](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220608232907785.png)

#### 2.2.8.11: 删除容器，验证数据是否恢复

```bash
root@s3:/data/redis/image# kubectl delete -f redis.yaml 
statefulset.apps "redis" deleted
service "redis" deleted
root@s3:/data/redis/image# kubectl apply -f redis.yaml 
statefulset.apps/redis created
service/redis created
root@s3:/data/redis/image# kubectl exec -it redis-0 bash -n n60
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
[root@redis-0 /]# redis-cli 
127.0.0.1:6379> KEYS *
1) "k6"
2) "k5"
3) "k4"
4) "k2"
5) "k1"
6) "k3"
```

### 2.2.9: redis高可用三主三从

```bash
nfs服务器创建数据目录
root@s5:/nfs/redis# mkdir /nfs/{redis-0,redis-1,redis-2,redis-3,redis-4,redis-5}
root@s5:/nfs# exportfs -r
```

#### 2.2.9.1：PV文件内容

```bash
root@s3:/data/redis/redis-cluster# cat pv-1.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-0
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-0
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-2
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-2
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-3
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-3
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-4
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-4
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-5
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/redis-5

```

#### 2.2.9.2: redis.conf文件内容

```bash
root@s3:/data/redis/redis-cluster# cat redis.conf 
bind 0.0.0.0
protected-mode yes
port 6379
tcp-backlog 511
timeout 0
appendonly yes
cluster-enabled yes
cluster-config-file /data/redis-data/nodes.conf
cluster-node-timeout 5000
dir /data/redis-data
tcp-keepalive 300
daemonize no
pidfile /var/run/redis_6379.pid
loglevel notice
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbchecksum yes
dbfilename dump.rdb

```

#### 2.2.9.3: Dockerfile文件内容

```bash
FROM harbor.magedu.net/nginx/centos-7.7:v1

ADD redis-5.0.14.tar.gz /usr/local/src

RUN ln -sv /usr/local/src/redis-5.0.14 /usr/local/redis && cd /usr/local/redis && make && cp src/redis-cli /usr/sbin/ && cp src/redis-server /usr/sbin/  && mkdir -pv /data/redis-data

ADD redis.conf /usr/local/redis/redis.conf

ADD run_redis.sh /usr/local/redis/run_redis.sh

EXPOSE 6379

CMD ["/usr/local/redis/run_redis.sh"]
```

#### 2.2.9.4: docker-build

```bash
root@s3:docker build -t harbor.magedu.net/prod/redis:v1
root@s3: docker push harbor.magedu.net/prod/redis:v1
```



#### 2.2.9.5: redis yaml文件内容

```bash
root@s3:/data/redis/redis-cluster# cat redis.yaml 
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: n60
  labels:
    app: redis
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
  clusterIP: None
---
apiVersion: v1
kind: Service
metadata:
  name: redis-access
  namespace: n60
  labels:
    app: redis
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    protocol: TCP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: n60
spec:
  serviceName: redis
  replicas: 6
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      terminationGracePeriodSeconds: 20
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - redis
              topologyKey: kubernetes.io/hostname
      containers:
      - name: redis
        image: harbor.magedu.net/prod/redis:v1
        ports:
        - containerPort: 6379
          name: redis
        - containerPort: 16379
          name: cluster
  volumeClaimTemplates:
  - metadata:
      name: data
      namespace: n60
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi  


root@s3:/data/redis/redis-cluster# kubectl apply -f redis.yaml 
service/redis created
service/redis-access created
statefulset.apps/redis created
```

#### 2.2.9.5: pod创建完成

```bash
root@s3:/data/redis/redis-cluster# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS      AGE
myserver-myapp-fdf4f5dcf-jdmvp   1/1     Running   5 (31h ago)   12d
redis-0                          1/1     Running   0             4m44s
redis-1                          1/1     Running   0             4m38s
redis-2                          1/1     Running   0             4m34s
redis-3                          1/1     Running   0             3m21s
redis-4                          1/1     Running   0             3m15s
redis-5                          1/1     Running   0             3m11s


#验证pvc状态
root@s3:/data/redis/redis-cluster# kubectl get pvc -n n60
NAME           STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-redis-0   Bound    redis-1   5Gi        RWO                           5m35s
data-redis-1   Bound    redis-5   5Gi        RWO                           5m29s
data-redis-2   Bound    redis-4   5Gi        RWO                           5m25s
data-redis-3   Bound    redis-0   5Gi        RWO                           4m12s
data-redis-4   Bound    redis-2   5Gi        RWO                           4m6s
data-redis-5   Bound    redis-3   5Gi        RWO                           4m2s

```

#### 2.2.9.6：初始化redis cluster

初始化只需要初始化一次，redis4之前的版本需要使用redis-tribe工具进行初始化，redis5开始使用redis-cli

#### 2.2.9.7：创建初始化pod

在n60创建一个临时容器用于初始化redis-cluster

```bash
root@s3:/data/redis/redis-cluster# kubectl run -it ubuntu1804 --image=ubuntu:18.04 -n n60 bash
root@ubuntu1804:/# apt update
root@ubuntu1804:/etc/apt# apt install python2.7 python-pip redis-tools dnsutils iputils-ping net-tools
root@ubuntu1804:/etc/apt# pip install --upgrade pip
root@ubuntu1804:/etc/apt# pip install redis-trib==0.5.1

root@ubuntu1804:/etc/apt# redis-trib.py create \
> `dig +short redis-0.redis.n60.svc.cluster.local`:6379 \
> `dig +short redis-1.redis.n60.svc.cluster.local`:6379 \
> `dig +short redis-2.redis.n60.svc.cluster.local`:6379
INFO:root:Instance at 10.200.78.140:6379 checked
INFO:root:Instance at 10.200.152.213:6379 checked
INFO:root:Add 8192 slots to 10.200.78.140:6379
INFO:root:Add 8192 slots to 10.200.152.213:6379


将redis-3加入redis-0
root@ubuntu1804:/# redis-trib.py replicate \
> --master-addr `dig +short redis-0.redis.n60.svc.cluster.local`:6379 \
> --slave-addr `dig +short redis-3.redis.n60.svc.cluster.local`:6379
Redis-trib 0.5.1 Copyright (c) HunanTV Platform developers
INFO:root:Instance at 10.200.152.217:6379 has joined 10.200.78.140:6379; now set replica
INFO:root:Instance at 10.200.152.217:6379 set as replica to 9dd8185dc5446c4d7d80822147a59f0e2c456e09

将redis4加入redis-1

root@ubuntu1804:/# redis-trib.py replicate --master-addr `dig +short redis-1.redis.n60.svc.cluster.local`:6379 --slave-addr `dig +short redis-4.redis.n60.svc.cluster.local`:6379
Redis-trib 0.5.1 Copyright (c) HunanTV Platform developers
INFO:root:Instance at 10.200.78.141:6379 has joined 10.200.152.213:6379; now set replica
INFO:root:Instance at 10.200.78.141:6379 set as replica to 8283dfd37ed1932ce21615f1d29c844fdf38cf72
s
将redis-5加入redis-2
root@ubuntu1804:~#redis-trib.py replicate --master-addr `dig +short redis-2.redis.n60.svc.cluster.local`:6379 --slave-addr `dig +short redis-5.redis.n60.svc.cluster.local`:6379
Redis-trib 0.5.1 Copyright (c) HunanTV Platform developers
INFO:root:Instance at 10.200.152.227:6379 has joined 10.200.78.147:6379; now set replica
INFO:root:Instance at 10.200.152.227:6379 set as replica to 8ba1ffdb52100a55487e9ff42c916eddc702d45c

```

#### 2.2.9.8: 验证redis cluster状态：

```bash
root@s3:/data/redis/redis-cluster# kubectl exec -it redis-0 bash -n n60
[root@redis-0 /]# redis-cli 
127.0.0.1:6379> CLUSTER INFO
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:5
cluster_my_epoch:1
cluster_stats_messages_ping_sent:381
cluster_stats_messages_pong_sent:382
cluster_stats_messages_meet_sent:1
cluster_stats_messages_sent:764
cluster_stats_messages_ping_received:380
cluster_stats_messages_pong_received:382
cluster_stats_messages_meet_received:2
cluster_stats_messages_received:764


127.0.0.1:6379> CLUSTER  nodes
09529c4cff3fc33f8c5a5a5fb926227f56f1b1d6 10.200.78.146:6379@16379 myself,master - 0 1661175003000 1 connected 0-5461
8ba1ffdb52100a55487e9ff42c916eddc702d45c 10.200.78.147:6379@16379 master - 0 1661175004319 0 connected 5462-10922
7b766394c049ce42837eccab4be3a0dff562fad4 10.200.152.224:6379@16379 master - 0 1661175003313 2 connected 10923-16383
885cf5eb4d58b15a3b0263d37dd476bc8d084141 10.200.152.227:6379@16379 slave 8ba1ffdb52100a55487e9ff42c916eddc702d45c 0 1661175003000 5 connected
562a12e1a3efe3225b7606e3719ea11a5a0abe90 10.200.152.226:6379@16379 slave 09529c4cff3fc33f8c5a5a5fb926227f56f1b1d6 0 1661175003514 3 connected
3ed68bc3d314b59c1ee5a4c3a456ba3732049845 10.200.78.148:6379@16379 slave 7b766394c049ce42837eccab4be3a0dff562fad4 0 1661175003312 4 connected


#测试在master写入数据：
root@s3:/data/redis/redis-cluster# kubectl exec -it redis-1 bash -n n60

[root@redis-1 /]# redis-cli 
127.0.0.1:6379> set key1 v1
(error) MOVED 9189 10.200.78.147:6379
127.0.0.1:6379> set key2 v2
(error) MOVED 4998 10.200.78.146:6379
127.0.0.1:6379> set key3 v3
(error) MOVED 935 10.200.78.146:6379
127.0.0.1:6379> set key4 v5
OK
127.0.0.1:6379> KEYS *
1) "key4"
127.0.0.1:6379> get key4
"v5"

#在slave验证数据
root@s3:/data/redis/redis-cluster# kubectl exec -it redis-4 bash -n n60
[root@redis-4 /]# redis-cli 
127.0.0.1:6379> KEYS *
1) "key4"

```

### 2.2.10：实战案例之MySQL主从架构：

https://kubernetes.io/zh-cn/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/

https://www.kubernetes.org.cn/statefulset

基于StatefulSet实现：

https://kubernetes.io/zh-cn/docs/tasks/run-application/run-replicated-stateful-application/

Pod调度运行时，如果应用不需要任何稳定的标书、有序的部署、删除和扩展，则应该使用一组无状态副本的控制器来部署应用，例如Deployment或ReplicaSet更适合无状态服务需求，而StatefulSet适合管理所有有状态的服务，比如MySQL，MongoDB集群等

```bash
StatefulSet本质上时Deploymetn的一种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的pod名称，启动顺序，在statefulset中，pod名为网络标时，还必须要用到共享存储，
在deployment中，与之对应的服务时servide，而在statefulset中与之对应的headless service headless service，既无头服务，于service的区别它没有cluster ip,解析它的名称时将返回该Headless service对应pod的endpoint列表

statefulset特点：
 给每个Pod分配固定且唯一的网络标识符
 给每个pod分配固定且持久化的外部存储
 对pod进行有序的部署和扩展
 对pod进有序的删除和终止
 对pod进有序的自动滚动更新
```

#### 2.2.10.1: StatefulSet的组成部分：

```bash
Headless service: 用来定义pod网络标识，指 的是短的service,
statefulSet: 定义具体应用，有多少个pod副本，并为每个pod定义了一个域名
volumeclaimTemplates: 存储卷申请模板，创建pvc，指定pvc名称大小，将自动创建pvc，且pvc必须由存储类供应
```

#### 2.2.10.2：镜像准备：

https://github.com/docker-library/ #github下载地址

基础镜像准备

```bash
#准备xtrabackup镜像
root@s5:~# docker pull registry.cn-hangzhou.aliyuncs.com/hxpdocker/xtrabackup:1.0
root@s5:~# docker tag registry.cn-hangzhou.aliyuncs.com/hxpdocker/xtrabackup:1.0 harbor.magedu.net/prod/xtrabackup:1.0
root@s5:~# docker push harbor.magedu.net/prod/xtrabackup:1.0

#准备mysql镜像
root@s5:~# docker pull mysql:5.7.36
root@s5:~# docker tag mysql:5.7 harbor.magedu.net/prod/mysql:5.7.36
root@s5:~# docker push harbor.magedu.net/prod/mysql:5.7.36
```

#### 2.2.10.3: 创建pv

pvc会自动基于pv创建，只需要多个可用的pv即可，pv数量取决于计划启动多少个mysql pod，本次创建3个pv，也就是最多启动5个mysqlpod

```bash
#nfs
root@s5:~# mkdir /nfs/mysql-0
root@s5:~# mkdir /nfs/mysql-1
root@s5:~# mkdir /nfs/mysql-2
root@s5:~# exportfs -r

root@s3:/data/mysql# cat mysql-persistentvolume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-0
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /nfs/mysql-0 
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /nfs/mysql-1
    server: 192.168.48.164
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-2
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    path: /nfs/mysql-2
    server: 192.168.48.164

```

#### 2.2.10.4: 创建configmap

```bash
root@s3:/data/mysql# cat mysql-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  namespace: n60
  labels:
    app: mysql
data:
  master.cnf: |
    # Apply this config only on the master.
    [mysqld]
    log-bin
    log_bin_trust_function_creators=1
    lower_case_table_names=1
  slave.cnf: |
    # Apply this config only on slaves.
    [mysqld]
    super-read-only
    log_bin_trust_function_creators=1

```

#### 2.2.10.5: 创建services

```bash
root@s3:/data/mysql# cat mysql-services.yaml 
apiVersion: v1
kind: Service
metadata:
  namespace: n60
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  namespace: n60
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql

```

#### 2.2.10.6：创建statefulet

```bash
root@s3:/data/mysql# cat mysql-statefulset.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: n60
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: harbor.magedu.net/prod/mysql:5.7.36 
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: harbor.magedu.net/prod/xtrabackup:1.0 
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: harbor.magedu.net/prod/mysql:5.7.36 
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 250Mi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: harbor.magedu.net/prod/xtrabackup:1.0 
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi
          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 2Gi


root@s3:/data/mysql# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS   AGE
mysql-0                          2/2     Running   0          4m42s
mysql-1                          2/2     Running   0          3m57s
mysql-2                          2/2     Running   0          3m31s
```

#### 2.2.10.7: 验证MySQL主从同步是否正常

![image-20220824235301306](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220824235301306.png)

#### 2.2.10.8：高可用测试

分别删除pod中的Master与slave节点，验证MySQL服务最终能否恢复到正常状态

#### 2.2.10.9：删除MySQL Master:

```bash
root@s3:/data/mysql# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS   AGE
myserver-myapp-fdf4f5dcf-59sf9   1/1     Running   0          28m
mysql-0                          2/2     Running   0          11m
mysql-1                          2/2     Running   0          10m
mysql-2                          2/2     Running   0          10m
root@s3:/data/mysql# kubectl delete po mysql-0 -n n60
pod "mysql-0" deleted
myapp   1/1     Running   0          21m
root@s3:/data/mysql# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS   AGE
myserver-myapp-fdf4f5dcf-59sf9   1/1     Running   0          28m
mysql-0                          1/2     Running   0          10s
mysql-1                          2/2     Running   0          11m
mysql-2                          2/2     Running   0          11m

```

#### 2.2.10.10: 删除MySQL Slave:

```bash
root@s3:/data/mysql# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS   AGE
mysql-0                          2/2     Running   0          59s
mysql-1                          2/2     Running   0          12m
mysql-2                          2/2     Running   0          12m
root@s3:/data/mysql# kubectl delete po mysql-1 -n n60
pod "mysql-1" deleted
root@s3:/data/mysql# kubectl get po -n n60
NAME                             READY   STATUS    RESTARTS   AGE
mysql-0                          2/2     Running   0          110s
mysql-1                          1/2     Running   0          10s
mysql-2                          2/2     Running   0          13m

```



## 2.3 Pod 的状态和探针：

https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle

### 2.3.1 pod状态：

![image-20220305173501631](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305173501631.png)

```bash
第一阶段：
	Pending:
	正在创建pod但是pod中的容器还没有全部被创建完成=[处于此状态的pod应该检查pod依赖的存储是否有权限挂载，镜像是否可以下载、调度是否正常等]
	
	Failed
	#Pod中有容器启动失败而导致Pod工作异常
	
	Unknown
	#由于某种原因无法活得pod当前状态，通常是由于pod所在的node节点通信错误
	
	Succeeded
	#pod中的所有容器都被成功终止集pod里所有的containers均与terminated
	
第二阶段：

	unschedulable:
	#pod不能被调度，kuber-scheduler没有匹配到合适的node节点
	
	PodScheduled
	#pod正处于调度中，在kube-scheduler刚开始调度的时候，还没有将pod分配到指定的node，在筛选出合适的节点后就会更新etcd数据，将pod分配到指定的Node
	
	initialized
	所有pod中的初始化容器已经完成了
	#
	imagePullbackoff:
	#pod所在的node节点下载镜像失败
	
	running
	#pod内部的容器已经被创建并且启动
	
	Ready
	#表示pod中的容器已经可以提供访问服务
```

![image-20220305193103966](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305193103966.png)

运维通过命令行或者脚本等去调度Api 进行创建容器,6443端口验证权限，验证请求中的Yml文件是否合法,字段缩进是否正确，没有问题把这些把这些事件写入到etcd，schedule监听api-version，发现有pod创建的操作，那么会根据当前k8s环境可用的node资源利用率进行调度，把pod调度到最优的节点上，把调度的结果在返回给etcd，pod和节点绑定，调度到的node节点的kubelet会通过api-server拿到事件，根据绑定事件镜像中的地址，镜像名称会初始化这个容器，kube-proxy通过api-server拿到网络规则，进行本地转发端口保存到本地

```bash
Error: #Pod启动过程中发生错误
NodeLost: #Pod 所在的节点失联
Unkown: #Pod 所在节点失联或其它未知异常
Waiting: #Pod 等待启动
Terminating: #Pod 正在被销毁
Pending：#pod 等待被调度
CrashLoopBackoff:#Pod 但是kubelet正在将它重启
InvalidImageName: #node节点无法解析镜像名称导致的镜像无法下载
ImageInspectErrot# 无法拉取检验镜像，镜像不完整导致
ErrImageNeverPull:# 策略禁止拉取镜像，镜像中心权限是私有等
ImagePullBackoff: 镜像拉取失败，但是正在重新拉取
RegistryUnavaiLable: #镜像服务器不可用，网络原因或harbor冗机
ErrImagePull: #镜像拉取出错，超时或下载被强制终止
CreateContainerConfigErrot: #不能创建Kubelet使用的容器配置
CreateContainerError: #创建容器失败
PreStarContainer: 执行preStart hook报错，Pod hook （钩子）是由kubernetes管理的kubelet发起的,当容器中的进程启动前或者容器中的进程终止之前运行，比如容器创建完成后里面的服务启动之前可以检查以下依赖的其它服务是否启动，或者容器退出之前可以把容器中的服务先通过命令停止
PostStartHooKError:#执行postStart hook 报错
RunContainerError: pod运行失败，容器中没有初始化PID为1的守护进程等
ContainersNotINitialized: #Pod没有初始化完毕
ContainersNotReady: #Pod没有准备完毕
ContainerCreating: #pod正在创建中
PodInitializing: #pod正在初始化中
DockerDaemonNotReady: #node节点decker服务没有启动
 NerworkPluginNotReady: #网络插件还没有完全启动
```



### 2.3.2 探针简介：

探针是由kubelet对容器执行的定期诊断，已保证pod的状态始终处于运行状态，要执行诊断，kubelet调用由容器实现的Handler（处理程序）有三种类型的处理程序:

```bash
ExecAction
#在容器内执行指定命令，如果命令退出时返回码为0则认为诊断成功

TCPSockerAction
#对指定端口上的容器的IP地址进行TCP检查，如果端口打开，则诊断被认为是成功的

HTTPGetAction
#对指定的端口和路径上的容器的IP地址执行HTTPGet请求，如果响应的状态码大于等于200且小于400，则诊断被认为是成功的
```

没测探测都获得以下三种结果之一：

成功：容器通过了诊断

失败：容器未通过诊断

未知：诊断失败，因此不会采取任何行动

### 2.3.3 配置探针：

基于探针实现对pod的状态检测

### 2.3.4 探针类型:

```bash
livenessProbe
#存活探针，检测容器是否正在运行，如果存活探测失败，则kubelet会杀死容器，并且容器将受到其重启策略的影响，如果容器不提供存活探针，则默认状态为Success,livenessProbe用于控制是否重启Pod

ReadinessProbe

#就绪探针，如果就绪探测失败，端点控制器将从于pod匹配到的所有Service的端点中删除改pod的IP地址，初始延迟之前的就绪状态默认为Failure(失败)，如果容器不提供的就绪探针，则默认状态为Success,readinglessProbe用于控制pod是否添加直service
```

### 2.3.5 探针配置 

探针有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为：

```bash
 initialDelaySeconds: 30
#初始化延迟时间，告诉kubelet在执行第一次探针前应该等待多少秒，默认是0秒，最小值是0

periodSeconds: 5
#探测周期间隔，指定了kubelet应该每多少秒执行一次存活探测，默认是10秒，最小值是1

timeoutSeconds: 1
#单词探测超时时间，探测的超时等待多少秒，默认值是1秒，最小值是1.

successThreshold: 1
#从失败转为成功的重试次数，探测器在失败后，被视为成功的最小连接成功数，默认值是1，存活探测的这个值必须是1，最小值是1

failureThreshold: 3
#从成功转为失败的重试次数，当pod启动了并且探测到失败，kubernetes的重试次数，存活探测情况下的放弃就意外着重新启动容器，就绪探测情况下的放弃pod，会被打上为就绪的标签，默认值是3，最小值是1
```

HTTP 探测器可要在httpGet上配置额外的字段：

```
Host；
#连接使用的主机名，默认是Pod的IP 也可以在HTTP头中设置host来代替

Scheme: http
#用于设置连接主机的方式（HTTP还是HTTPS）默认是HTTP

Path： /monitor/index.html
#访问HTTP服务的路径

httpHeaders:
#请求中自定义的http头，HTTP头字段允许重复

Port：80
#访问容器的端口号或者端口名，如果数字必须在1~65535之间
```

### 2.3.6 存活HTTP探针示例

```
root@k8s-master1:~/pod/livenessProbe# cat livenessProbe.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deploy 
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: nginx
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ng-server
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30006
    targetPort: 80
  selector:
    app: ng-label

root@k8s-master1:~/pod/livenessProbe# kubectl apply -f livenessProbe.yaml 
deployment.apps/ng-deploy created
service/ng-server created

#测试
root@k8s-master2:~# while true;do curl http://192.168.48.99:30006;sleep 1;done

#进入容器删除页面
root@ng-deploy-85bdbcb7d4-lh9bj:/usr/share/nginx/html# rm index.html 

```

![image-20220305213814701](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305213814701.png)

![image-20220305213909078](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305213909078.png)

#### 2.3.6.1 就绪HTTP探针

```
root@k8s-master1:~/pod/livenessProbe# cat readlinessProbe.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ng-server
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: ng-label
root@k8s-master1:~/pod/livenessProbe# kubectl apply -f readlinessProbe.yaml 
deployment.apps/ng-deploy created
service/ng-server created

#进入容器删除页面
oot@ng-deploy-559bf8c9f4-5bqfl:/usr/share/nginx/html# rm index.html 

#验证
```

![image-20220305215342930](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305215342930.png)

![image-20220305215359886](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305215359886.png)

#### 2.3.6.2 存活就绪探针结合使用

```
root@k8s-master1:~/pod/livenessProbe# cat deplou.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: http
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ng-server
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30005
  selector:
    app: ng-label

```

### 2.3.7 TCP探针示例：

```
root@k8s-master1:~/pod/livenessProbe# cat tcp.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deploy 
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: nginx
        image: harbor.magedu.net/n60/nginx:1.18.0
        ports:
        - containerPort: 80
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ng-server
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30006
    targetPort: 80
  selector:
    app: ng-label
root@k8s-master1:~/pod/livenessProbe# kubectl apply -f tcp.yaml 
deployment.apps/ng-deploy created
service/ng-server created

```

### 2.3.8 ExecAction探针：

可以基于指定的命令对pod进行特定的状态检查

```
root@k8s-master1:~/pod/livenessProbe# cat execaction.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-deploy 
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-label
  template:
    metadata:
      labels:
        app: ng-label
    spec:
      containers:
      - name: nginx
        image: redis 
        ports:
        - containerPort: 6379
        livenessProbe:
          exec:
            command:
            - /usr/local/bin/redis-cli
            - quit
          initialDelaySeconds: 5
          periodSeconds: 3
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ng-server
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30006
    targetPort: 6379
  selector:
    app: ng-label

```

### 2.3.9 livenessProbe和readinessProbe的对比：

```
配置参数一样
livenessProbe #连续探测失败会重启、重建pod，readinessProbe不会执行重启或者重建pod操作
livenssProbe #连续检查指定次数后会将容器值于（Crash Loop BackOff）且不可用，readinessProbe不会

readlinessProbe #连续探测失败会从service的endpointed中删除改Pod，livenessProbe不具备此功能，但是会将容器挂起livenssProbe

livenessProbe用户控制是否重启pod，readlinessProbe用于控制pod是否添加只service

建议两个探针都配置
```

#### 2.3.9.1 Pod重启策略：

k8s在pod出现异常的时候会自动将pod重启以恢复Pod中的服务

```
restartPolicy:
  Always: 当容器异常时，k8s自动重启该容器
  OnFailure: 当容器失败时（容器停止运行且退出码不为0），k8s自动重启该容器
  Never：不论容器运行状态如何都不会重启该容器，job或cronjob
```

#### 2.3.9.2 镜像拉取策略：

```
imagePullpolicy: IfNotPresent #node节点没有此镜像就去指定仓库拉取，node有就使用node本地镜像
imagePullpolicy：Always #每次重建pod都会重新拉取镜像
imagePullPolicy: Never #从不到镜像中心拉取镜像，只使用本地镜像
```

## 2.4 容器网络

https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/

容器网络的目的

```bash
1.容器到容器：一个pod多个容器（LNMP）容器基于注册中心发现直接进行调用的
2.pod到pod：同宿主机和不同宿主机且经过service转发请求的
3.pod到外部：pod中的服务需要访问宿主机网络以外的环境，如硬件存储、运行在虚拟机或者物理机的数据库、ceph存储、外网API接口等
4.外部到Pod: 用于的请求从外网经过防火墙及负载均衡，透过nodeport经过service转发给Pod的请求
```



![image-20220319180114743](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220319180114743.png)

### 2.4.1 网络通信方式

#### 2.4.1.1 二层通信

基于目标mac地址通信，不可跨局域网，通常是由交换机实现报文转发

```bash
private mode
 private模式下，同一父接口下的子接口之间彼此隔离，不能通信，外部也无法访问
 
 vepa(Virtual Ethernet Port Aggregator 虚拟以太网端口聚合器)mode:
   vepa模式下，子接口之间的通信流量需要导到外部支持 902.1Qbg/VPEA 功能的交换机上（可以是物理的或者虚拟的），经由外部交换机转发，再绕回来
   
bridge mode:
  bridge 模式下，模拟的是Linux bridge的功能，但比bridge要好的一点是每个接口的MAC地址是已知的，不用学习，所以这种模式下，子接口之间就是直接可以通信的
  
passthru mode(直通模式)

 passthru 模式，只允许单个子接口连接父接口
 
 source mode
  这种模式，直接收mac为指定的mac地址的报文
```

#### 2.4.1.2 三层通信

基于目标IP通信，也叫做IP交换技术，解决了跨局域网、跨网络通信，通常是由路由器，防火墙、三层交换机等

![image-20220319182657313](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220319182657313.png)

#### 2.4.1.3 网桥（bridge）

安装完docker之后会默认生成一个docker的网桥设备，网桥设备通过mac地址转发报文到到各个容器，如果容器要访问当前宿主机以外的容器或者网络，则会使用宿主机的路由功能进行源地转换

![image-20220319221451907](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220319221451907.png)

#### 2.4.1.4 Vlan

VLAN（Virtual Local Area Network）既虚拟局域网，是一个物理（交换机）网络在逻辑上划分多个广播域的通信技术，VLAN内的主机间可以直接通信，而VLAN网络外的主机需要通过三层网络设备转发才可以通信，因此一个VLAN可以将服务器的广播报文限制在一个VLAN内，从而降低单个网络环境的广播报文，vlan采用12位标时vlan ID，既一个交换机设备最多为2^12=4096个vlan



![image-20220319223019885](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220319223019885.png)

#### 2.4.1.5 overlay 网络简介

叠加网络或者覆盖网络，在物理网络的基础之上叠加实现新的虚拟网络，即可使网络的中的容器可以相互通信



![image-20220320130355872](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320130355872.png)



![image-20220320130654839](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320130654839.png)



#### 2.4.1.6 overlay 网络实现方式

```bash
VxLAN: VxLAN全称是Visual eXtensible Local Area Network (虚拟扩展本地局域网)，主要有Cisso推出，vxlan是一个VLAN的扩展协议，是由IETF定义的NVO3（NetworkVirtualization over Layer3）标准技术之一，VXLAN的特点是将L2的以太桢封装到UDP报文（既L2 overL4）中，并在L3网络中传输，既使用MACin UDP的方法对报文进行重新封装，VxLAN本质上是一种overlay的隧道封装技术，它将L2的以太网封装成L4的UDP数据包，然后在L3中的网络中传输，效果就像L2的以太网帧在一个广播域中传输一样，实际上L2的以太网帧跨域了L3网络传输，但是确不受L3网络的限制，vxlan采用24位标识vlan ID号，因此可以支持2^24=16777216个vlan，其可扩展性比vlan强大的多，可以支持大规模数据中心的网络需求。
```



![image-20220320133617019](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320133617019.png)







```
VTEP(VXLAN Tunnel Endpoint vxlan隧道端点)，VTEP是VXLAN网络的边缘设备，是VXLAN隧道的起点和终点，VXLAN对用户原始数据帧和封装和解封装钧在VTEP上进行，用于VXLAN报文的封装和解封装，VTEP与物理网络相连，分配的地址为物理网IP地址VXLAN报文中源IP地址为本节点的VTEP地址，VXLAN报文中的目的地址为对端节点的VTEP地址，一对VTEP地址就对应着一个VXLAN隧道，服务器上的虚拟交换机（隧道flannel.1就是VTEP），比如一个虚拟机网络中的多个VXLAN就需要多个VTEP对不同网络的报文进行封装于解封装

VNI（VXLAN Network identifier）VXLAN网络标识VNI类似VLAN，用于区分VXLAN段，不同VXLAN段的虚拟机不能直接二层相互通信，一个VNI表示一个租户，即使多个终端用户属于同一个VNI，也表示一个租户
```



![image-20220320134715951](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320134715951.png)



```
NVGRE： Network Virtualization using Generic Routing Encapsulation,主要支持者是Microsoft 于VXLAN不同的是，NVGRE没有采用标准传输协议（TCP/UDP），而是借助通用路由封装协议(GRE)，NVGRE使用头部的低24为作为租户网络标识符（TNI），与VXLAN一样可以支持1777216个vlan
```



```
/ # traceroute 10.200.0.2
traceroute to 10.200.0.2 (10.200.0.2), 30 hops max, 46 byte packets
 1  10.200.1.1 (10.200.1.1)  0.013 ms  0.008 ms  0.009 ms
 2  10.200.0.0 (10.200.0.0)  0.417 ms  0.160 ms  0.235 ms
 3  10.200.0.2 (10.200.0.2)  0.474 ms  0.270 ms  0.425 ms

```

#### 2.4.1.7 underlay简介

Underlay网络就是传统IT基础设施网络，由交换机和路由器等设备组成，借助以太网协议、路由协议和VLAN协议等驱动，它还是Overlay网络的底层网络，为Overlay网络提供数据通信服务，容器网络中的Underlay网络是指借助驱动程序将宿主机的底层网络接口直接暴露给容器使用的一种网络构建技术，较为常见的解决方案有MAC VLAN IP VLAN和直接路由等

Underlay依赖网络和网络进行跨主机通信

#### 2.4.1.8 bridge与macvlan模式

Bridge 桥接模式

MACVLAN 支持在同一个以太网接口上虚拟出多个网络接口（子接口），每个虚拟接口拥有唯一的MAC地址并可配置网卡子接口IP

![image-20220320165816899](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320165816899.png)

#### 2.4.1.9 IP VLAN

```
IP VLAN 类似于MACVLAN，它同样创建新的虚拟网络接口并为每个接口分配唯一的IP地址，不同在于，每个虚拟接口将共享使用物理接口的MAC地址，从而不在违反防止MAC欺骗的交换机的安全策略，且不要求在物理接口上启用混杂模式
```



![image-20220320170704338](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320170704338.png)



```
IP VLAN有L2和L3两种模式，其中IP VLAN L2的工作模式类似于MACVLAN备用做网桥或者交换机，而IPVLAN L3模式中，子接口地址不一样，但是公用宿主机的MAC地址，虽然支持多种网络模型，但MAC VLAN和IP VLAN不能同时在同一物理接口使用，一般使用MAC VLAN
```

### 2.4.2 VXLAN 通信过程

![image-20220320194350744](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320194350744.png)

#### 2.4.2.1vxlan简单通信流程

```
1.vm A发送L2桢于VM请求于VM B通信
2.源宿主机VTEP添加或者封装VXLAN、UDP及IP头部报文
3.网络设备将封装后的报文通过标准的报文在三层进行转发到目标主机
4.目标宿主机VTEP删除或者解封装VXLAN、UDP及IP头部
5.将原始L2桢发送给目标VM
```

![image-20220320195130939](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320195130939.png)

#### 2.4.2.3 vxlan 通信流程

![image-20220320201042427](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220320201042427.png)

### 2.4.3 flannel

官方：https://coreos.com/flannel/docs/latest

文档：https://coreos.com/flannel/docs/latest/kubernetes.html

由CentOS开源的针对K8s的网络服务，其目的为解决k8s集群中的各主机上的pod相互通信的问题，其借助于etcd维护IP地址分配，并为每一个node服务器分配一个不同的ip地址段

Flannel网络模型（后端），Flannel目前有三种实现方式UDP/VXLAN/host-gw

```bash
UDP:早期版本的Flannel使用UDP封装完成报文的跨越主机转发，其安全性能略有不足

VXLAN,VXLAN本质上是一种tunnel（隧道）协议，用来基于3层网络实现虚拟的2层网络，目前flannel的网络模型已经是基于VXLAN的叠加（覆盖）网络，目前推荐使用vxlan作为网络模型

Host-gw:也就是Host GateWay 通过在node节点上创建到达各目标容器地址的路由表而完成报文的，转发，因此这种方式要求各node节点本身处于同一个局域网（二层网络）中，因此不适用于网络变动频繁或者比较大型的网络环境，但是其性能较好
```



Flannel组件的解释

```
cni0:网桥设备，每创建一个pod都会创建一对veth pair，其中一端是pod中的eht0，另一端是cni0网桥中的端口（网卡），pod中从网卡eth0发出的流量都会发送到cni0网桥设备的端口（网卡）上，cni0设备获得ip地址是该节点分配到的网段的第一个地址
Flannel.1:overlay网络的设备，用来进行vxlan报文的处理（封包和解包），不同node之间的Pod数据流量都从overlay设备以隧道的形式发送到对端
```

Flannel的系统文件目录

```
[root@k8s-node ~]# find / -name flannel
/run/flannel
/var/lib/cni/flannel
/usr/bin/flannel

```

#### 2.4.3.1 flannel pod状态

```bash
[root@k8s-master ~]# kubectl get pod -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
kube-flannel-ds-b67dl                1/1     Running   0          36m
kube-flannel-ds-zpppk                1/1     Running   0          36m

```

#### 2.4.3.2 创建测试pod

```bash
[root@k8s-master ~]#kubectl run net-test1 --image=alpine sleep 360000
```



#### 2.4.3.3 验证pod状态

```bash
[root@k8s-master ~]# kubectl  get po -o wide
NAME        READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
net-test1   1/1     Running   0          25m    10.244.1.2   k8s-node1    <none>           <none>
net-test5   1/1     Running   0          2m2s   10.244.0.7   k8s-master   <none>           <none>


```

#### 2.4.3.4 当前node主机cni信息

```bash
[root@k8s-master ~]# cat /var/lib/cni/flannel/1da38e57e76fdae2daf1365b5f246fc74e96e0b914dd71bb32eafe4ae866c5a1 
{"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}
```

#### 2.4.3.5 当前node主机IP地址范围

```bash
[root@k8s-master ~]# cat /run/flannel/subnet.env 
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
```

#### 2.4.3.6 当前Node主机路由

```bash
[root@k8s-master ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.48.2    0.0.0.0         UG    100    0        0 ens33
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.48.0    0.0.0.0         255.255.255.0   U     100    0        0 ens33
```

#### 2.4.3.7 验证flannel的VXLAN配置

验证当前backend类型

```bash
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.200.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }

```

vxlan使用UDP端口8472传输封装好的报文

```bash
[root@k8s-master flannel]# netstat -tuanlp | grep 8472
udp        0      0 0.0.0.0:8472            0.0.0.0:*                           -                   
```

![image-20220321215532308](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220321215532308.png)

测试网络通信

```bash
[root@k8s-master ~]# kubectl exec -it net-test1 sh
/ # apk update
/ # apk add net-tools
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:92:00:20:4A:05  
          inet addr:10.244.1.2  Bcast:10.244.1.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:3492 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1593 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2625534 (2.5 MiB)  TX bytes:90367 (88.2 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # ping 10.244.0.10
PING 10.244.0.10 (10.244.0.10): 56 data bytes
64 bytes from 10.244.0.10: seq=0 ttl=62 time=0.555 ms
64 bytes from 10.244.0.10: seq=1 ttl=62 time=0.451 ms
64 bytes from 10.244.0.10: seq=2 ttl=62 time=0.465 ms
64 bytes from 10.244.0.10: seq=3 ttl=62 time=0.483 ms
^C
--- 10.244.0.10 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.451/0.488/0.555 ms
/ # traceroute 10.244.0.10
traceroute to 10.244.0.10 (10.244.0.10), 30 hops max, 46 byte packets
 1  10.244.1.1 (10.244.1.1)  0.010 ms  0.007 ms  0.004 ms
 2  10.244.0.0 (10.244.0.0)  0.398 ms  0.573 ms  0.563 ms
 3  10.244.0.10 (10.244.0.10)  0.780 ms  0.845 ms  0.367 ms

```

抓包：

```bash
[root@k8s-node1 ~]# tcpdump -i any -vvv -nn port 8472
tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes
05:31:19.745770 IP (tos 0x0, ttl 64, id 45130, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.147.33574 > 192.168.48.146.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 19066, offset 0, flags [DF], proto ICMP (1), length 84)
    10.244.1.2 > 10.244.0.10: ICMP echo request, id 36, seq 0, length 64
05:31:19.746041 IP (tos 0x0, ttl 64, id 39742, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.146.56438 > 192.168.48.147.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 47906, offset 0, flags [none], proto ICMP (1), length 84)
    10.244.0.10 > 10.244.1.2: ICMP echo reply, id 36, seq 0, length 64
05:31:20.746139 IP (tos 0x0, ttl 64, id 46091, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.147.33574 > 192.168.48.146.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 19791, offset 0, flags [DF], proto ICMP (1), length 84)
    10.244.1.2 > 10.244.0.10: ICMP echo request, id 36, seq 1, length 64
05:31:20.746412 IP (tos 0x0, ttl 64, id 40166, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.146.56438 > 192.168.48.147.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 48520, offset 0, flags [none], proto ICMP (1), length 84)
    10.244.0.10 > 10.244.1.2: ICMP echo reply, id 36, seq 1, length 64
05:31:21.747221 IP (tos 0x0, ttl 64, id 46476, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.147.33574 > 192.168.48.146.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 19966, offset 0, flags [DF], proto ICMP (1), length 84)
    10.244.1.2 > 10.244.0.10: ICMP echo request, id 36, seq 2, length 64
05:31:21.747512 IP (tos 0x0, ttl 64, id 40333, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.146.56438 > 192.168.48.147.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 49071, offset 0, flags [none], proto ICMP (1), length 84)
    10.244.0.10 > 10.244.1.2: ICMP echo reply, id 36, seq 2, length 64
05:31:22.748484 IP (tos 0x0, ttl 64, id 47319, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.147.33574 > 192.168.48.146.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 20695, offset 0, flags [DF], proto ICMP (1), length 84)
    10.244.1.2 > 10.244.0.10: ICMP echo request, id 36, seq 3, length 64
05:31:22.748773 IP (tos 0x0, ttl 64, id 40496, offset 0, flags [none], proto UDP (17), length 134)
    192.168.48.146.56438 > 192.168.48.147.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 1
IP (tos 0x0, ttl 63, id 49389, offset 0, flags [none], proto ICMP (1), length 84)
    10.244.0.10 > 10.244.1.2: ICMP echo reply, id 36, seq 3, length 64

```



#### 2.4.3.8 VXLAN Directrouting

Directrouting为同一个二层网络中的node节点启用直接路由机制，类似host-gw模式

```bash
 [root@k8s-master ~]# vim kube-flannel.yml 
   net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan",
        "Directrouting": true
      }
    }


#推荐使用
相同的node子网通信，不需要overlay封装和解封装（性能要强），而且还支持node跨子网，跨子网overlay
```

#### 2.4.3.9：验证修改后的路由表

```bash
#修改为Directrouting之前的路由表
[root@k8s-master ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.48.2    0.0.0.0         UG    100    0        0 ens33
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.48.0    0.0.0.0         255.255.255.0   U     100    0        0 ens33

#修改为Directrouting之后的路由表
[root@k8s-master ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.48.2    0.0.0.0         UG    100    0        0 ens33
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      192.168.48.147  255.255.255.0   UG    0      0        0 ens33
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.48.0    0.0.0.0         255.255.255.0   U     100    0        0 ens33

```

#### 2.4.3.10: 验证修改后的路由效果

```bash
#修改之前的路由效果
/ # traceroute 10.244.0.10
traceroute to 10.244.0.10 (10.244.0.10), 30 hops max, 46 byte packets
 1  10.244.1.1 (10.244.1.1)  0.010 ms  0.007 ms  0.004 ms
 2  10.244.0.0 (10.244.0.0)  0.398 ms  0.573 ms  0.563 ms
 3  10.244.0.10 (10.244.0.10)  0.780 ms  0.845 ms  0.367 ms
 
 #修改之后的路由效果
 / # traceroute 10.244.0.10
traceroute to 10.244.0.10 (10.244.0.10), 30 hops max, 46 byte packets
 1  10.244.1.1 (10.244.1.1)  0.017 ms  0.016 ms  0.009 ms
 2  192-168-48-146.kubernetes.default.svc.cluster.local (192.168.48.146)  0.926 ms  1.776 ms  1.237 ms
 3  10.244.0.10 (10.244.0.10)  2.575 ms  1.011 ms  0.813 ms

```

#### 2.4.3.11: 抓包

同IP子网的Node的通信会抓不到包，因为相同的node不会通过vxlan封装报文，跨子网的node通信会使用overlay报文封装

```bash
[root@k8s-node1 ~]# tcpdump -i any -vvv -nn port 8472
tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes #抓不到
```



#### 2.4.3.8 ost-gw网络模型

```
#设置falnnel后端
FLANNEL_BACKEND: "host-gw"

```

### 2.4.4 calico

官网：https://www.tigera.io/project-calico/

Calico是一个纯三层的网络解决方案，为容器提供多Node之间的访问通信，calico将每一个Node节点都当作一个路由器（route），各节点BGP（Border Gateway Protocol）边界网关协议 学习并在node节点生成路由规则，从而将不同node节点上的pod连接起来进行通信



```
calico简介：
网络通过第3层路由技术（如静态路由或BGP路由分配）或第二层地址学习来感知工作负载IP地址，因此，他们可以将未封装的流量路由到作为最终目的地的端点的正确主机，但是，并非所有网络能够路由工作负载IP地址，例如公共云环境、跨VPC子网边界的AWS，以及无法通过BGP、Calico对应到underlay网络或无法轻松配置静态路由的其它场景，这就是为什么Calico支持封装，因此你可以在工作负载之间发送流量，而无需底层网络知道工作负载IP地址

calico封装类型
Calico支持两种类型的封装：VXLAN和IP-in-IP,VXLAN在IP中没有IP的某些环境中受支持（例如Azure）,VXLAN的每数据包开销高，因为报头较大，但除非你运行的网络密集工作负载，否则你通常不会注意到这种差异，这两种封装之间的另一个小差异是calico的vxland实现不使用BGP，Calico的IP-in-IP是在Calico节点之间使用BGP协议实现跨子网
```

BGP是一个去中心化的协议，它通过自动学习和维护路由表实现网络的可用性，但是并不是所有的网络都支持BGP，另外为了跨网络实现更大规模的网络管理，calico还支持IP-in-IP的叠加模型，简称IPIP，IPIP可以实现跨不同网段建立路由通信，但是会存在安全性问题，其在内核内置，可以通过Calico的配置文件设置是否启用IPIP，在公司内部如果K8s的node节点没有跨越网段建议关闭IPIP

```
IPIP是一种将各Node的路由之间坐一个tunnel，在把两个网络连接起来的模式，启用IPIP模式时，Calico将在各node上创建名为tunl0的虚拟网络接口
BGP模式则直接使用物理机作为虚拟路由器（vRouter）,不在创建额外的tunnel
```

calico核心组件

```
Felix: calico的agent，运行在每一台node节点上，其主要时维护路由规则、汇报当前节点状态以确保pod的跨主机通信

BGP client： 每台node都运行，其主要负责监听node节点上由felix生成的路由信息，然后通过BGP协议广播至其它剩余的node节点，从而相互学习路由实现pod通信
Route Reflector: 集群种的路由反射器，calico v3.3开始支持，当Calico BGP客户端将路由从其FIB（Forward Information dataBase,转发信息库）通告到Route Reflector时，Route Reflector会将这些路由通告给部署集群种的其它节点，Route Reflector专门用于管理BGP网络路由规则，不会产生Pod数据通信


```

![image-20220322222619137](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220322222619137.png)

#### 2.4.4.1 验证当前路由表

```
root@k8s-master1:~# calicoctl node status
Calico process is running.

IPv4 BGP status
+----------------+-------------------+-------+----------+-------------+
|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+----------------+-------------------+-------+----------+-------------+
| 192.168.48.163 | node-to-node mesh | up    | 14:23:10 | Established |
| 192.168.48.164 | node-to-node mesh | up    | 14:23:20 | Established |
| 192.168.48.165 | node-to-node mesh | up    | 14:21:30 | Established |
+----------------+-------------------+-------+----------+-------------+

```

#### 2.4.4.2 开启IPIP的通道状态

```
开启IPIP
root@k8s-master1:~# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.48.2    0.0.0.0         UG    100    0        0 ens33
10.200.36.64    192.168.48.164  255.255.255.192 UG    0      0        0 tunl0
10.200.159.128  0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.169.128  192.168.48.165  255.255.255.192 UG    0      0        0 tunl0
10.200.224.0    192.168.48.163  255.255.255.192 UG    0      0        0 tunl0
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 ens33
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.48.0    0.0.0.0         255.255.255.0   U     100    0        0 ens33

```

1.能使用calico，就使用calico

​    calico支持网络规则

2.使用启用overlay

  考虑后期的网络扩展，node是否存在会跨子网的问题

3.不启用overlay

​    flannel host-gw  不支持k8s网络规则

​    calico BGP  不支持跨网络

4.overlay

  calico的IPIP

flannel vxlan

## 2.5 请求流程iptaables

### 2.5.1 iptables 规则匹配-PREROUTING链

流程：

```
PREROUTING 链的规则可以存在于：raw表，mangle表，nat表。
INPUT链的规则可以存在于：mangle表,filter表（centos7中还有nat表）
FORWARD 链的规则可以存在于:mangle表，flter表
OUTPUT链的规则可以存在于：raw表mangle表，nat表，flter表
POSTROUTING链的规则可以存在于:mangle表，nat表

```

![image-20220326132620974](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220326132620974.png)

#### 2.5.1.1 匹配raw表的PREOUTING链

```
iptables有5个链
PREOUTING、INPUTFORWARD，OUTPUT，POSTROUTING

4个表
filter，nat,mangle,raw

4个表的优先级由高到低的顺序为:raw-->mangle-->nat-->filter
```

```
使用raw表解决ip_conntrack: table full、dropping packer的场景
RAW表可以应用在那些不需要做nar的情况下，可以提高性能，如大量访问的web服务器，可以让80端口不在让iptables做数据包的链接跟踪处理，以提高用户的访问速度

RAW表只使用在PREROUTING链和OUTPUT链上，因为优先级最高，从而可以对收到的数据包在连接跟踪前进行处理，一但用户使用了RAW表，在某个链上，RAW表处理完成后，将跳过NAT表和ip_conntrack处理，既不在做地址转换和数据包的连接跟踪处理了

如果pRROUTING链上,既有mangle表，也有nar表，那么先由mangle处理，然后由nar表处理
```

```
root@k8s-node1:~# iptables -t raw -vnL cali-PREROUTING
Chain cali-PREROUTING (1 references)
 pkts bytes target     prot opt in     out     source               destination         
88248  229M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:XFX5xbM8B9qR10JG */ MARK and 0xfff0ffff
 2781  224K MARK       all  --  cali+  *       0.0.0.0/0            0.0.0.0/0            /* cali:EWMPb0zVROM-woQp */ MARK or 0x40000
    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:mPIOOWmbH3iO0R90 */ mark match 0x40000/0x40000 rpfilter validmark invert
85467  229M cali-from-host-endpoint  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:8eOxmFpkWr0RjKXR */ mark match 0x0/0x40000
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:F-nExspj_POyei0v */ mark match 0x10000/0x10000

```

PREOUTING表的raw链主要是对报文进行打标机，没有执行访问转发操作

## 2.6 NetWorkPolicy

两个namespace,linux和python，分别代表不同项目的pod

每个namespace运行多个Pod，且pod可以运行在不同的node主机

测试环境为每个namespace分别运行一个nginx pod和一个tomcat pod用于测试不同主机的pod运行在同一个ns的场景，以及跨ns的访问通信限制

### 2.6.1 创建namespace及测试Pod

```
root@k8s-master1:~# kubectl create ns linux
namespace/linux created
root@k8s-master1:~# kubectl create ns python
namespace/python created
root@k8s-master1:~# kubectl label ns linux nsname=linux
namespace/linux labeled
root@k8s-master1:~# kubectl label ns python nsname=python
namespace/python labeled



#创建测试pod
root@k8s-master1:~# kubectl run net-test-pod1 --image=centos:7.9.2009 sleep 1000000 -n linux
pod/net-test-pod1 created
root@k8s-master1:~# kubectl run net-test-pod1 --image=centos:7.9.2009 sleep 1000000 -n python
pod/net-test-pod1 created
root@k8s-master1:~# kubectl run net-test-pod1 --image=centos:7.9.2009 sleep 1000000
pod/net-test-pod1 created
```

#### 2.6.1.1 linux Namespave nginx yaml 文件内容



```bash
root@k8s-master1:/apps# cat nginx.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: linux-nginx-deployment-label
  name: linux-nginx-deployment
  namespace: linux
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linux-nginx-selector
  template:
    metadata:
      labels:
        app: linux-nginx-selector
    spec:
      containers:
      - name: linux-nginx-container
        image: nginx:1.20.2-alpine 
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          protocol: TCP
          name: http
        - containerPort: 443
          protocol: TCP
          name: https
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: linux-nginx-service-label
  name: linux-nginx-service
  namespace: linux
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30004
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
    nodePort: 30443
  selector:
    app: linux-nginx-selector

```

#### 2.6.1.2 tomcat 文件内容

```bash
root@k8s-master1:/apps# cat tomcat.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: linux-tomcat-app1-deployment-label
  name: linux-tomcat-app1-deployment
  namespace: linux
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linux-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: linux-tomcat-app1-selector
    spec:
      containers:
      - name: linux-tomcat-app1-container
        image: tomcat:7.0.109-jdk8-openjdk 
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: linux-tomcat-app1-service-label
  name: linux-tomcat-app1-service
  namespace: linux
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30005
  selector:
    app: linux-tomcat-app1-selector

```

#### 2.6.1.3 创建tomcat首页

```bash
root@k8s-master1:/apps# kubectl exec -it linux-tomcat-app1-deployment-6f8864d5d9-jwfsx bash -n linux

root@linux-tomcat-app1-deployment-6f8864d5d9-jwfsx:/usr/local/tomcat/webapps# mkdir app
root@linux-tomcat-app1-deployment-6f8864d5d9-jwfsx:/usr/local/tomcat/webapps# echo "linux app" > app/index.jsp

```

#### 2.6.1.4 验证tomcat

![image-20220326190052409](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220326190052409.png)

#### 2.6.1.4 nginx 配置

```
root@k8s-master1:~# kubectl exec -it linux-nginx-deployment-5cd9566d7f-wl87m sh -n linux
/ # cat /etc/nginx/conf.d/default.conf
server {
.....
...
	location /app {
	proxy_pass http://linux-tomcat-app1-service;	
}
....

}

/ # nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
/ # nginx -s reload

```

#### 2.6.1.5 验证nginx

![image-20220326190932030](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220326190932030.png)

#### 2.6.1.6: python Namespace yaml文件

```bash
root@s3:/data/python# cat nginx.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: python-nginx-deployment-label
  name: python-nginx-deployment
  namespace: python
spec:
  replicas: 1
  selector:
    matchLabels:
      app: python-nginx-selector
  template:
    metadata:
      labels:
        app: python-nginx-selector
        project: python
    spec:
      containers:
      - name: python-nginx-container
        image: nginx:1.20.2-alpine
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          protocol: TCP
          name: http
        - containerPort: 443
          protocol: TCP
          name: https
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: python-nginx-service-label
  name: python-nginx-service
  namespace: python
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30014
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
    nodePort: 30453
  selector:
    app: python-nginx-selector
    project: python 




root@s3:/data/python# cat testpod-busybox.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: testpod-busybox-deployment-label
  name: testpod-busybox-deployment
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: python-nginx-selector
      project: python
  template:
    metadata:
      labels:
        app: python-nginx-selector
        project: python
    spec:
      containers:
      - name: testpod-busybox-container
        image: busybox:latest 
        command:
          - sleep
          - "50000000"
        imagePullPolicy: Always
root@s3:/data/python# cat tomcat.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: python-tomcat-app1-deployment-label
  name: python-tomcat-app1-deployment
  namespace: python
spec:
  replicas: 1
  selector:
    matchLabels:
      app: python-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: python-tomcat-app1-selector
    spec:
      containers:
      - name: python-tomcat-app1-container
        image: tomcat:7.0.109-jdk8-openjdk 
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: python-tomcat-app1-service-label
  name: python-tomcat-app1-service
  namespace: python
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30015
  selector:
    app: python-tomcat-app1-selector




```

#### 2.6.1.7: 创建tomcat首页

```bash
root@s3:/data/python# kubectl exec -it python-tomcat-app1-deployment-7996ff6f76-5b69x bash -n python
root@python-tomcat-app1-deployment-7996ff6f76-5b69x:/usr/local/tomcat# mkdir webapps/apps
root@python-tomcat-app1-deployment-7996ff6f76-5b69x:/usr/local/tomcat# echo "web python aap" > webapps/apps/index.jsp

```

![image-20220618135204374](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220618135204374.png)

#### 2.6.1.8: nginx配置

```bash
root@s3:/data/python# kubectl exec -it python-nginx-deployment-86c8f68749-b64x6 sh -n python
/ # apk add vim
/ # vi /etc/nginx/conf.d/default.conf
....
    location /apps {
        proxy_pass http://python-tomcat-app1-service;
        }
....
/ # nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
/ # nginx -s reload
```

![image-20220618135602123](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220618135602123.png)

### 2.6.2: 案例

#### 2.6.2.1：案例之限制pod单位，允许特定标签访问

* 不允许从其它namespace访问目标pod，既默认禁止了跨ns访问目标pod

* 非明确允许的pod同namespace也无法访问

* 不允许从宿主机访问目标pod

* 改策略只允许同namespace含有特定标签的源pod访问目标pod，比如tomcat只允许了有特定标签的源pod nginx访问

* 该策略不影响其它namespace的pod内部之间的相互访问，既linux的pod于linux的pod访问正常

* 该策略不影响各namespace的pod与非明确禁止的Pod之间的访问，既linux的pod访问python的其它Pod也正常

  

  ```bash
  root@s3:/data/case# cat case1-inageress-podselector.yaml 
  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: tomcat-access-networkpolicy
    namespace: python
  spec:
    policyTypes:
    - Ingress
    podSelector:
      matchLabels:
        app: python-tomcat-app1-selector #对匹配到的目的Pod应用以下规则
    ingress:#入栈规则，如果指定目标端口就是匹配全部端口和协议，协议TCP, UDP, or SCTP
    - from:
      - podSelector:
          matchLabels:
            app: python-nginx-selector #如果存在多个matchLabel条件，是or的关系，即要同时满足条件A、条件B、条件X
  
  root@s3:/data/case# kubectl apply -f case1-inageress-podselector.yaml 
  networkpolicy.networking.k8s.io/tomcat-access-networkpolicy created
  
  root@s3:/data/case# kubectl get networkpolicies.networking.k8s.io -n python
  NAME                          POD-SELECTOR                      AGE
  tomcat-access-networkpolicy   app=python-tomcat-app1-selector   2m1s
  
  ```

##### 2.6.2.1.1:  访问验证

```bash
#允许访问
root@s3:/data/case# kubectl exec -it python-nginx-deployment-86c8f68749-b64x6 sh -n python
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
/ # curl 10.200.78.154:8080/apps/index.jsp
web python aap

#使用不带python-nginx-selector标签访问
[root@net-test-pod1 /]# curl 10.200.78.154:8080/apps/index.jsp
#结果访问不到


#删除规则在进行访问
[root@net-test-pod1 /]# curl 10.200.78.154:8080/apps/index.jsp
web python aap

```

#### 2.6.2.2: 案例之以pod加端口限制为单位

只允许同namespace含有特定的标签源pod访问目标pod的指定端口

```bash
root@s3:/data/case# cat case2-inageress-podselector.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tomcat-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: python-tomcat-app1-selector
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: python-nginx-selector
    ports:
    - protocol: TCP
      port: 8080
```

##### 2.6.2.2.1: 验证

```bash
#允许8080端口可以访问
root@s3:/data/case# kubectl exec -it python-nginx-deployment-86c8f68749-b64x6 sh -n python
/ # curl 10.200.78.154:8080/apps/index.jsp
web python aap

#把文件8080端口改成80是否可以访问
root@s3:/data/case# kubectl delete -f case2-inageress-podselector.yaml
....
   ports:
    - protocol: TCP
      port: 80
      
#访问测试
/ # curl 10.200.78.154:8080/apps/index.jsp
#结果访问不到

1.只允许指定的源pod访问同namespace目标Pod的指定端口
2.非允许的端口将被禁止访问
```

#### 2.6.2.3： 案例之允许同namespace所有的Pod访问

```bash
root@s3:/data/case# cat case3-inageress-podselector.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tomcat-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Ingress
  podSelector: #目标POd
    matchLabels:
      app: python-tomcat-app1-selector
  ingress:
  - from:
    - podSelector:#匹配源pod,matchLabels: {}为不限制源pod即允许所有pod,写法等同于resources(不加就是不限制)
        matchLabels: {}
    ports:#入栈规则，如果指定目标端口就是匹配全部端口和协议，协议TCP, UDP, or SCTP
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 80

```

##### 2.6.2.3.1: 验证

```bash
root@s3:/data/case# kubectl exec -it net-test-pod1 bash -n python
[root@net-test-pod1 /]# curl 10.200.78.154:8080/apps/index.jsp
web python aap
```

#### 2.6.2.4: 允许同namespace的所有pod访问当前namespace的目标Pod所有端口

1.其它namepave 无法访问目标ns的pod

2.同namespace的pod可以访问当前namespace中的所有pod的任意端口

```
root@s3:/data/case# cat case4-inageress-podselector.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tomcat-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Ingress
  podSelector:#目标pod
    matchLabels: {}#匹配所有目标pod
  ingress:
  - from:
    - podSelector:#匹配源pod,matchLabels: {}为不限制源pod即允许所有pod,写法等同于resources(不加就是不限
        matchLabels: {}

```

#### 2.6.2.4：白名单

```bash
root@s3:/data/case# cat case6-inagess-ipblock.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tomcat-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: python-tomcat-app1-selector
  ingress:
  - from:
    - ipBlock:
        cidr: 10.200.0.0/16 #白名单，允许访问的地址范围，没有允许的将禁止访问目标pod
        except:
        - 10.200.218.0/24 #在以上范围内禁止访问的源IP地址
        - 10.200.229.0/24 #在以上范围内禁止访问的源IP地址
        - 10.200.78.153/24 #在以上范围内禁止访问的源IP地址
    ports: #入栈规则，如果指定目标端口就是匹配全部端口和协议，协议TCP, UDP, or SCTP
    - protocol: TCP
      port: 8080 #允许通过TCP协议访问目标pod的8080端口，但是其它没有允许的端口将全部禁止访问
      #port: 80
    - protocol: TCP
      port: 3306
    - protocol: TCP
      port: 6379

```



##### 2.6.2.4.1: 验证

```bash
root@s3:/data/case# kubectl get po -o wide -n linux
NAME                                            READY   STATUS    RESTARTS   AGE     IP               NODE             NOMINATED NODE   READINESS GATES
linux-nginx-deployment-5cd9566d7f-vmr6m         1/1     Running   0          5h26m   10.200.152.253   192.168.48.169   <none>           <none>
linux-tomcat-app1-deployment-6f8864d5d9-zrpbr   1/1     Running   0          5h26m   10.200.78.153    192.168.48.170   <none>           <none>
net-test-pod1                                   1/1     Running   0          6h10m   10.200.78.151    192.168.48.170   <none>           <none>
root@s3:/data/case# kubectl exec -it linux-tomcat-app1-deployment-6f8864d5d9-zrpbr bash -n linux
root@linux-tomcat-app1-deployment-6f8864d5d9-zrpbr:/usr/local/tomcat# curl 10.200.78.154:8080/apps/index.jsp 
#结果访问不到
1.只要在白名单范围内没有被except指定禁止的源pod ip 都允许访问
2.在只设置了ipBlock匹配的前提下，其它namespace中没有在except范围的Pod也可以访问目标pod，及linux nfs中的pod只要不在except地址范围内，也可也访问python ns中的Pod了
```

#### 2.6.2.5：案例之以namespace限制为单位

```bash
kubectl  label  ns linux nsname=linux
kubectl  label  ns python nsname=python


root@s3:/data/case# cat case6-inagess-ipblock.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tomcat-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Ingress
  podSelector: #目标pod
    matchLabels: {} #允许访问python namespace 中的所有pod
  ingress:
  - from:
    - namespaceSelector:
#        matchLabels: {} #允许所有namespace访问python namespace指定的目标端口或指定的pod加指定端口
        matchLabels:
          nsname: linux #只允许指定的namespace访问
    - namespaceSelector:
        matchLabels:
          nsname: python #只允许指定的namespace访问

```

##### 2.6.2.5.1: 验证

```bash
#允许指定ns访问
root@s3:/data/case# kubectl exec -it linux-tomcat-app1-deployment-6f8864d5d9-zrpbr bash -n linux
root@linux-tomcat-app1-deployment-6f8864d5d9-zrpbr:/usr/local/tomcat# curl 10.200.78.154:8080/apps/index.jsp
web python aap

1.被明确允许的ns中的Pod可以访问目标Pod
2.没有明确声明允许的ns将禁止访问
3.没有明确声明允许的话，即使同一个namsepace也禁止访问
4.比如只允许了linux和python两个ns，那么default中的pod将无法访问
```

#### 2.6.2.6： Egress出栈限制

出口方向目的IP及目的端口限制0只允许访问指定的目的地址范围及端口

```bash
root@s3:/data/case# kubectl exec -it linux-tomcat-app1-deployment-6f8864d5d9-zrpbr  bash -n linux
root@s3:/data/case# cat case7-Egress-ipBlock.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Egress
  podSelector: #目标pod选择器
    matchLabels: #基于label匹配目标pod
      app: python-tomcat-app1-selector #匹配python namespace中app的值为python-tomcat-app1-selector的pod,然后基于egress中的指定网络策略进行出口方向的网络限制
  egress:
  - to:
    - ipBlock:
        cidr: 10.200.0.0/16 #允许匹配到的pod出口访问的目的CIDR地址范围
    - ipBlock:
        cidr: 192.168.48.166/24 #允许匹配到的pod出口访问的目的主机
    ports:
    - protocol: TCP
      port: 80 #允许匹配到的pod访问目的端口为80的访问
    - protocol: TCP
      port: 53 #允许匹配到的pod访问目的端口为53 即DNS的解析
    - protocol: UDP
      port: 53 #允许匹配到的pod访问目的端口为53 即DNS的解析

```

##### 2.6.2.6.1: 验证

```bash
#访问80
root@linux-tomcat-app1-deployment-6f8864d5d9-zrpbr:/usr/local/tomcat# curl 10.200.152.253:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
#访问8080 
root@linux-tomcat-app1-deployment-6f8864d5d9-zrpbr:/usr/local/tomcat# curl 10.200.78.154:8080/apps/index.jsp

1.基于Egress白名单，定义ns中匹配成功的pod可以访问ipBlock指定的地址和ports指定的端口
2.匹配成功的pod访问未明确定义在Egress的白名单的其它IP的请求，将拒绝
3.没有匹配成功的源pod，主动发情的出口访问请求不受影响
```

#### 2.6.2.7: 出口方向目的Pod限制-只允许指定的pod及端口

```bash
root@s3:/data/case# cat case8-Egress-ipBlock.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Egress
  podSelector:
    matchLabels:
      app: python-nginx-selector
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: python-tomcat-app1-selector
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

```

##### 2.6.2.7.1: 验证

```bash
root@s3:/data/case# kubectl exec -it python-nginx-deployment-86c8f68749-b64x6 sh -n pytho
/ # curl 10.200.78.154:8080/apps/index.jsp
web python aap

1.匹配成功的源pod只能访问指定的目的pod的指定端口
2.其它没有允许的出口请求禁止访问
```

#### 2.6.2.8： 指定ns访问目标pod

```bash
root@s3:/data/case# cat case9-inagess-ipblock.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-access-networkpolicy
  namespace: python
spec:
  policyTypes:
  - Egress
  podSelector:#目标pod选择器
    matchLabels:#基于label匹配目标pod
      app: python-nginx-selector#匹配python namespace中app的值为python-tomcat-app1-selector的pod,然后基于egress中的指定网络策略进行出口方向的网络限制
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          nsname: python#指定允许访问的目的namespace
    - namespaceSelector:
        matchLabels:
          nsname: linux#指定允许访问的目的namespace
    ports:
    - protocol: TCP
      port: 8080 #允许80端口的访问
    - protocol: TCP
      port: 53 #允许DNS的解析
    - protocol: UDP
      port: 53


1.匹配成功的源pod可以指定的目标ns
2.不能访问除指定的ns以外的其它ns及外网

```



## 2.7 ingress

### 2.7.1 ingress控制器简介

https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/index.md  #安装部署 
https://docs.nginx.com/nginx-ingress-controller/intro/overview/
https://kubernetes.io/docs/concepts/services-networking/ingress/
https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/  #ingress 项目



![image-20220327112814587](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327112814587.png)

#### 2.7.1.1 创建ingress控制器

```
root@k8s-master1:/apps/ingress# cat 1.0.ingress-control-k8s-v1.22.yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx

---
# Source: ingress-nginx/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: ingress-nginx
automountServiceAccountToken: true
---
# Source: ingress-nginx/templates/controller-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: ingress-nginx
data:
---
# Source: ingress-nginx/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
  name: ingress-nginx
rules:
  - apiGroups:
      - ''
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ''
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
---
# Source: ingress-nginx/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: ingress-nginx
---
# Source: ingress-nginx/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: ingress-nginx
rules:
  - apiGroups:
      - ''
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ''
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - configmaps
    resourceNames:
      - ingress-controller-leader
    verbs:
      - get
      - update
  - apiGroups:
      - ''
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ''
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: ingress-nginx/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: ingress-nginx
---
# Source: ingress-nginx/templates/controller-service-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller-admission
  namespace: ingress-nginx
spec:
  type: ClusterIP
  ports:
    - name: https-webhook
      port: 443
      targetPort: webhook
      appProtocol: https
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller
---
# Source: ingress-nginx/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/component: controller
  revisionHistoryLimit: 10
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/component: controller
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirst
      containers:
        - name: controller
          image: registry.cn-beijing.aliyuncs.com/kole_chang/controller:v1.0.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown
          args:
            - /nginx-ingress-controller
            - --election-id=ingress-controller-leader
            - --controller-class=k8s.io/ingress-nginx
            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
            - --validating-webhook=:8443
            - --validating-webhook-certificate=/usr/local/certificates/cert
            - --validating-webhook-key=/usr/local/certificates/key
            - --watch-ingress-without-class=true
          securityContext:
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            runAsUser: 101
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: LD_PRELOAD
              value: /usr/local/lib/libmimalloc.so
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
            - name: webhook
              containerPort: 8443
              protocol: TCP
          volumeMounts:
            - name: webhook-cert
              mountPath: /usr/local/certificates/
              readOnly: true
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
        - name: webhook-cert
          secret:
            secretName: ingress-nginx-admission
---
# Source: ingress-nginx/templates/controller-ingressclass.yaml
# We don't support namespaced ingressClass yet
# So a ClusterRole and a ClusterRoleBinding is required
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: nginx
  namespace: ingress-nginx
spec:
  controller: k8s.io/ingress-nginx
---
# Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml
# before changing this value, check the required kubernetes version
# https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
  name: ingress-nginx-admission
webhooks:
  - name: validate.nginx.ingress.kubernetes.io
    matchPolicy: Equivalent
    rules:
      - apiGroups:
          - networking.k8s.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - ingresses
    failurePolicy: Fail
    sideEffects: None
    admissionReviewVersions:
      - v1
    clientConfig:
      service:
        namespace: ingress-nginx
        name: ingress-nginx-controller-admission
        path: /networking/v1/ingresses
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress-nginx-admission
  namespace: ingress-nginx
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ingress-nginx-admission
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
    verbs:
      - get
      - update
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ingress-nginx-admission
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx-admission
subjects:
  - kind: ServiceAccount
    name: ingress-nginx-admission
    namespace: ingress-nginx
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ingress-nginx-admission
  namespace: ingress-nginx
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
rules:
  - apiGroups:
      - ''
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ingress-nginx-admission
  namespace: ingress-nginx
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
  - kind: ServiceAccount
    name: ingress-nginx-admission
    namespace: ingress-nginx
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ingress-nginx-admission-create
  namespace: ingress-nginx
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
spec:
  template:
    metadata:
      name: ingress-nginx-admission-create
      labels:
        helm.sh/chart: ingress-nginx-4.0.1
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/version: 1.0.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: admission-webhook
    spec:
      containers:
        - name: create
          image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
            - --namespace=$(POD_NAMESPACE)
            - --secret-name=ingress-nginx-admission
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ingress-nginx-admission-patch
  namespace: ingress-nginx
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
spec:
  template:
    metadata:
      name: ingress-nginx-admission-patch
      labels:
        helm.sh/chart: ingress-nginx-4.0.1
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/version: 1.0.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: admission-webhook
    spec:
      containers:
        - name: patch
          image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=ingress-nginx-admission
            - --namespace=$(POD_NAMESPACE)
            - --patch-mutating=false
            - --secret-name=ingress-nginx-admission
            - --patch-failure-policy=Fail
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 2000
        
        
        
root@k8s-master1:/apps/ingress# kubectl apply -f 1.0.ingress-control-k8s-v1.22.yaml 
```

![image-20220327135235452](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327135235452.png)

#### 2.7.1.2 创建tomcat app1

```
root@k8s-master1:/apps/ingress# cat tomcat-app1.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: tomcat-app1 
  name: tomcat-app1
  namespace: n60
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat-app1
  template:
    metadata:
      labels:
        app: tomcat-app1
    spec:
      containers:
      - name: tomcat-app1
        image: tomcat:7.0.94-alpine 
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: tomcat-app1
  name: tomcat-app1
  namespace: n60
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30008
  selector:
    app: tomcat-app1

```

tomcat app2

```
root@k8s-master1:/apps/ingress# cat tomcat-app2.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: tomcat-app2
  name: tomcat-app2
  namespace: n60
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat-app2
  template:
    metadata:
      labels:
        app: tomcat-app2
    spec:
      containers:
      - name: tomcat-app2
        image: tomcat:7.0.94-alpine 
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: tomcat-app2
  name: tomcat-app2
  namespace: n60
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30009
  selector:
    app: tomcat-app2

```

#### 2.7.1.3 验证app1 app2

![image-20220619150444988](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220619150444988.png)





![image-20220327144537761](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327144537761.png)

#### 2.7.1.4 实现单个域名的ingress

```
root@k8s-master1:/apps/ingress# cat 2.1.ingress_single-host.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-web
  namespace: n60
  annotations:
    kubernetes.io/ingress.class: "nginx" ##指定Ingress Controller的类型
    nginx.ingress.kubernetes.io/use-regex: "true" ##指定后面rules定义的path可以使用正则表达式
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600" ##连接超时时间,默认为5s
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600" ##后端服务器回转数据超时时间,默认为60s
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600" ##后端服务器响应超时时间,默认为60s
    nginx.ingress.kubernetes.io/proxy-body-size: "50m" ##客户端上传文件，最大大小，默认为20m
    nginx.ingress.kubernetes.io/app-root: /index.html
spec:
  rules:
  - host: www.yangge.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: tomcat-app1
            port:
              number: 80


root@k8s-master1:/apps/ingress# kubectl get ingress -n n60
NAME        CLASS    HOSTS            ADDRESS                                                       PORTS   AGE
nginx-web   <none>   www.yangge.com   192.168.48.163,192.168.48.164,192.168.48.165,192.168.48.166   80      58s

```

#### 2.7.1.5 验证

![image-20220327145725417](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327145725417.png)

#### 2.7.1.6 实现多域名的ingress

```
root@k8s-master1:/apps/ingress# cat 2.2.ingress_multi-host.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-web
  namespace: n60
  annotations:
    kubernetes.io/ingress.class: "nginx" ##指定Ingress Controller的类型
    nginx.ingress.kubernetes.io/use-regex: "true" ##指定后面rules定义的path可以使用正则表达式
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600" ##连接超时时间,默认为5s
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600" ##后端服务器回转数据超时时间,默认为60s
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600" ##后端服务器响应超时时间,默认为60s
    nginx.ingress.kubernetes.io/proxy-body-size: "10m" ##客户端上传文件，最大大小，默认为20m
    nginx.ingress.kubernetes.io/app-root: /index.html
spec:
  rules:
  - host: www.yangge.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: tomcat-app1
            port:
              number: 80
  - host: www.lcy.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: tomcat-app2
            port:
              number: 80


root@k8s-master1:/apps/ingress# kubectl get ingress -n n60
NAME        CLASS    HOSTS                        ADDRESS                                                       PORTS   AGE
nginx-web   <none>   www.yangge.com,www.lcy.com   192.168.48.163,192.168.48.164,192.168.48.165,192.168.48.166   80      56s

```

![image-20220327150141685](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327150141685.png)

![image-20220327150159334](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327150159334.png)

![image-20220327150211299](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327150211299.png)

#### 2.7.1.7 实现基于URL请求流量转发的ingress

```
root@k8s-master1:/apps/ingress# cat ingress-url.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-web
  namespace: n60
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    nginx-ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx-inagess.kubernetes.io/proxy-body-size: "10m"
    nginx.inagess.kubernetes.io/app-root: /index.html
spec:
  rules:
  - host: www.yangge.com
    http:
      paths:
      - pathType: Prefix
        path: "/app1"
        backend:
          service:
            name: tomcat-app1
            port:
              number: 80
      - pathType: Prefix
        path: "/app2"
        backend:
          service:
            name: tomcat-app2
            port:
              number: 80    

```

![image-20220327151506377](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327151506377.png)

![image-20220327151514506](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327151514506.png)

#### 2.7.1.8 实现单个htpps域名的ingress

##### 2.7.1.8.1 签发证书

```
root@k8s-master1:/apps/ingress# mkdir certs
root@k8s-master1:/apps/ingress# cd certs/
root@k8s-master1:/apps/ingress/certs# openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 3560 -nodes -subj '/CN=www.yangge.com'
Can't load /root/.rnd into RNG
139932803883456:error:2406F079:random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/root/.rnd
Generating a RSA private key
.................................................................++++
...++++
writing new private key to 'ca.key'
-----

root@k8s-master1:/apps/ingress/certs#openssl req -new -newkey rsa:4096 -keyout server.key -out server.csr -nodes -subj '/CN=www.yangge.com'

root@k8s-master1:/apps/ingress/certs# openssl x509 -req -sha256 -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt 
Signature ok
subject=CN = www.yangge.com
Getting CA Private Key

```

##### 2.7.1.8.2 证书上传至k8s：

```
root@k8s-master1:/apps/ingress/certs# kubectl create secret generic tls-secret --from-file=tls.crt=server.crt --from-file=tls.key=server.key -n n60

root@k8s-master1:/apps/ingress/certs# kubectl get secret -n n60
NAME                  TYPE                                  DATA   AGE
default-token-cjcwb   kubernetes.io/service-account-token   3      99m
tls-secret            Opaque                                2      36s

```

##### 2.7.1.8.3 验证

![image-20220327154047727](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327154047727.png)

#### 2.7.1.9 实现多个https域名的ingress

##### 2.7.1.9.1 证书签发

```
root@k8s-master1:/apps/ingress/certs# openssl req -new -newkey rsa:4096   -keyout lcy.key -out lcy.csr -nodes -subj '/CN=www.lcy.com'

root@k8s-master1:/apps/ingress/certs# openssl x509 -req -sha256 -days 3650 -in lcy.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out lcy.crt
Signature ok
subject=CN = www.lcy.com
Getting CA Private Key

root@k8s-master1:/apps/ingress/certs# kubectl create secret generic lcy-tls-secret --from-file=tls.crt=lcy.crt --from-file=tls.key=lcy.key -n n60
secret/lcy-tls-secret created

```

```
root@k8s-master1:/apps/ingress# cat ingress-https1.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-web
  namespace: n60
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - www.yangge.com
    secretName: tls-secret
  - hosts:
    - www.lcy.com
    secretName: lcy-tls-secret
  rules:
  - host: www.yangge.com
    http:
      paths:
      - pathType: Prefix
        path: "/app1"
        backend:
          service:
            name: tomcat-app1
            port:
              number: 80
  - host: www.lcy.com
    http:
      paths:
      - pathType: Prefix
        path: "/app2"
        backend:
          service:
            name: tomcat-app2
            port:
              number: 80

```

##### 2.7.1.9.2 验证

![image-20220327160302806](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327160302806.png)

![image-20220327160313415](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327160313415.png)

## 2.8: StatefulSet

### 2.8.1什么是 StatefulSet

Pod 的管理对象 RC、Deployment、DaemonSet 和 Job 都是面向无状态的服务，但实际中有很多服务是有状态的，比如 Mysql集群、MongoDB集群、ZooKeeper集群等，可以使用 StatefulSet 来管理有状态的服务。

StatefulSet 是Kubernetes中的一种控制器，他解决的什么问题呢？我们知道Deployment是对应用做了一个简化设置，Deployment认为一个应用的所有的pod都是一样的，他们之间没有顺序，也无所谓在那台宿主机上。需要扩容的时候就可以通过pod模板加入一个，需要缩容的时候就可以任意杀掉一个。但是实际的场景中，并不是所有的应用都能做到没有顺序等这种状态，尤其是分布式应用，他们各个实例之间往往会有对应的关系，例如：主从、主备。还有数据存储类应用,它的多个实例，往往会在本地磁盘存一份数据，而这些实例一旦被杀掉，即使从建起来，实例与数据之间关系也会丢失，而这些实例有不对等的关系，实例与外部存储有依赖的关系的应用，被称作“有状态应用”。StatefulSet与Deployment相比，相同于他们管理相同容器规范的Pod，不同的时候，StatefulSet为pod创建一个持久的标识符，他可以在任何编排的时候得到相同的标识符。
StatefulSet 有如下一些特性：

- StatefulSet 里的每个 Pod 都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设 StatefulSet 的名字叫 kafka，那么第1个 Pod 叫 kafka-0，第2个叫kafka-1，以此类推。
- StatefulSet 控制的 Pod 副本的启停顺序是受控的，操作第 n 个 Pod 时，前 n-1 个 Pod 已经是运行且准备好的状态。
- StatefulSet 里的 Pod 采用稳定的持久化存储卷，通过 PV/PVC 来实现，删除 Pod 时默认不会删除与 StatefulSet 相关的存储卷（为了保证数据的安全）。

StatefulSet 除了要与PV卷捆绑使用以存储 Pod 的状态数据，还要与 Headless Service(无头服务) 配合使用，即在每个 StatefulSet 的定义中要声明它属于哪个 Headless Service。Headless Service 与普通 Service 的区别在于，它没有 Cluster IP，如果解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应的全部 Pod 的 Endpoint 列表。StatefulSet 在 Headless Service 的基础上又为 StatefulSet 控制的每个 Pod 实例创建了一个 DNS 域名，这个域名的格式为：

```shell
$(podname).$(headless service name)
```

比如一个 3 节点的 kafka 的 StatefulSet 集群，对应的 Headless Service 的名字为 kafka，StatefulSet 的名字为 kafka，则 StatefulSet 里面的 3 个 Pod 的 DNS 名称分别为kafka-0.kafka、kafka-1.kafka、kafka-2.kafka，这些 DNS 名称可以直接在集群的配置文件中固定下来。

### 2.8.2StatefulSet 的应用特点

- 稳定且有唯一的网络标识符 当节点挂掉，既pod重新调度后其PodName和HostName不变，基于Headless Service来实现
- 稳定且持久的存储  当节点挂掉，既pod重新调度能访问到相同的持久化存储，基于PVC 实现
- 有序、平滑的扩展、部署 即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现。
- 有序、平滑的收缩、删除 既Pod是有顺序的，在收缩或者删除的时候要依据定义的顺序依次进行（既从N-1到0，既倒序）。

我们可以把这些抽象成两种应用状态:

- **拓扑状态。**是应用多个实例之间的不完全对等的关系，这些应用实例是必须按照定义的顺序启动的。例如主应用A先于从应用B启动，如果把A和B删掉，应用还要按照先启动主应用A再启动从应用B，且创建的应用必须和原来的应用的网络标识一样（既PodName和HostName）。这样他们就可以按照原来的顺序创建了。

- 之间不是完全对等的关系

  极客时间版权所有: https://time.geekbang.org/column/article/41017

  存储状态。

  应用实例分别绑定了不同的数据存储，Pod A第一次读到的数据要和10分钟后读到的数据是同一份。哪怕这期间Pod A被重建。这种典型的例子就是数据库应用的多个存储实例。

所以 StatefulSet的核心功能就是，通过某种方式记录应用状态，在Pod被重建的时候，通过这种方式还可以恢复原来的状态。

从上面的应用场景可以发现，StatefulSet由以下几个部分组成：

- Headless Service 用于定义网络标识（DNS）
- volumeClaimTemplates  用于创建PV
- StatefulSet  用于定义具体应用

### 2.8.3: Headless Service

我们知道kubernetes中的service是定义pod暴露外部访问的一种机制，例如：3个pod，我们可以定义一个service通过标签选择器选到这三个pod，然后这个service就可以访问这个pod。我们这里具体讲一下Headless service。

Headless service是Service通过DNS访问的其中一种方式，只要我们访问"mypod.stsname.namespace.svc.cluster.local"，我们就会访问到stsname下的mypod。而Service DNS的方式下有两种处理方法：

- 标准服务 Normal Service 这里访问"mypod.stsname.namespace.svc.cluster.local"的时候会得到mypod的service的IP，既VIP。
- 无头服务 Headless Service 这里访问"mypod.stsname.namespace.svc.cluster.local"的时候会得到mypod的IP，这里我们可以看到区别是，Headless Service 不需要分配一个VIP，而是通过DNS访问的方式可以解析出带代理的 Pod 的 IP

Headleaa Service的定义方式：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-headless
  namespace: default
spec:
  selector:
    app: myapp
    release: dev
  clusterIP: "None"
  ports:
  - port: 80
    targetPort: 80
```

Headless Service也是一个标准的Service的YAML文件，只不过clusterIP定义为None，既，这个service没有VIP作为"头"。以DNS会记录的方式记录所有暴露它代理的Pod。而它代理的Pod依然会采用标签选择器的机制选择。既：所有携带了 app=myapp 标签的pod。都会被service代理起来。

kubectl explain sts.spec 主要字段解释：

- replicas   副本数
- selector  那个pod是由自己管理的
- serviceName  必须关联到一个无头服务商
- template 定义 pod 模板（其中定义关联那个存储卷）
- volumeClaimTemplates  生成PVC

### 2.8.4: 实例 

#### 2.8.4.1 nfs准备

```bash
root@s5:~# vim /etc/exports 
/nfs/v1 *(rw,no_root_squash)
/nfs/v2 *(rw,no_root_squash)
/nfs/v3 *(rw,no_root_squash)
/nfs/v4 *(rw,no_root_squash)
/nfs/v5 *(rw,no_root_squash)
#配置生效
root@s5:~# exportfs -r

root@s5:~# showmount -e
Export list for s5:
/nfs/v4 *
/nfs/v5 *
/nfs/v3 *
/nfs/v2 *
/nfs/v1 *
```

#### 2.8.4.2：PVyaml内容

```bash
root@s3:/data/pv# cat nfs-persistentvolume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-persistentvolume
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/v1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-persistentvoume-1
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce 
  nfs:
    server: 192.168.48.164
    path: /nfs/v2
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-persistentvolume-2
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/v3
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-persistentvolume-3
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/v4
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-persistentvolume-4
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    server: 192.168.48.164
    path: /nfs/v5


root@s3:/data/pv# kubectl get pv
NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs-persistentvolume     2Gi        RWO            Retain           Available                                   42s
nfs-persistentvolume-2   2Gi        RWO            Retain           Available                                   42s
nfs-persistentvolume-3   2Gi        RWO            Retain           Available                                   42s
nfs-persistentvolume-4   2Gi        RWO            Retain           Available                                   42s
nfs-persistentvoume-1    2Gi        RWO            Retain           Available                                   42s

```

#### 2.8.4.3: StatefulSet文件内容

```bash
root@s3:/data/pv# cat status.yaml 
apiVersion: v1
kind: Service
metadata:
  name: myapp
  labels:
    app: myapp-pod
spec:
  clusterIP: None
  ports:
  - name: http
    port: 80
  selector:
    app: myapp
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp-pod
spec:
  serviceName: myapp
  replicas: 3
  selector:
    matchLabels:
      app: myapp-pod
  template:
    metadata:
      labels:
        app: myapp-pod
    spec:
      containers:
      - name: web
        image: ikubernetes/myapp:v1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - name: myappdata
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: myappdata
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
          
root@s3:/data/pv# kubectl apply -f status.yaml 
service/myapp created
statefulset.apps/myapp-pod configured

root@s3:/data/pv# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   16d
myapp        ClusterIP   None         <none>        80/TCP    76s



myapp-pod   2/2     4m4sroot@s3:/data/pv# kubectl get sts
NAME        READY   AGE
myapp-pod   3/3     15s
root@s3:/data/pv# kubectl get pvc
NAME                    STATUS   VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myappdata-myapp-pod-0   Bound    nfs-persistentvolume     2Gi        RWO                           59s
myappdata-myapp-pod-1   Bound    nfs-persistentvoume-1    2Gi        RWO                           55s
myappdata-myapp-pod-2   Bound    nfs-persistentvolume-2   2Gi        RWO                           51s

```

#### 2.8.4.4: 有序删除

```bash
root@s3:/data/pv# kubectl delete -f status.yaml 
service "myapp" deleted
statefulset.apps "myapp-pod" deleted
root@s3:/data/pv# kubectl get po -w
NAME          READY   STATUS    RESTARTS   AGE
myapp-pod-0   1/1     Running   0          4m41s
myapp-pod-1   1/1     Running   0          4m37s
myapp-pod-2   1/1     Running   0          4m33s
myapp-pod-0   1/1     Terminating   0          4m55s
myapp-pod-1   1/1     Terminating   0          4m51s
myapp-pod-2   1/1     Terminating   0          4m47s
myapp-pod-0   0/1     Terminating   0          4m57s
myapp-pod-0   0/1     Terminating   0          4m57s
myapp-pod-0   0/1     Terminating   0          4m57s
myapp-pod-2   0/1     Terminating   0          4m50s
myapp-pod-2   0/1     Terminating   0          4m50s
myapp-pod-2   0/1     Terminating   0          4m50s
myapp-pod-1   0/1     Terminating   0          4m54s
myapp-pod-1   0/1     Terminating   0          4m54s
myapp-pod-1   0/1     Terminating   0          4m54s

```

#### 2.8.4.5: 有序创建

```bash
root@s3:/data/pv# kubectl get po -w
NAME          READY   STATUS    RESTARTS   AGE
myapp-pod-0   0/1     Pending   0          0s
myapp-pod-0   0/1     Pending   0          0s
myapp-pod-0   0/1     ContainerCreating   0          0s
myapp-pod-0   1/1     Running             0          3s
myapp-pod-1   0/1     Pending             0          0s
myapp-pod-1   0/1     Pending             0          0s
myapp-pod-1   0/1     ContainerCreating   0          0s
myapp-pod-1   1/1     Running             0          2s
myapp-pod-2   0/1     Pending             0          0s
myapp-pod-2   0/1     Pending             0          0s
myapp-pod-2   0/1     ContainerCreating   0          0s
myapp-pod-2   1/1     Running             0          3s

```

#### 2.8.4.6：有序扩展

```bash
root@s3:/data/pv# kubectl scale sts myapp-pod --replicas=5
statefulset.apps/myapp-pod scaled
root@s3:/data/pv# kubectl get po -w
NAME          READY   STATUS    RESTARTS   AGE
myapp-pod-0   1/1     Running   0          35s
myapp-pod-1   1/1     Running   0          30s
myapp-pod-2   1/1     Running   0          26s
myapp-pod-3   0/1     Pending   0          0s
myapp-pod-3   0/1     Pending   0          0s
myapp-pod-3   0/1     Pending   0          1s
myapp-pod-3   0/1     ContainerCreating   0          1s
myapp-pod-3   1/1     Running             0          4s
myapp-pod-4   0/1     Pending             0          0s
myapp-pod-4   0/1     Pending             0          0s
myapp-pod-4   0/1     Pending             0          1s
myapp-pod-4   0/1     ContainerCreating   0          1s
myapp-pod-4   1/1     Running             0          4s

```

#### 2.8.4.7：有序缩容

```bash
root@s3:~# kubectl patch sts myapp-pod -p '{"spec":{"replicas":2}}'
statefulset.apps/myapp-pod patched
root@s3:/data/pv# kubectl get po -w
NAME          READY   STATUS    RESTARTS   AGE
myapp-pod-0   1/1     Running   0          112s
myapp-pod-1   1/1     Running   0          107s
myapp-pod-2   1/1     Running   0          103s
myapp-pod-3   1/1     Running   0          59s
myapp-pod-4   1/1     Running   0          55s
myapp-pod-4   1/1     Terminating   0          103s
myapp-pod-4   0/1     Terminating   0          105s
myapp-pod-4   0/1     Terminating   0          105s
myapp-pod-4   0/1     Terminating   0          105s
myapp-pod-3   1/1     Terminating   0          109s
myapp-pod-3   0/1     Terminating   0          110s
myapp-pod-3   0/1     Terminating   0          110s
myapp-pod-3   0/1     Terminating   0          110s
myapp-pod-2   1/1     Terminating   0          2m34s
myapp-pod-2   0/1     Terminating   0          2m36s
myapp-pod-2   0/1     Terminating   0          2m36s
myapp-pod-2   0/1     Terminating   0          2m36s

```

#### 2.8.4.8: 滚动跟新（金丝雀发布）

```bash
root@s3:~# kubectl patch sts myapp-pod -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":4}}}}'
statefulset.apps/myapp-pod patched
root@s3:~# kubectl describe sts myapp-pod
Name:               myapp-pod
Namespace:          default
CreationTimestamp:  Sun, 19 Jun 2022 19:46:12 +0800
Selector:           app=myapp-pod
Labels:             <none>
Annotations:        <none>
Replicas:           5 desired | 5 total
Update Strategy:    RollingUpdate
  Partition:        4
Pods Status:        5 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:


root@s3:/data/pv# kubectl set image sts/myapp-pod web=ikubernetes/myapp:v2
statefulset.apps/myapp-pod image updated

root@s3:/data/pv# kubectl get sts -o wide
NAME        READY   AGE    CONTAINERS   IMAGES
myapp-pod   4/5     168m   web          ikubernetes/myapp:v2

root@s3:/data/pv# kubectl describe po myapp-pod-4 | grep image
  Normal  Pulled     65s   kubelet            Container image "ikubernetes/myapp:v2" already present on machine


root@s3:/data/pv# kubectl patch sts myapp-pod -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'
statefulset.apps/myapp-pod patched

root@s3:/data/pv# kubectl get po -w
NAME          READY   STATUS    RESTARTS   AGE
myapp-pod-0   1/1     Running   0          170m
myapp-pod-1   1/1     Running   0          170m
myapp-pod-2   1/1     Running   0          48m
myapp-pod-3   1/1     Running   0          48m
myapp-pod-4   1/1     Running   0          114s
myapp-pod-3   1/1     Terminating   0          49m
myapp-pod-3   0/1     Terminating   0          49m
myapp-pod-3   0/1     Terminating   0          49m
myapp-pod-3   0/1     Terminating   0          49m
myapp-pod-3   0/1     Pending       0          0s
myapp-pod-3   0/1     Pending       0          0s
myapp-pod-3   0/1     ContainerCreating   0          0s
myapp-pod-3   1/1     Running             0          4s
myapp-pod-2   1/1     Terminating         0          49m
myapp-pod-2   0/1     Terminating         0          49m
myapp-pod-2   0/1     Terminating         0          49m
myapp-pod-2   0/1     Terminating         0          49m
myapp-pod-2   0/1     Pending             0          0s
myapp-pod-2   0/1     Pending             0          0s
myapp-pod-2   0/1     ContainerCreating   0          0s
myapp-pod-2   1/1     Running             0          5s
myapp-pod-1   1/1     Terminating         0          171m
myapp-pod-1   0/1     Terminating         0          171m
myapp-pod-1   0/1     Terminating         0          171m
myapp-pod-1   0/1     Terminating         0          171m
myapp-pod-1   0/1     Pending             0          0s
myapp-pod-1   0/1     Pending             0          0s
myapp-pod-1   0/1     ContainerCreating   0          0s
myapp-pod-1   1/1     Running             0          3s
myapp-pod-0   1/1     Terminating         0          171m
myapp-pod-0   0/1     Terminating         0          171m
myapp-pod-0   0/1     Terminating         0          171m
myapp-pod-0   0/1     Terminating         0          171m
myapp-pod-0   0/1     Pending             0          0s
myapp-pod-0   0/1     Pending             0          0s
myapp-pod-0   0/1     ContainerCreating   0          0s
myapp-pod-0   1/1     Running             0          5s

root@s3:/data/pv# kubectl describe po myapp-pod-0 | grep image
  Normal  Pulled     66s   kubelet            Container image "ikubernetes/myapp:v2" already present on machine
```



## 2.9: Service Account

Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的。它与User account不同

- User     account是为人设计的，而service account则是为Pod中的进程调用Kubernetes API而设计；

- User     account是跨namespace的，而service account则是仅局限它所在的namespace；

- 每个namespace都会自动创建一个default     service account

- Token controller检测service account的创建，并为它们创建[secret](https://www.kubernetes.org.cn/secret)

- 开启ServiceAccount     Admission Controller后

- - 每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout）
  - 验证Pod引用的service      account已经存在，否则拒绝创建
  - 如果Pod没有指定ImagePullSecrets，则把service      account的ImagePullSecrets加到Pod中
  - 每个container启动后都会挂载该service      account的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/

 当创建 pod 的时候，如果没有指定一个 service account，系统会自动在与该pod 相同的 namespace 下为其指派一个default service account。而pod和apiserver之间进行通信的账号，称为serviceAccountName。如下：

```bash
[root@k8s-master ~]# kubectl describe pods myapp-0
Name:         myapp-0
Namespace:    default
Priority:     0
default-token-kwbkn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-kwbkn
    Optional:    false

```

从上面可以看到每个Pod无论定义与否都会有个存储卷，这个存储卷为default-token-*** token令牌，这就是pod和serviceaccount认证信息。通过secret进行定义，由于认证信息属于敏感信息，所以需要保存在secret资源当中，并以存储卷的方式挂载到Pod当中。从而让Pod内运行的应用通过对应的secret中的信息来连接apiserver，并完成认证。每个 namespace 中都有一个默认的叫做 default 的 service account 资源。进行查看名称空间内的secret，也可以看到对应的default-token。让当前名称空间中所有的pod在连接apiserver时可以使用的预制认证信息，从而保证pod之间的通信

```bash
[root@k8s-master ~]# kubectl get sa
NAME      SECRETS   AGE
default   1         22d
[root@k8s-master ~]# kubectl get sa -n ingress-nginx
NAME                           SECRETS   AGE
default                        1         17d
nginx-ingress-serviceaccount   1         17d
[root@k8s-master ~]# kubectl get secret
NAME                  TYPE                                  DATA   AGE
basic-auth            Opaque                                1      17d
default-token-kwbkn   kubernetes.io/service-account-token   3      22d
mysecret              Opaque                                2      6d4h
tls-secret            kubernetes.io/tls                     2      17d
[root@k8s-master ~]# kubectl get secret -n ingress-nginx
NAME                                       TYPE                                  DATA   AGE
default-token-fcn27                        kubernetes.io/service-account-token   3      17d
nginx-ingress-serviceaccount-token-848j5   kubernetes.io/service-account-token   3      17d

```

而默认的service account 仅仅只能获取当前Pod自身的相关属性，无法观察到其他名称空间Pod的相关属性信息。如果想要扩展Pod，假设有一个Pod需要用于管理其他Pod或者是其他资源对象，是无法通过自身的名称空间的serviceaccount进行获取其他Pod的相关属性信息的，此时就需要进行手动创建一个serviceaccount，并在创建Pod时进行定义。那么serviceaccount该如何进行定义呢？？？实际上，service accout也属于一个k8s资源，如下查看service account的定义方式：

 

```bash
[root@k8s-master ~]# kubectl explain sa
KIND:     ServiceAccount
VERSION:  v1

DESCRIPTION:
     ServiceAccount binds together: * a name, understood by users, and perhaps
     by peripheral systems, for an identity * a principal that can be
     authenticated and authorized * a set of secrets

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   automountServiceAccountToken	<boolean>
     AutomountServiceAccountToken indicates whether pods running as this service
     account should have an API token automatically mounted. Can be overridden
     at the pod level.

   imagePullSecrets	<[]Object>
     ImagePullSecrets is a list of references to secrets in the same namespace
to use for pulling any images in pods that reference this ServiceAccount.
     ImagePullSecrets are distinct from Secrets because Secrets can be mounted
     in the pod, but ImagePullSecrets are only accessed by the kubelet. More
     info:
     https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
 secrets	<[]Object>
     Secrets is the list of secrets allowed to be used by pods running using
     this ServiceAccount. More info:
     https://kubernetes.io/docs/concepts/configuration/secret

```



#只输出不创建

```bash
root@s3:/data/pv# kubectl create serviceaccount mysa -o yaml --dry-run
W0619 23:22:04.404462   12490 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: mysa

```

### 2.9.1: service account的创建

```bash
root@s3:/data# kubectl create serviceaccount mysa -n n60 -o yaml --dry-run
W0623 17:50:03.161813   99349 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: mysa
  namespace: n60
  
  root@s3:/data# kubectl create serviceaccount mysa -n n60 -o yaml --dry-run > serviceaccount.yam #指定导出为yaml文件 节省敲键盘
  
root@s3:/data# kubectl get sa/mysa -n n60 -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: "2022-06-23T09:45:02Z"
  name: mysa
  namespace: n60
  resourceVersion: "423959"
  uid: 05600d26-b6a6-4e75-8544-d4a46194aee4
secrets:
- name: mysa-token-qrnkg


看到有一个 token 已经被自动创建，并被 service account 引用。设置非默认的 service account，只需要在 pod 的spec.serviceAccountName 字段中将name设置为您想要用的 service account 名字即可。在 pod 创建之初 service account 就必须已经存在，否则创建将被拒绝。需要注意的是不能更新已创建的 pod 的 service account。
```



### 2.9.2: serviceaccount的自定义使用

这里在default名称空间创建了一个sa为admin，可以看到已经自动生成了一个**Tokens：a**dmin-token-nbkcb**。**

```bash
root@s3:~ kubectl create serviceaccount admin
serviceaccount/admin created
root@s3:/data# cat admin.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-sa-demo
  namespace: n60
  labels:
    app: myapp
    tier: frontend
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
    ports:
    - name: http
      containerPort: 80
  serviceAccountName: admin
```



### 2.9.1： 帮助命令

```bash
root@s3:~# kubectl config --help 
.....
vailable Commands:
  current-context Display the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  delete-user     Delete the specified user from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  get-users       Display users defined in the kubeconfig
  rename-context  Rename a context from the kubeconfig file
  set             Set an individual value in a kubeconfig file
  set-cluster     Set a cluster entry in kubeconfig  #设置一个集群
  set-context     Set a context entry in kubeconfig #设置上下文
  set-credentials Set a user entry in kubeconfig #设置用户账户
  unset           Unset an individual value in a kubeconfig file
  use-context     Set the current-context in a kubeconfig file #设置谁是当前上下文
  view            Display merged kubeconfig settings or a specified kubeconfig
file
....
root@s3:~# kubectl config view
apiVersion: v1
clusters:#集群列表
- cluster:
    certificate-authority-data: DATA+OMITTED #认证证书方式
    server: https://127.0.0.1:6443
  name: cluster1
contexts:#上下文列表
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
current-context: context-cluster1 #当前上下文
kind: Config
preferences: {}
users:#用户列表
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```

### 2.9.2：自签名证书

```bash
root@s3:/etc/kubeasz/clusters/k8s-01/ssl# pwd
/etc/kubeasz/clusters/k8s-01/ssl
root@s3:/etc/kubeasz/clusters/k8s-01/ssl# openssl genrsa -out magedu.key
Generating RSA private key, 2048 bit long modulus (2 primes)
...................+++++
.......................+++++
e is 65537 (0x010001)
genra 生成RSA私钥
-out magedu.key 生成的私钥文件名
2048 私钥长度

#报错 
root@s3:/etc/kubeasz/clusters/k8s-01/ssl# openssl req -new -key magedu.key -out magedu.csr -subj "/CN=magedu"
Can't load /root/.rnd into RNG
139928160874944:error:2406F079:random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/root/.rnd
#解决
root@s3:~# pwd
/root
root@s3:~# openssl rand -writerand .rnd
root@s3:/etc/kubeasz/clusters/k8s-01/ssl# openssl req -new -key magedu.key -out magedu.csr -subj "/CN=magedu"
req 生成证书签名请求
-new 新生成
-key 私钥文件
-out 生成的CSR文件
-subj 生成CSR证书的参数
subj参数说明如下
/C= Country 国家 CN
/ST= State or Province 省 Guangdong
/L= Location or City 城市 Guangzhou
/O= Organization 组织或企业 xdevops
/OU= Organization Unit 部门 xdevops
/CN= Common Name 域名或IP gitlab.xdevops.cn

#签发证书
root@s3:/etc/kubernetes/ssl# openssl  x509 -req -in magedu.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out magedu.crt -days 3650
Signature ok
subject=CN = magedu
Getting CA Private Key
#验证证书内容
root@s3:/data/certs# openssl x509 -in magedu.crt -text -noout

```



### 2.9.3：设置用户

```bash
root@s3:/etc/kubernetes/ssl# kubectl config set-credentials magedu --client-certificate=/etc/kubernetes/ssl/magedu.crt --client-key=/etc/kubernetes/ssl/magedu.key --embed-certs=true
User "magedu" set.
root@s3:/data/certs# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
current-context: context-cluster1
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: magedu
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```

### 2.9.4: 设置上下文

```bash
root@s3:~/.kube# kubectl config set-context magedu@cluster1 --cluster=cluster1 --user=magedu
Context "magedu@cluster1" created.
root@s3:~/.kube# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
- context:
    cluster: cluster1
    user: magedu
  name: magedu@cluster1
current-context: context-cluster1
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: magedu
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED


```

### 2.9.5: 切换用户

```bash
root@s3:~/.kube# kubectl config use-context magedu@cluster1
Switched to context "magedu@cluster1".
root@s3:~/.kube# kubectl get po
error: You must be logged in to the server (Unauthorized)


#切换到管理员
root@s3:~/.kube# kubectl config use-context context-cluster1
Switched to context "context-cluster1".
root@s3:~/.kube# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
- context:
    cluster: cluster1
    user: magedu
  name: magedu@cluster1
current-context: context-cluster1
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: magedu
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```

### 2.9.6： 自定义集群config

```bash
root@s3:~# kubectl config set-cluster mycluster --kubeconfig=/tmp/test.conf --server="https://192.168.48.166:6443" --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true
Cluster "mycluster" set.

root@s3:~# kubectl config view --kubeconfig=/tmp/test.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.48.166:6443
  name: mycluster
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null
```



## 2.9kubernetes资源限制



### 2.9.1 kubernetes中资源限制概括

![image-20220327165040637](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327165040637.png)



CPU 以核心为单位。

memory 以字节为单位。

requests 为kubernetes scheduler执行pod调度时node节点至少需要拥有的资源。

limits 为pod运行成功后最多可以使用的资源上限。

requests: 需求最低保障；

limits: 限制，硬限制

Qos:

​		Guranteed: 每个容器    优先级最高

​					同时设置CPU和内存的requests和limits

​                      cpu.limits=cpu.requests

​                      memory.limits=memory.requests 

​         Burstable:  中等优先级

​             至少有一个容器设置CPU或内存资源的requests属性

​          BestEffort:

​               没有任何一个容器设置了requests或limits属性 ； 最低优先级



当服务器资源不足，BestEffort 被优先终止

#### ![](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220327171058465.png)

#### 2.9.1.1 实现对单个容器cpu及memory资源限制

https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-memory-resource/



```
root@k8s-master1:/apps/pod# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange
  namespace: n60
spec:
  limits:
  - type: Container  #限制的资源类型
    max:
      cpu: "500m"     #限制单个容器的最大CPU
      memory: "500Mi"  #限制单个容器的最大内存
    min:
      cpu: "250m"       #限制单个容器最小CPU
      memory: "250Mi"    #限制单个容器的最小内存
    default:
      cpu: "500m"       #默认单个容器的CPU限制
      memory: "500Mi"   #默认单个容器的内存限制
    defaultRequest:      
      cpu: "500m"      #默认单个容器的CPU创建请求
      memory: "250Mi"   #默认单个容器的内存创建请求
    maxLimitRequestRatio:
      cpu: 1            #限制CPU limit/request比值最大为2
      memory: 1  #限制内存limit/request比值最大为1.5
  - type: Pod
    max:
      cpu: "1"     #限制单个pod的最大CPU
      memory: "1Gi"  #限制单个POD的最大内存
  - type: PersistentVolumeClaim
    max:
      storage: 20Gi #限制PVC最大的requests.storage
    min: 
      storage: 10Gi #限制PVC最小的requests.storage
      
      root@k8s-master1:/apps/pod# kubectl describe limitrange limitrange -n n60
Name:                  limitrange
Namespace:             n60
Type                   Resource  Min    Max    Default Request  Default Limit  Max Limit/Request Ratio
----                   --------  ---    ---    ---------------  -------------  -----------------------
Container              cpu       250m   500m   500m             500m           1
Container              memory    250Mi  500Mi  250Mi            500Mi          1
Pod                    cpu       -      1      -                -              -
Pod                    memory    -      1Gi    -                -              -
PersistentVolumeClaim  storage   10Gi   20Gi   -                -              -

```

#### 2.9.1.2 kubernetes对整个namespace资源限制

https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/

```bash
root@k8s-master1:/apps/pod# cat case6-ResourceQuota-magedu.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-magedu
  namespace: magedu
spec:
  hard:
    requests.cpu: "8"
    limits.cpu: "8"
    requests.memory: 4Gi
    limits.memory: 4Gi
    requests.nvidia.com/gpu: 4
    pods: "2"
    services: "6"


limits.cpu	所有非终止状态的 Pod，其 CPU 限额总量不能超过该值。
limits.memory	所有非终止状态的 Pod，其内存限额总量不能超过该值。
requests.cpu	所有非终止状态的 Pod，其 CPU 需求总量不能超过该值。
requests.memory	所有非终止状态的 Pod，其内存需求总量不能超过该值。
hugepages-<size>	对于所有非终止状态的 Pod，针对指定尺寸的巨页请求总数不能超过此值。
cpu	与 requests.cpu 相同。
memory	与 requests.memory 相同
```

## 3.0 kubernetes RBAC鉴权

https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/ #使用RBAC鉴权

https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/ #鉴权概述

基于角色（Role）的访问控制（RBAC）是一种基于组织中用户的角色来调节控制对 计算机或网络资源的访问的方法。

RBAC 鉴权机制使用 `rbac.authorization.k8s.io` [API 组](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) 来驱动鉴权决定，允许你通过 Kubernetes API 动态配置策略。

要启用 RBAC，在启动 [API 服务器](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/) 时将 `--authorization-mode` 参数设置为一个逗号分隔的列表并确保其中包含 `RBAC`。

### Role 和 ClusterRole 

RBAC 的 *Role* 或 *ClusterRole* 中包含一组代表相关权限的规则。 这些权限是纯粹累加的（不存在拒绝某操作的规则）。

Role 总是用来在某个[名字空间](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/namespaces/) 内设置访问权限；在你创建 Role 时，你必须指定该 Role 所属的名字空间。

与之相对，ClusterRole 则是一个集群作用域的资源。这两种资源的名字不同（Role 和 ClusterRole）是因为 Kubernetes 对象要么是名字空间作用域的，要么是集群作用域的， 不可两者兼具。

ClusterRole 有若干用法。你可以用它来：

1. 定义对某名字空间域对象的访问权限，并将在各个名字空间内完成授权；
2. 为名字空间作用域的对象设置访问权限，并跨所有名字空间执行授权；
3. 为集群作用域的资源定义访问权限。

### 3.0.1 查看创建role帮助

```bash
root@s3:~# kubectl create role --help
.....
Usage:
  kubectl create role NAME --verb=verb --resource=resource.group/subresource
[--resource-name=resourcename] [--dry-run=server|client|none] [options]

```

#### 3.0.1.1：创建role角色

```bash
root@s3:~# kubectl create role pods-reader --verb=get,list,watch --resource=pods --dry-run -o yaml
W0621 22:27:31.781251   21650 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pods-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
root@s3:~# kubectl create role pods-reader --verb=get,list,watch --resource=pods --dry-run -o yaml > role-demon.yaml
root@s3:~# cat role-demon.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pods-reader
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
root@s3:~# kubectl apply -f role-demon.yaml 
role.rbac.authorization.k8s.io/pods-reader created
root@s3:~/manifests# kubectl get role
NAME          CREATED AT
pods-reader   2022-06-21T14:31:47Z
root@s3:~/manifests# kubectl describe role pods-reader
Name:         pods-reader
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [get list watch]
```

#### 3.0.1.2: 创建rolebinding

```bash
root@s3:~/manifests# kubectl crate rolebinding --help
.....
Usage:
  kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME
[--user=username] [--group=groupname]
[--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
[options]

root@s3:~/manifests# kubectl create rolebinding magedu-read-pods --role=pods-reader --user=magedu --dry-run -o yaml
W0621 22:37:39.626789   29692 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: magedu-read-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu
  
  
  root@s3:~/manifests# kubectl create rolebinding magedu-read-pods --role=pods-reader --user=magedu --dry-run -o yaml > rolebinding.yaml

root@s3:~/manifests# cat rolebinding.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: magedu-read-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu
  
  root@s3:~/manifests# kubectl apply -f rolebinding.yaml 
rolebinding.rbac.authorization.k8s.io/magedu-read-pods created
root@s3:~/manifests# kubectl describe rolebinding magedu-read-pods
Name:         magedu-read-pods
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  pods-reader
Subjects:
  Kind  Name    Namespace
  ----  ----    ---------
  User  magedu 
```

#### 3.0.1.3：切换用户验证

```bash
root@s3:~# kubectl config use-context magedu@cluster1
Switched to context "magedu@cluster1".
root@s3:~# kubectl get po
NAME     READY   STATUS    RESTARTS   AGE
pod-sa   1/1     Running   0          88s


root@s3:~# kubectl get po -n kube-system
Error from server (Forbidden): pods is forbidden: User "magedu" cannot list resource "pods" in API group "" in the namespace "kube-system"

1.验证只能在当前名称空间使用
```

#### 3.0.1.4：创建clusterrole

```bash
root@s3:~# kubectl create clusterrole --help
....
Usage:
  kubectl create clusterrole NAME --verb=verb --resource=resource.group
[--resource-name=resourcename] [--dry-run=server|client|none] [options]
...

root@s3:~# kubectl create clusterrole cluster-reader --verb=get,list,watch --resource=pods --dry-run -o yaml
W0621 23:48:08.774412   85950 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: cluster-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
root@s3:~# kubectl create clusterrole cluster-reader --verb=get,list,watch --resource=pods --dry-run -o yaml > clusterrole.yaml

root@s3:~# cat clusterrole.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
root@s3:~# kubectl apply -f clusterrole.yaml 
clusterrole.rbac.authorization.k8s.io/cluster-reader created

```

#### 3.0.1.5 创建普通用户测试

```bash
root@s3:~# useradd -m k8s -s /bin/bash
root@s3:~# cp -rp .kube/ /home/k8s/
root@s3:~# chown -R k8s.k8s /home/k8s/
root@s3:~# su - k8s
k8s@s3:~$ kubectl config use-context magedu@cluster1
Switched to context "magedu@cluster1".

k8s@s3:~$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
- context:
    cluster: cluster1
    user: magedu
  name: magedu@cluster1
current-context: magedu@cluster1
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: magedu
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

k8s@s3:~$ kubectl get po
NAME     READY   STATUS    RESTARTS   AGE
pod-sa   1/1     Running   0          15m

```

#### 3.0.1.6 删除rolebinding 绑定clusterrolebinding

```bash
root@s3:~# kubectl get rolebinding
NAME               ROLE               AGE
magedu-read-pods   Role/pods-reader   50m
root@s3:~# kubectl delete rolebinding magedu-read-pods
rolebinding.rbac.authorization.k8s.io "magedu-read-pods" deleted


root@s3:~# kubectl create clusterrolebinding --help
Usage:
  kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username]
[--group=groupname] [--serviceaccount=namespace:serviceaccountname]
[--dry-run=server|client|none] [options]

root@s3:~# kubectl create clusterrolebinding magedu-read-all-pods --clusterrole=cluster-reader --user=magedu --dry-run -o yaml
W0622 00:00:10.336944   95575 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: magedu-read-all-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu

root@s3:~# kubectl create clusterrolebinding magedu-read-all-pods --clusterrole=cluster-reader --user=magedu --dry-run -o yaml > clusterrolebinding.yaml

root@s3:~# cat clusterrolebinding.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: magedu-read-all-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu

root@s3:~# kubectl apply -f clusterrolebinding.yaml 
clusterrolebinding.rbac.authorization.k8s.io/magedu-read-all-pods created

root@s3:~# kubectl describe clusterrolebinding magedu-read-all-pods
Name:         magedu-read-all-pods
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  cluster-reader
Subjects:
  Kind  Name    Namespace
  ----  ----    ---------
  User  magedu 
```

#### 3.0.1.7: 普通用户测试

```bash
k8s@s3:~$ kubectl get po
NAME     READY   STATUS    RESTARTS   AGE
pod-sa   1/1     Running   0          24m
k8s@s3:~$ kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS        AGE
calico-kube-controllers-647f956d86-wm8q6   1/1     Running   10 (120m ago)   19d
calico-node-hq7wz                          1/1     Running   14 (120m ago)   19d
calico-node-p7jtd                          1/1     Running   18 (118m ago)   19d
calico-node-tlvtj                          1/1     Running   16 (118m ago)   19d
calico-node-vvpwt                          1/1     Running   10 (118m ago)   19d
coredns-5785f8c645-89cl5                   1/1     Running   10 (120m ago)   19d

```

#### 3.0.1.8：rolebinding绑定clusterrole

```bash
root@s3:~# kubectl delete clusterrolebinding magedu-read-all-pods
clusterrolebinding.rbac.authorization.k8s.io "magedu-read-all-pods" deleted

root@s3:~# kubectl create rolebinding magedu-read-pods --clusterrole=cluster-reader --user=magedu --dry-run -o yaml
W0622 21:28:17.623235   14070 helpers.go:555] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: magedu-read-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu
root@s3:~# kubectl create rolebinding magedu-read-pods --clusterrole=cluster-reader --user=magedu --dry-run -o yaml > rolebinding-clusterrole-demo.yaml
root@s3:~# cat rolebinding-clusterrole-demo.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: magedu-read-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: magedu

root@s3:~# kubectl apply -f rolebinding-clusterrole-demo.yaml 
rolebinding.rbac.authorization.k8s.io/magedu-read-pods created

root@s3:~# kubectl describe rolebinding magedu-read-pods
Name:         magedu-read-pods
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  cluster-reader
Subjects:
  Kind  Name    Namespace
  ----  ----    ---------
  User  magedu  

```

#### 3.0.1.9: 访问测试

```bash
#使用普通用户
root@s3:~# su - k8s
k8s@s3:~$ ls
k8s@s3:~$ kubectl get po
NAME     READY   STATUS    RESTARTS   AGE
pod-sa   1/1     Running   1          21h
k8s@s3:~$ kubectl get po -n kube-system
Error from server (Forbidden): pods is forbidden: User "magedu" cannot list resource "pods" in API group "" in the namespace "kube-system"

1.rolebinding只对当前的名称空间有效
```



### 在指定namespace创建账号：

```
root@k8s-master1:/apps/RBAC# kubectl  create serviceaccount magedu -n n60
serviceaccount/magedu created
```

#### 3.0.1.1 创建role规则

```
root@k8s-master1:/apps/RBAC# cat role.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: n60
  name: magedu-role
rules:
  - apiGroups: ["*"]
    resources: ["pods","pods/exec"]
    verbs: ["*"]
    #RO-Role
    #verbs: ["get","watch","list"]
  - apiGroups: ["extensions","apps/v1"]
    resources: ["deployments"]
    verbs: ["get","list","watch","create","update","path","delete"]
    
root@k8s-master1:/apps/RBAC# kubectl apply -f role.yaml 
role.rbac.authorization.k8s.io/magedu-role created

root@k8s-master1:/apps/RBAC# kubectl get role -n n60
NAME          CREATED AT
magedu-role   2022-03-27T14:28:56Z

root@k8s-master1:/apps/RBAC# kubectl describe role magedu-role -n n60
Name:         magedu-role
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  pods.*/exec             []                 []              [*]
  pods.*                  []                 []              [*]
  deployments.apps/v1     []                 []              [get list watch create update path delete]
  deployments.extensions  []                 []              [get list watch create update path delet

```

#### 3.0.1.2 将规则于账号进行绑定

```
root@k8s-master1:/apps/RBAC# cat role-bind.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-binding
  namespace: n60
subjects:
- kind: ServiceAccount
  name: magedu
  namespace: n60
roleRef:
  kind: Role
  name: magedu-role
  apiGroup: rbac.authorization.k8s.io

root@k8s-master1:/apps/RBAC# kubectl apply -f role-bind.yaml 
rolebinding.rbac.authorization.k8s.io/role-binding created

root@k8s-master1:/apps/RBAC# kubectl get rolebinding -n n60
NAME           ROLE               AGE
role-binding   Role/magedu-role   50s
root@k8s-master1:/apps/RBAC# kubectl describe rolebinding role-binding -n n60
Name:         role-binding
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  magedu-role
Subjects:
  Kind            Name    Namespace
  ----            ----    ---------
  ServiceAccount  magedu  n60

```

#### 3.0.1.3 获取token名称

```
root@k8s-master1:/apps/RBAC# kubectl get secrets -n n60
NAME                  TYPE                                  DATA   AGE
default-token-cjcwb   kubernetes.io/service-account-token   3      8h
lcy-tls-secret        Opaque                                2      6h42m
magedu-token-pxtc2    kubernetes.io/service-account-token   3      14m
tls-secret            Opaque 

root@k8s-master1:/apps/RBAC# kubectl describe secrets magedu-token-pxtc2 -n n60
```

#### 3.0.1.4 使用base加密

```
eyJhbGciOiJSUzI1NiIsImtpZCI6InJNSVBzV3lvb0hodHRJemN4UGgxX0U2VThYTzJLZ3puLXZxdDg1ZWIxRkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJuNjAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWFnZWR1LXRva2VuLXB4dGMyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1hZ2VkdSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU1OTk4MGFlLTIzMzEtNGQ4MC1iMGM4LThlOTE4ZDQzNDYxNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpuNjA6bWFnZWR1In0.s-AXvrzhFTKhu9cqIsuaiQsADBdEpPItd4qmamjU8_36TqWjcQgPDqTv79Yk-1aOH6tzDzDnjYJB8R0dWwSc4auPbNqwbQSvsn6JydSpKn3hNPHRujbh1ZXfSYvj0CHXtnf1Y-F1mdlN7SWY-YnV1gjKosKLWZR5K8FCSY-39QOwJFNhhHeVN7zHzf82ZZw4kyi642CYmxW-Z5IPwmU7xFQsRnnR0nXsTlSI1FUKwXwA_g5BHmtxtAnzanegCG9MKOvfSe2l59rKH9GOWvcy7E-qJGEwD8mcBzteh8JeM0y7B2tX6NLRRPUec-iV9FX5LMYtMn5E1f8aoViZMZNpyQ
```

#### 3.0.1.5 基于kube-config文件登录

```
root@k8s-master1:/apps/RBAC# cat magedu-csr.json 
{
  "CN": "China",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

```

#### 3.0.1.6 签发证书

```
root@k8s-master1:/apps/RBAC# ln -sv /etc/kubeasz/bin/cfssl* /usr/bin/


root@k8s-master1:/apps/RBAC# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubeasz/clusters/k8s-cluster1/ssl/ca-config.json -profile=kubernetes magedu-csr.json | cfssljson -bare magedu

root@s3:/apps/rbac# ls magedu*
magedu.csr  magedu-csr.json  magedu-key.pem  magedu.pem


```

#### 3.0.1.7 生成用户kubeconfig文件

```bash
root@s3:/apps/rbac# kubectl config set-cluster cluster1 --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=https://192.168.48.166:6443 --kubeconfig=magedu.kubeconfig
Cluster "cluster1" set.

```







####  2.9.1.8 设置客户端认证参数

```bash
root@s3:/apps/rbac# kubectl config set-credentials magedu --client-certificate=/etc/kubernetes/ssl/magedu.pem --client-key=/etc/kubernetes/ssl/magedu-key.pem --embed-certs=true --kubeconfig=magedu.kubeconfig
User "magedu" set.


root@s3:/apps/rbac# ll magedu*
-rw-r--r-- 1 root root  993 2月   1 11:30 magedu.csr
-rw-r--r-- 1 root root  218 2月   1 11:27 magedu-csr.json
-rw------- 1 root root 1679 2月   1 11:30 magedu-key.pem
-rw------- 1 root root 4260 2月   1 11:32 magedu.kubeconfig
-rw-r--r-- 1 root root 1383 2月   1 11:30 magedu.pem


root@s3:/apps/rbac# cat magedu.kubeconfig 
apiVersion: v1
clusters: null
server: https://192.168.48.166:6443
contexts: null
current-context: ""
kind: Config
preferences: {}
users:
- name: magedu
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwRENDQXJpZ0F3SUJBZ0lVVTA0OVFadGpKQXFKRnBCUVhReXNldVJQOWtZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qTXdNakF4TURNeU5UQXdXaGdQTWpBM016QXhNVGt3TXpJMU1EQmFNR0F4Q3pBSkJnTlYKQkFZVEFrTk9NUkF3RGdZRFZRUUlFd2RDWldsS2FXNW5NUkF3RGdZRFZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRApWUVFLRXdOck9ITXhEekFOQmdOVkJBc1RCbE41YzNSbGJURU9NQXdHQTFVRUF4TUZRMmhwYm1Fd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEVHBlbkxCczIwYU9XdXRIOEluZ05QZ0hTVUxnb2cKL2tYSkZrRnZOQjROM3FOcGNUT0I0ZVF2QldGNXZwYkRRdVlhejBaaUg5cERUOUtwK0lUVE1zV29nOHUvZlJ0Zwo1cUkvbmdNajU5b05ia3NPN2I4bWFFMXIxQnIwUVJSeEdDTklwSFRUVUNWaHpXMlJlTmlUOVlxOVZvUUFtbDRZCkRLVUtTcS9KalVLMnpCMG1wVUdGeWhlZVFXdkpnM1FOb1hRcWcyZEFzQ2xjZ1VDZll5NmpWdEU0a1pZY1hoVzIKT1UvSEJIOUhuNUlJUEd0a1BhTmVJM1ZHcVVybGhlU2tNdkR1Qlp5dEp2bnNHV2lhK2Z1UFZYZXU3OU8xRjBZUApvd1RPRFczY09zelIwcU5wVEppbUQwa3dUcUlVcGhhZ1VMY1l3Wlh1eU1ueHBpYUhmcXdDcnkyUkFnTUJBQUdqCmZ6QjlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFkQmdOVkhRNEVGZ1FVSVpNYllvQkVNNDJyTDM3NXFhYnM3b0M2bDVndwpId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6ajNHZjdvSWNJczUwd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBRjlQRmtNbHRSdWs1MnNiYzJaRFd5ME5CMy8rajFaMXI5NkZJV2Q4WW9EK3prenlrUE41TEVPbFJJM3kKWXErQVRJaFNBVTFzR05idGw4RysvZWhzWnp3Wjd0aVdlVWxISXNUV3cvSXVPZys3Z3ZyWXZQTUFMdXZRbm4xYwo1OEhTV1V6dHdUNCtyVDZoUnI1TDdqbFU4Q0ZQeThtVnZDMG1wd1BRUzl3dWlTeTVsN0NoQmxrdGlrUGVpbTQxCmtaakczZmZqZUFPYndMZnQ1RFZiRklCY3NiTjc5SHE4VE9tb0FjZ2I5dVZIa2cyTFpoSElJa1d4ZVdveUhkcW8KdjQ2b3ZwaklKc2M3bCtuWjN0anQrMlpjaU8wcTdhR0ZOdkhRUFRQZTBKVnhIL2UzTXlkVnh0Z0ErUHNHNGs3OQpCOElFeENGcjB4RHR3U0VXZDhEZEhkUnJBMm89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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


```

#### 2.9.1.9 设置上下文参数

https://kubernetes.io/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/



```bash
root@s3:/apps/rbac# kubectl config set-context cluster1 --cluster=cluster1 --user=magedu --namespace=magedu --kubeconfig=magedu.kubeconfig
Context "cluster1" created.
root@s3:/apps/rbac# cat magedu.kubeconfig 
apiVersion: v1
clusters: null
server: https://192.168.48.166:6443
contexts:
- context:
    cluster: cluster1
    namespace: magedu
    user: magedu
  name: cluster1
current-context: ""
kind: Config
preferences: {}
users:
- name: magedu
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwRENDQXJpZ0F3SUJBZ0lVVTA0OVFadGpKQXFKRnBCUVhReXNldVJQOWtZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qTXdNakF4TURNeU5UQXdXaGdQTWpBM016QXhNVGt3TXpJMU1EQmFNR0F4Q3pBSkJnTlYKQkFZVEFrTk9NUkF3RGdZRFZRUUlFd2RDWldsS2FXNW5NUkF3RGdZRFZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRApWUVFLRXdOck9ITXhEekFOQmdOVkJBc1RCbE41YzNSbGJURU9NQXdHQTFVRUF4TUZRMmhwYm1Fd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEVHBlbkxCczIwYU9XdXRIOEluZ05QZ0hTVUxnb2cKL2tYSkZrRnZOQjROM3FOcGNUT0I0ZVF2QldGNXZwYkRRdVlhejBaaUg5cERUOUtwK0lUVE1zV29nOHUvZlJ0Zwo1cUkvbmdNajU5b05ia3NPN2I4bWFFMXIxQnIwUVJSeEdDTklwSFRUVUNWaHpXMlJlTmlUOVlxOVZvUUFtbDRZCkRLVUtTcS9KalVLMnpCMG1wVUdGeWhlZVFXdkpnM1FOb1hRcWcyZEFzQ2xjZ1VDZll5NmpWdEU0a1pZY1hoVzIKT1UvSEJIOUhuNUlJUEd0a1BhTmVJM1ZHcVVybGhlU2tNdkR1Qlp5dEp2bnNHV2lhK2Z1UFZYZXU3OU8xRjBZUApvd1RPRFczY09zelIwcU5wVEppbUQwa3dUcUlVcGhhZ1VMY1l3Wlh1eU1ueHBpYUhmcXdDcnkyUkFnTUJBQUdqCmZ6QjlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFkQmdOVkhRNEVGZ1FVSVpNYllvQkVNNDJyTDM3NXFhYnM3b0M2bDVndwpId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6ajNHZjdvSWNJczUwd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBRjlQRmtNbHRSdWs1MnNiYzJaRFd5ME5CMy8rajFaMXI5NkZJV2Q4WW9EK3prenlrUE41TEVPbFJJM3kKWXErQVRJaFNBVTFzR05idGw4RysvZWhzWnp3Wjd0aVdlVWxISXNUV3cvSXVPZys3Z3ZyWXZQTUFMdXZRbm4xYwo1OEhTV1V6dHdUNCtyVDZoUnI1TDdqbFU4Q0ZQeThtVnZDMG1wd1BRUzl3dWlTeTVsN0NoQmxrdGlrUGVpbTQxCmtaakczZmZqZUFPYndMZnQ1RFZiRklCY3NiTjc5SHE4VE9tb0FjZ2I5dVZIa2cyTFpoSElJa1d4ZVdveUhkcW8KdjQ2b3ZwaklKc2M3bCtuWjN0anQrMlpjaU8wcTdhR0ZOdkhRUFRQZTBKVnhIL2UzTXlkVnh0Z0ErUHNHNGs3OQpCOElFeENGcjB4RHR3U0VXZDhEZEhkUnJBMm89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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

```

#### 2.9.2.1 设置默认上下文

```bash
root@s3:/apps/rbac# kubectl config use-context cluster1 --kubeconfig=magedu.kubeconfig
Switched to context "cluster1".
root@s3:/apps/rbac# cat magedu.kubeconfig 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR1RENDQXFDZ0F3SUJBZ0lVWEQ1M3V3eitWeU9XWlFlcC9PYmg3KzZFOElVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qSXdOakF5TVRJek5qQXdXaGdQTWpFeU1qQTFNRGt4TWpNMk1EQmFNR0V4Q3pBSkJnTlYKQkFZVEFrTk9NUkV3RHdZRFZRUUlFd2hJWVc1bldtaHZkVEVMTUFrR0ExVUVCeE1DV0ZNeEREQUtCZ05WQkFvVApBMnM0Y3pFUE1BMEdBMVVFQ3hNR1UzbHpkR1Z0TVJNd0VRWURWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXFybG03eUhpVDFrODFLcUJSWmZjTGJnSWRiaC8KcDd6NkFWUXVZMDFQRVpRbkJ1ZHhtRzNJVWVSRExpRi95aXgzL2RXYnNTRStzRW5ZYkZWV3phMFhKd1BFNFM4agpIT1RzMWNWd1pLT2puUk8yeTQ0elpJOGdlZDZwYjcxWXRab2lZUWJQenZSRzNoVU9QcG5uWFdDZnA4bTNwaWhRCnFXRUJ2LzhaWHBMTGNER3A1Y0JOWUhveXQ4RENKSk5UWkREbWI4ZFpFNmRsa3hPdE0zUk1MWDJkYTQxeXZ5dEgKcXR1T2lLMzVUUytlY2hIVzhwblovMytKQzFnd0QxQ2Q3dHh6R1VTb1ppOXV3MXEvU1UwWWM0RWh6YThoUzh5aAp6aXVvRXZqbDBic250cjFSa0FJakhhM1IrYmczRUlvSjdsVDREYUlrZ0RJamx2S3dNRnpQL2RmUHlRSURBUUFCCm8yWXdaREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdFZ1lEVlIwVEFRSC9CQWd3QmdFQi93SUJBakFkQmdOVkhRNEUKRmdRVUJlWHQ2VlV1aDV2Mm9xemozR2Y3b0ljSXM1MHdId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6agozR2Y3b0ljSXM1MHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRFRrUEdsdGtnbE5IUTdVVU9CRzFXV0Y0dGVYCjZmUjhEQmdaOUp0Y0RsaGFCSEd0YUpPOHE5U3JDNXRJc2VoN2RiWTVFOFJ1VU5ETzh2eGhENytSRUFyTXR4ZlEKNEJyaHRSaU5LZGcxK0RkQmNiNVI0T2sxTmtSZEJqdjhwZ0RLSHpuSEs0QWtkRWNBd3VkMUoycU9IQXJiNTZjMgphakJjVS9MQTNUTFRXcnpwcFZib2JHRzY4aEp2c1hTSzFTZFNncUZPc2pNY2E4bVNDYjBDcE5OU0JtK2s2MWZiCjlIbzZCbVY3a1loNXZCY2hFa2tKcG9nQ2VGV0Y5Qmt0cFJveThXRlZKTzdwdUEvNm94N2JyKzV2a0U2VzJpZ2EKS0hKN0JJN04wenY5N0toMkdZNVpibVBORElpaEMxWm45OGJGajQ4YVZUSzNJMTNtOUMzTmtJcEhoczQ9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://192.168.48.166:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    namespace: magedu
    user: magedu
  name: cluster1
current-context: cluster1
kind: Config
preferences: {}
users:
- name: magedu
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwRENDQXJpZ0F3SUJBZ0lVVTA0OVFadGpKQXFKRnBCUVhReXNldVJQOWtZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qTXdNakF4TURNeU5UQXdXaGdQTWpBM016QXhNVGt3TXpJMU1EQmFNR0F4Q3pBSkJnTlYKQkFZVEFrTk9NUkF3RGdZRFZRUUlFd2RDWldsS2FXNW5NUkF3RGdZRFZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRApWUVFLRXdOck9ITXhEekFOQmdOVkJBc1RCbE41YzNSbGJURU9NQXdHQTFVRUF4TUZRMmhwYm1Fd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEVHBlbkxCczIwYU9XdXRIOEluZ05QZ0hTVUxnb2cKL2tYSkZrRnZOQjROM3FOcGNUT0I0ZVF2QldGNXZwYkRRdVlhejBaaUg5cERUOUtwK0lUVE1zV29nOHUvZlJ0Zwo1cUkvbmdNajU5b05ia3NPN2I4bWFFMXIxQnIwUVJSeEdDTklwSFRUVUNWaHpXMlJlTmlUOVlxOVZvUUFtbDRZCkRLVUtTcS9KalVLMnpCMG1wVUdGeWhlZVFXdkpnM1FOb1hRcWcyZEFzQ2xjZ1VDZll5NmpWdEU0a1pZY1hoVzIKT1UvSEJIOUhuNUlJUEd0a1BhTmVJM1ZHcVVybGhlU2tNdkR1Qlp5dEp2bnNHV2lhK2Z1UFZYZXU3OU8xRjBZUApvd1RPRFczY09zelIwcU5wVEppbUQwa3dUcUlVcGhhZ1VMY1l3Wlh1eU1ueHBpYUhmcXdDcnkyUkFnTUJBQUdqCmZ6QjlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFkQmdOVkhRNEVGZ1FVSVpNYllvQkVNNDJyTDM3NXFhYnM3b0M2bDVndwpId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6ajNHZjdvSWNJczUwd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBRjlQRmtNbHRSdWs1MnNiYzJaRFd5ME5CMy8rajFaMXI5NkZJV2Q4WW9EK3prenlrUE41TEVPbFJJM3kKWXErQVRJaFNBVTFzR05idGw4RysvZWhzWnp3Wjd0aVdlVWxISXNUV3cvSXVPZys3Z3ZyWXZQTUFMdXZRbm4xYwo1OEhTV1V6dHdUNCtyVDZoUnI1TDdqbFU4Q0ZQeThtVnZDMG1wd1BRUzl3dWlTeTVsN0NoQmxrdGlrUGVpbTQxCmtaakczZmZqZUFPYndMZnQ1RFZiRklCY3NiTjc5SHE4VE9tb0FjZ2I5dVZIa2cyTFpoSElJa1d4ZVdveUhkcW8KdjQ2b3ZwaklKc2M3bCtuWjN0anQrMlpjaU8wcTdhR0ZOdkhRUFRQZTBKVnhIL2UzTXlkVnh0Z0ErUHNHNGs3OQpCOElFeENGcjB4RHR3U0VXZDhEZEhkUnJBMm89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
root@s3:/apps/rbac# 
root@s3:/apps/rbac# ls
magedu.csr  magedu-csr.json  magedu-key.pem  magedu.kubeconfig  magedu.pem
root@s3:/apps/rbac# cat magedu.kubeconfig 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR1RENDQXFDZ0F3SUJBZ0lVWEQ1M3V3eitWeU9XWlFlcC9PYmg3KzZFOElVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qSXdOakF5TVRJek5qQXdXaGdQTWpFeU1qQTFNRGt4TWpNMk1EQmFNR0V4Q3pBSkJnTlYKQkFZVEFrTk9NUkV3RHdZRFZRUUlFd2hJWVc1bldtaHZkVEVMTUFrR0ExVUVCeE1DV0ZNeEREQUtCZ05WQkFvVApBMnM0Y3pFUE1BMEdBMVVFQ3hNR1UzbHpkR1Z0TVJNd0VRWURWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXFybG03eUhpVDFrODFLcUJSWmZjTGJnSWRiaC8KcDd6NkFWUXVZMDFQRVpRbkJ1ZHhtRzNJVWVSRExpRi95aXgzL2RXYnNTRStzRW5ZYkZWV3phMFhKd1BFNFM4agpIT1RzMWNWd1pLT2puUk8yeTQ0elpJOGdlZDZwYjcxWXRab2lZUWJQenZSRzNoVU9QcG5uWFdDZnA4bTNwaWhRCnFXRUJ2LzhaWHBMTGNER3A1Y0JOWUhveXQ4RENKSk5UWkREbWI4ZFpFNmRsa3hPdE0zUk1MWDJkYTQxeXZ5dEgKcXR1T2lLMzVUUytlY2hIVzhwblovMytKQzFnd0QxQ2Q3dHh6R1VTb1ppOXV3MXEvU1UwWWM0RWh6YThoUzh5aAp6aXVvRXZqbDBic250cjFSa0FJakhhM1IrYmczRUlvSjdsVDREYUlrZ0RJamx2S3dNRnpQL2RmUHlRSURBUUFCCm8yWXdaREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdFZ1lEVlIwVEFRSC9CQWd3QmdFQi93SUJBakFkQmdOVkhRNEUKRmdRVUJlWHQ2VlV1aDV2Mm9xemozR2Y3b0ljSXM1MHdId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6agozR2Y3b0ljSXM1MHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRFRrUEdsdGtnbE5IUTdVVU9CRzFXV0Y0dGVYCjZmUjhEQmdaOUp0Y0RsaGFCSEd0YUpPOHE5U3JDNXRJc2VoN2RiWTVFOFJ1VU5ETzh2eGhENytSRUFyTXR4ZlEKNEJyaHRSaU5LZGcxK0RkQmNiNVI0T2sxTmtSZEJqdjhwZ0RLSHpuSEs0QWtkRWNBd3VkMUoycU9IQXJiNTZjMgphakJjVS9MQTNUTFRXcnpwcFZib2JHRzY4aEp2c1hTSzFTZFNncUZPc2pNY2E4bVNDYjBDcE5OU0JtK2s2MWZiCjlIbzZCbVY3a1loNXZCY2hFa2tKcG9nQ2VGV0Y5Qmt0cFJveThXRlZKTzdwdUEvNm94N2JyKzV2a0U2VzJpZ2EKS0hKN0JJN04wenY5N0toMkdZNVpibVBORElpaEMxWm45OGJGajQ4YVZUSzNJMTNtOUMzTmtJcEhoczQ9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://192.168.48.166:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    namespace: magedu
    user: magedu
  name: cluster1
current-context: cluster1
kind: Config
preferences: {}
users:
- name: magedu
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwRENDQXJpZ0F3SUJBZ0lVVTA0OVFadGpKQXFKRnBCUVhReXNldVJQOWtZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qTXdNakF4TURNeU5UQXdXaGdQTWpBM016QXhNVGt3TXpJMU1EQmFNR0F4Q3pBSkJnTlYKQkFZVEFrTk9NUkF3RGdZRFZRUUlFd2RDWldsS2FXNW5NUkF3RGdZRFZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRApWUVFLRXdOck9ITXhEekFOQmdOVkJBc1RCbE41YzNSbGJURU9NQXdHQTFVRUF4TUZRMmhwYm1Fd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEVHBlbkxCczIwYU9XdXRIOEluZ05QZ0hTVUxnb2cKL2tYSkZrRnZOQjROM3FOcGNUT0I0ZVF2QldGNXZwYkRRdVlhejBaaUg5cERUOUtwK0lUVE1zV29nOHUvZlJ0Zwo1cUkvbmdNajU5b05ia3NPN2I4bWFFMXIxQnIwUVJSeEdDTklwSFRUVUNWaHpXMlJlTmlUOVlxOVZvUUFtbDRZCkRLVUtTcS9KalVLMnpCMG1wVUdGeWhlZVFXdkpnM1FOb1hRcWcyZEFzQ2xjZ1VDZll5NmpWdEU0a1pZY1hoVzIKT1UvSEJIOUhuNUlJUEd0a1BhTmVJM1ZHcVVybGhlU2tNdkR1Qlp5dEp2bnNHV2lhK2Z1UFZYZXU3OU8xRjBZUApvd1RPRFczY09zelIwcU5wVEppbUQwa3dUcUlVcGhhZ1VMY1l3Wlh1eU1ueHBpYUhmcXdDcnkyUkFnTUJBQUdqCmZ6QjlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFkQmdOVkhRNEVGZ1FVSVpNYllvQkVNNDJyTDM3NXFhYnM3b0M2bDVndwpId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6ajNHZjdvSWNJczUwd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBRjlQRmtNbHRSdWs1MnNiYzJaRFd5ME5CMy8rajFaMXI5NkZJV2Q4WW9EK3prenlrUE41TEVPbFJJM3kKWXErQVRJaFNBVTFzR05idGw4RysvZWhzWnp3Wjd0aVdlVWxISXNUV3cvSXVPZys3Z3ZyWXZQTUFMdXZRbm4xYwo1OEhTV1V6dHdUNCtyVDZoUnI1TDdqbFU4Q0ZQeThtVnZDMG1wd1BRUzl3dWlTeTVsN0NoQmxrdGlrUGVpbTQxCmtaakczZmZqZUFPYndMZnQ1RFZiRklCY3NiTjc5SHE4VE9tb0FjZ2I5dVZIa2cyTFpoSElJa1d4ZVdveUhkcW8KdjQ2b3ZwaklKc2M3bCtuWjN0anQrMlpjaU8wcTdhR0ZOdkhRUFRQZTBKVnhIL2UzTXlkVnh0Z0ErUHNHNGs3OQpCOElFeENGcjB4RHR3U0VXZDhEZEhkUnJBMm89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
```

#### 2.9.2.2 获取token

```bash
root@s3:/apps/rbac# kubectl get secrets -n magedu | grep magedu
magedu-token-j9m2f    kubernetes.io/service-account-token   3      49m
root@s3:/apps/rbac# kubectl describe secrets magedu-token-j9m2f -n magedu
Name:         magedu-token-j9m2f
Namespace:    magedu
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: magedu
              kubernetes.io/service-account.uid: 828abe1a-1bc0-43e3-9ba9-69ce0768f577

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1350 bytes
namespace:  6 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InQ2U2Njd0ZXTk9OYlR1alM4S3R3X29nV1dYdFhMLWdPLVJWaVNxWXE1ZWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYWdlZHUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWFnZWR1LXRva2VuLWo5bTJmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1hZ2VkdSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjgyOGFiZTFhLTFiYzAtNDNlMy05YmE5LTY5Y2UwNzY4ZjU3NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYWdlZHU6bWFnZWR1In0.aySGfLFcXjlsIj01i4tDjAKeIS7QpjuiM3gNAS7jK9zR3xDPOmpBDbkm3bwRXX9xF_ADpHV_AS6WYn8IYyilPMG5BT_eYV4O8-T-shsmv-wXV-XKQkhHcx0i-SFtWOWKINlXBkhoSJa9UzCQDSs1eFtt8k15qruBoE1q1u8nOLS2CnJyp7pbOd_DAIAc2DUbkkqKEtslQZJvx_QUozjXSaMTGIOoYrWVl8gr1IR6iswE66huS4zLIM9YdLnUPFe2x1agaCMOXg5UrwUUeWvBTS1JHRRPxQBirEvT3lSHIyCFHVppL-UjsdrUmGO1HaygJW_mavwwR-YYnImedl4ijw

```

#### 2.9.3.2将token 写入用户config文件

```bash
root@s3:/apps/rbac# cat magedu.kubeconfig 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR1RENDQXFDZ0F3SUJBZ0lVWEQ1M3V3eitWeU9XWlFlcC9PYmg3KzZFOElVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qSXdOakF5TVRJek5qQXdXaGdQTWpFeU1qQTFNRGt4TWpNMk1EQmFNR0V4Q3pBSkJnTlYKQkFZVEFrTk9NUkV3RHdZRFZRUUlFd2hJWVc1bldtaHZkVEVMTUFrR0ExVUVCeE1DV0ZNeEREQUtCZ05WQkFvVApBMnM0Y3pFUE1BMEdBMVVFQ3hNR1UzbHpkR1Z0TVJNd0VRWURWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXFybG03eUhpVDFrODFLcUJSWmZjTGJnSWRiaC8KcDd6NkFWUXVZMDFQRVpRbkJ1ZHhtRzNJVWVSRExpRi95aXgzL2RXYnNTRStzRW5ZYkZWV3phMFhKd1BFNFM4agpIT1RzMWNWd1pLT2puUk8yeTQ0elpJOGdlZDZwYjcxWXRab2lZUWJQenZSRzNoVU9QcG5uWFdDZnA4bTNwaWhRCnFXRUJ2LzhaWHBMTGNER3A1Y0JOWUhveXQ4RENKSk5UWkREbWI4ZFpFNmRsa3hPdE0zUk1MWDJkYTQxeXZ5dEgKcXR1T2lLMzVUUytlY2hIVzhwblovMytKQzFnd0QxQ2Q3dHh6R1VTb1ppOXV3MXEvU1UwWWM0RWh6YThoUzh5aAp6aXVvRXZqbDBic250cjFSa0FJakhhM1IrYmczRUlvSjdsVDREYUlrZ0RJamx2S3dNRnpQL2RmUHlRSURBUUFCCm8yWXdaREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdFZ1lEVlIwVEFRSC9CQWd3QmdFQi93SUJBakFkQmdOVkhRNEUKRmdRVUJlWHQ2VlV1aDV2Mm9xemozR2Y3b0ljSXM1MHdId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6agozR2Y3b0ljSXM1MHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRFRrUEdsdGtnbE5IUTdVVU9CRzFXV0Y0dGVYCjZmUjhEQmdaOUp0Y0RsaGFCSEd0YUpPOHE5U3JDNXRJc2VoN2RiWTVFOFJ1VU5ETzh2eGhENytSRUFyTXR4ZlEKNEJyaHRSaU5LZGcxK0RkQmNiNVI0T2sxTmtSZEJqdjhwZ0RLSHpuSEs0QWtkRWNBd3VkMUoycU9IQXJiNTZjMgphakJjVS9MQTNUTFRXcnpwcFZib2JHRzY4aEp2c1hTSzFTZFNncUZPc2pNY2E4bVNDYjBDcE5OU0JtK2s2MWZiCjlIbzZCbVY3a1loNXZCY2hFa2tKcG9nQ2VGV0Y5Qmt0cFJveThXRlZKTzdwdUEvNm94N2JyKzV2a0U2VzJpZ2EKS0hKN0JJN04wenY5N0toMkdZNVpibVBORElpaEMxWm45OGJGajQ4YVZUSzNJMTNtOUMzTmtJcEhoczQ9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://192.168.48.166:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    namespace: magedu
    user: magedu
  name: cluster1
current-context: cluster1
kind: Config
preferences: {}
users:
- name: magedu
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwRENDQXJpZ0F3SUJBZ0lVVTA0OVFadGpKQXFKRnBCUVhReXNldVJQOWtZd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lURUxNQWtHQTFVRUJoTUNRMDR4RVRBUEJnTlZCQWdUQ0VoaGJtZGFhRzkxTVFzd0NRWURWUVFIRXdKWQpVekVNTUFvR0ExVUVDaE1EYXpoek1ROHdEUVlEVlFRTEV3WlRlWE4wWlcweEV6QVJCZ05WQkFNVENtdDFZbVZ5CmJtVjBaWE13SUJjTk1qTXdNakF4TURNeU5UQXdXaGdQTWpBM016QXhNVGt3TXpJMU1EQmFNR0F4Q3pBSkJnTlYKQkFZVEFrTk9NUkF3RGdZRFZRUUlFd2RDWldsS2FXNW5NUkF3RGdZRFZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRApWUVFLRXdOck9ITXhEekFOQmdOVkJBc1RCbE41YzNSbGJURU9NQXdHQTFVRUF4TUZRMmhwYm1Fd2dnRWlNQTBHCkNTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEVHBlbkxCczIwYU9XdXRIOEluZ05QZ0hTVUxnb2cKL2tYSkZrRnZOQjROM3FOcGNUT0I0ZVF2QldGNXZwYkRRdVlhejBaaUg5cERUOUtwK0lUVE1zV29nOHUvZlJ0Zwo1cUkvbmdNajU5b05ia3NPN2I4bWFFMXIxQnIwUVJSeEdDTklwSFRUVUNWaHpXMlJlTmlUOVlxOVZvUUFtbDRZCkRLVUtTcS9KalVLMnpCMG1wVUdGeWhlZVFXdkpnM1FOb1hRcWcyZEFzQ2xjZ1VDZll5NmpWdEU0a1pZY1hoVzIKT1UvSEJIOUhuNUlJUEd0a1BhTmVJM1ZHcVVybGhlU2tNdkR1Qlp5dEp2bnNHV2lhK2Z1UFZYZXU3OU8xRjBZUApvd1RPRFczY09zelIwcU5wVEppbUQwa3dUcUlVcGhhZ1VMY1l3Wlh1eU1ueHBpYUhmcXdDcnkyUkFnTUJBQUdqCmZ6QjlNQTRHQTFVZER3RUIvd1FFQXdJRm9EQWRCZ05WSFNVRUZqQVVCZ2dyQmdFRkJRY0RBUVlJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFkQmdOVkhRNEVGZ1FVSVpNYllvQkVNNDJyTDM3NXFhYnM3b0M2bDVndwpId1lEVlIwakJCZ3dGb0FVQmVYdDZWVXVoNXYyb3F6ajNHZjdvSWNJczUwd0RRWUpLb1pJaHZjTkFRRUxCUUFECmdnRUJBRjlQRmtNbHRSdWs1MnNiYzJaRFd5ME5CMy8rajFaMXI5NkZJV2Q4WW9EK3prenlrUE41TEVPbFJJM3kKWXErQVRJaFNBVTFzR05idGw4RysvZWhzWnp3Wjd0aVdlVWxISXNUV3cvSXVPZys3Z3ZyWXZQTUFMdXZRbm4xYwo1OEhTV1V6dHdUNCtyVDZoUnI1TDdqbFU4Q0ZQeThtVnZDMG1wd1BRUzl3dWlTeTVsN0NoQmxrdGlrUGVpbTQxCmtaakczZmZqZUFPYndMZnQ1RFZiRklCY3NiTjc5SHE4VE9tb0FjZ2I5dVZIa2cyTFpoSElJa1d4ZVdveUhkcW8KdjQ2b3ZwaklKc2M3bCtuWjN0anQrMlpjaU8wcTdhR0ZOdkhRUFRQZTBKVnhIL2UzTXlkVnh0Z0ErUHNHNGs3OQpCOElFeENGcjB4RHR3U0VXZDhEZEhkUnJBMm89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InQ2U2Njd0ZXTk9OYlR1alM4S3R3X29nV1dYdFhMLWdPLVJWaVNxWXE1ZWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYWdlZHUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWFnZWR1LXRva2VuLWo5bTJmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1hZ2VkdSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjgyOGFiZTFhLTFiYzAtNDNlMy05YmE5LTY5Y2UwNzY4ZjU3NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYWdlZHU6bWFnZWR1In0.aySGfLFcXjlsIj01i4tDjAKeIS7QpjuiM3gNAS7jK9zR3xDPOmpBDbkm3bwRXX9xF_ADpHV_AS6WYn8IYyilPMG5BT_eYV4O8-T-shsmv-wXV-XKQkhHcx0i-SFtWOWKINlXBkhoSJa9UzCQDSs1eFtt8k15qruBoE1q1u8nOLS2CnJyp7pbOd_DAIAc2DUbkkqKEtslQZJvx_QUozjXSaMTGIOoYrWVl8gr1IR6iswE66huS4zLIM9YdLnUPFe2x1agaCMOXg5UrwUUeWvBTS1JHRRPxQBirEvT3lSHIyCFHVppL-UjsdrUmGO1HaygJW_mavwwR-YYnImedl4ijw
```



## 3.1：Kubernetes之Pod资源调度

### 3.1.1：Pod资源调度





API Server在接受客户端提交Pod对象创建请求后，然后是通过调度器（kube-schedule）从集群中选择一个可用的最佳节点来创建并运行Pod。而这一个创建Pod对象，在调度的过程当中有3个阶段：节点预选、节点优选、节点选定，从而筛选出最佳的节点。如图：



![image-20220625183612291](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220625183612291.png)

- 节点预选：基于一系列的预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点的预选
- 节点优选：对预选出的节点进行优先级排序，以便选出最合适运行Pod对象的节点
- 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择

当我们有需求要将某些Pod资源运行在特定的节点上时，我们可以通过组合节点标签，以及Pod标签或标签选择器来匹配特定的预选策略并完成调度，如MatchInterPodAfinity、MatchNodeSelector、PodToleratesNodeTaints等预选策略，这些策略常用于为用户提供自定义Pod亲和性或反亲和性、节点亲和性以及基于污点及容忍度的调度机制。



#### 3.1.1.1：常用的预选策略

预选策略实际上就是节点过滤器，例如节点标签必须能够匹配到Pod资源的标签选择器（MatchNodeSelector实现的规则），以及Pod容器的资源请求量不能大于节点上剩余的可分配资源（PodFitsResource规则）等等。执行预选操作，调度器会逐一根据规则进行筛选，如果预选没能选定一个合适的节点，此时Pod会一直处于Pending状态，直到有一个可用节点完成调度。其常用的预选策略如下：

- CheckNodeCondition：检查是否可以在节点报告磁盘、网络不可用或未准备好的情况下将Pod对象调度其上。
- HostName：如果Pod对象拥有spec.hostname属性，则检查节点名称字符串是否和该属性值匹配。
- PodFitsHostPorts：如果Pod对象定义了ports.hostPort属性，则检查Pod指定的端口是否已经被节点上的其他容器或服务占用。
- MatchNodeSelector：如果Pod对象定义了spec.nodeSelector属性，则检查节点标签是否和该属性匹配。
- NoDiskConflict：检查Pod对象请求的存储卷在该节点上可用。
- PodFitsResources：检查节点上的资源（CPU、内存）可用性是否满足Pod对象的运行需求。
- PodToleratesNodeTaints：如果Pod对象中定义了spec.tolerations属性，则需要检查该属性值是否可以接纳节点定义的污点（taints）。
- PodToleratesNodeNoExecuteTaints：如果Pod对象定义了spec.tolerations属性，检查该属性是否接纳节点的NoExecute类型的污点。
- CheckNodeLabelPresence：仅检查节点上指定的所有标签的存在性，要检查的标签以及其可否存在取决于用户的定义。
- CheckServiceAffinity：根据当前Pod对象所属的Service已有其他Pod对象所运行的节点调度，目前是将相同的Service的Pod对象放在同一个或同一类节点上。
- MaxEBSVolumeCount：检查节点上是否已挂载EBS存储卷数量是否超过了设置的最大值，默认值：39
- MaxGCEPDVolumeCount：检查节点上已挂载的GCE     PD存储卷是否超过了设置的最大值，默认值：16
- MaxAzureDiskVolumeCount：检查节点上已挂载的Azure     Disk存储卷数量是否超过了设置的最大值，默认值：16
- CheckVolumeBinding：检查节点上已绑定和未绑定的PVC是否满足Pod对象的存储卷需求。
- NoVolumeZoneConflct：在给定了区域限制的前提下，检查在该节点上部署Pod对象是否存在存储卷冲突。
- CheckNodeMemoryPressure：在给定了节点已经上报了存在内存资源压力过大的状态，则需要检查该Pod是否可以调度到该节点上。
- CheckNodePIDPressure：如果给定的节点已经报告了存在PID资源压力过大的状态，则需要检查该Pod是否可以调度到该节点上。
- CheckNodeDiskPressure：如果给定的节点存在磁盘资源压力过大，则检查该Pod对象是否可以调度到该节点上。
- MatchInterPodAffinity：检查给定的节点能否可以满足Pod对象的亲和性和反亲和性条件，用来实现Pod亲和性调度或反亲和性调度。

在上面的这些预选策略里面，CheckNodeLabelPressure和CheckServiceAffinity可以在预选过程中结合用户自定义调度逻辑，这些策略叫做可配置策略。其他不接受参数进行自定义配置的称为静态策略。



#### 3.1.1.2：优先函数

预选策略筛选出一个节点列表就会进入优选阶段，在这个过程调度器会向每个通过预选的节点传递一系列的优选函数来计算其优先级分值，优先级分值介于0-10之间，其中0表示不适用，10表示最适合托管该Pod对象。

另外，调度器还支持给每个优选函数指定一个简单的值，表示权重，进行节点优先级分值计算时，它首先将每个优选函数的计算得分乘以权重，然后再将所有优选函数的得分相加，从而得出节点的最终优先级分值。权重可以让管理员定义优选函数倾向性的能力，其计算优先级的得分公式如下：

finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + ......

下图是关于优选函数的列表图：

![image-20220625183736460](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220625183736460.png)

#### 3.1.1.3：节点亲和调度

节点亲和性是用来确定Pod对象调度到哪一个节点的规则，这些规则基于节点上的自定义标签和Pod对象上指定的标签选择器进行定义。

定义节点亲和性规则有2种：硬亲和性（require）和软亲和性（preferred）

- 硬亲和性：实现的是强制性规则，是Pod调度时必须满足的规则，否则Pod对象的状态会一直是Pending
- 软亲和性：实现的是一种柔性调度限制，在Pod调度时可以尽量满足其规则，在无法满足规则时，可以调度到一个不匹配规则的节点之上。

定义节点亲和规则的两个要点：一是节点配置是否合乎需求的标签，而是Pod对象定义合理的标签选择器，这样才能够基于标签选择出期望的目标节点。



需要注意的是preferredDuringSchedulingIgnoredDuringExecution和requiredDuringSchedulingIgnoredDuringExecution名字中后半段字符串IgnoredDuringExecution表示的是，在Pod资源基于节点亲和性规则调度到某个节点之后，如果节点的标签发生了改变，调度器不会讲Pod对象从该节点上移除，因为该规则仅对新建的Pod对象有效。



#### 3.1.1.4：node节点标签调度

```bash
root@s3:~/manifests/schudel# kubectl label node 192.168.48.169 disktype=ssd
root@s3:~/manifests/schudel# cat nodeselector.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo
  namespace: default
  labels: 
    app: myapp
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  nodeSelector:
    disktype: ssd
    
root@s3:~/manifests/schudel# kubectl  apply -f nodeselector.yaml 
pod/pod-demo created
root@s3:~/manifests/schudel# kubectl get po -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP               NODE             NOMINATED NODE   READINESS GATES
pod-demo   1/1     Running   0          4m40s   10.200.152.231   192.168.48.169   <none>           <none>

root@s3:~/manifests/schudel# kubectl get node --show-labels
NAME             STATUS                     ROLES    AGE   VERSION   LABELS
192.168.48.163   Ready,SchedulingDisabled   master   23d   v1.22.5   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.48.163,kubernetes.io/os=linux,kubernetes.io/role=master
192.168.48.166   Ready,SchedulingDisabled   master   23d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.48.166,kubernetes.io/os=linux,kubernetes.io/role=master
192.168.48.169   Ready                      node     22d   v1.22.5   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.48.169,kubernetes.io/os=linux,kubernetes.io/role=node
192.168.48.170   Ready                      node     23d   v1.22.5   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.48.170,kubernetes.io/os=linux,kubernetes.io/role=node
```

#### 3.1.1.5: 节点硬亲和性

下面的配置清单中定义的Pod对象，使用节点硬亲和性和规则定义将当前Pod调度到标签为zone=foo的节点上：

```bash
root@s3:~/manifests/schudel# cat affinity-node.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-daemon-1
  namespace: default
  labels: 
    app: myapp-label
spec:
  containers:
  - name: myapp-pod
    image: ikubernetes/myapp:v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values:
            - foo
root@s3:~/manifests/schudel# kubectl apply -f affinity-node.yaml 
pod/pod-daemon-1 created
由于集群中没有节点含有便签为zone=fo所以创建Pod一直处于pending状态
root@s3:~/manifests/schudel# kubectl get po
NAME           READY   STATUS    RESTARTS   AGE
pod-daemon-1   0/1     Pending   0          7s
root@s3:~/manifests/schudel# kubectl describe po pod-daemon-1
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  2m44s  default-scheduler  0/4 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 2 node(s) were unschedulable.
  Warning  FailedScheduling  86s    default-scheduler  0/4 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 2 node(s) were unschedulable
  
给node1节点打上zone=foo的标签，可以看到成功调度到node1节点上
root@s3:~/manifests/schudel# kubectl label node 192.168.48.169 zone=foo
node/192.168.48.169 labeled
root@s3:~/manifests/schudel# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
pod-daemon-1   1/1     Running   0          3m56s
  
```



在定义节点亲和性时，requiredDuringSchedulingIgnoredDuringExecution字段的值是一个对象列表，用于定义节点硬亲和性，它可以由一个或多个nodeSelectorTerms定义的对象组成，此时值需要满足其中一个nodeSelectorTerms即可。

而nodeSelectorTerms用来定义节点选择器的条目，它的值也是一个对象列表，由1个或多个matchExpressions对象定义的匹配规则组成，多个规则是逻辑与的关系，这就表示某个节点的标签必须要满足同一个nodeSelectorTerms下所有的matchExpression对象定义的规则才能够成功调度。如下：

 

如下配置清单，必须存在满足标签zone=foo和ssd=myapp的节点才能够调度成功

```bash
root@s3:~/manifests/schudel# cat affinity-node.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-daemon-1
  namespace: default
  labels: 
    app: myapp-label
spec:
  containers:
  - name: myapp-pod
    image: ikubernetes/myapp:v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values:
            - foo
          - key: ssd
            operator: In
            values:
            - myapp
#Pod一值处于pending状态
root@s3:~/manifests/schudel# kubectl get po
NAME           READY   STATUS    RESTARTS   AGE
pod-daemon-1   0/1     Pending   0          4s
root@s3:~/manifests/schudel# kubectl label node 192.168.48.169 ssd=myapp
node/192.168.48.169 labeled
root@s3:~/manifests/schudel# kubectl get po
NAME           READY   STATUS    RESTARTS   AGE
pod-daemon-1   1/1     Running   0          96s
```



#### 3.1.1.6: 节点软亲和性

```bash
root@s3:~/manifests/schudel# kubectl explain pod.spec.affinity.nodeAffinity
root@s3:~/manifests/schudel# cat affinity-node2.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-demo
  namespace: default
  labels: 
    app: myapp
spec:
  replicas: 5
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - foo
                - bar
            weight: 60
root@s3:~/manifests/schudel# kubectl apply -f affinity-node2.yaml 
deployment.apps/pod-demo created

#可以看到5个Pod分别分布在不同的节点上，node01上没有对应的标签也会调度上进行创建Pod，体现软亲和性
root@s3:~/manifests/schudel# kubectl get po -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP               NODE             NOMINATED NODE   READINESS GATES
pod-demo-bfb744844-bzpwq   1/1     Running   0          8s    10.200.78.189    192.168.48.170   <none>           <none>
pod-demo-bfb744844-n9s2c   1/1     Running   0          8s    10.200.152.238   192.168.48.169   <none>           <none>
pod-demo-bfb744844-rt7d5   1/1     Running   0          8s    10.200.78.190    192.168.48.170   <none>           <none>
pod-demo-bfb744844-tvzr5   1/1     Running   0          8s    10.200.78.191    192.168.48.170   <none>           <none>
pod-demo-bfb744844-z8t6d   1/1     Running   0          8s    10.200.152.240   192.168.48.169   <none>           <none>

如果我们给node01打上zone=foo的标签，再去创建时，你会发现所有的Pod都调度在这个节点上。因为节点的软亲和性，会尽力满足Pod中定义的规则，如下
root@s3:~/manifests/schudel# kubectl label node 192.168.48.169 zone=foo
root@s3:~/manifests/schudel# kubectl delete -f affinity-node2.yaml
root@s3:~/manifests/schudel# kubectl apply -f affinity-node2.yaml 
deployment.apps/pod-demo created

root@s3:~/manifests/schudel# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP               NODE             NOMINATED NODE   READINESS GATES
pod-demo-59875f759c-95s8f   1/1     Running   0          9s    10.200.152.247   192.168.48.169   <none>           <none>
pod-demo-59875f759c-mfllq   1/1     Running   0          9s    10.200.152.255   192.168.48.169   <none>           <none>
pod-demo-59875f759c-mmjf6   1/1     Running   0          9s    10.200.152.242   192.168.48.169   <none>           <none>
pod-demo-59875f759c-wlwhb   1/1     Running   0          9s    10.200.78.135    192.168.48.169   <none>           <none>
pod-demo-59875f759c-zz5n9   1/1     Running   0          9s    10.200.78.133    192.168.48.169   <none>           <none>

上面的实验结果显示，当2个标签没有都存在一个node节点上时，Pod对象会被分散在集群中的三个节点上进行创建并运行，之所以如此，是因为使用了 节点软亲和性的预选方式，所有的节点都能够通过MatchNodeSelector预选策略的筛选。当我们将2个标签都集合在node01上时，所有Pod对象都会运行在node01之上
```

### 3.1.2: Pod资源亲和调度

#### 3.1.2.1： pod硬亲和度

在出于高效通信的需求，有时需要将一些Pod调度到相近甚至是同一区域位置（比如同一节点、机房、区域）等等，比如业务的前端Pod和后端Pod，此时这些Pod对象之间的关系可以叫做亲和性。

同时出于安全性的考虑，也会把一些Pod之间进行隔离，此时这些Pod对象之间的关系叫做反亲和性（anti-affinity）。

调度器把第一个Pod放到任意位置，然后和该Pod有亲和或反亲和关系的Pod根据该动态完成位置编排，这就是Pod亲和性和反亲和性调度的作用。Pod的亲和性定义也存在硬亲和性和软亲和性的区别，其约束的意义和节点亲和性类似。

Pod的亲和性调度要求各相关的Pod对象运行在同一位置，而反亲和性则要求它们不能运行在同一位置。这里的位置实际上取决于节点的位置拓扑，拓扑的方式不同，Pod是否在同一位置的判定结果也会有所不同。

如果基于各个节点的kubernetes.io/hostname标签作为评判标准，那么会根据节点的hostname去判定是否在同一位置区域。

```bash
root@s3:/data# kubectl run tomcat -l app=tomcat --image=ikubernetes/myapp:v1
pod/tomcat created
root@s3:/data# kubectl get po -l app=tomcat -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES
tomcat   1/1     Running   0          16s   10.200.78.144   192.168.48.170   <none>           <none>

```

从上面我们可以看到新创建的tomcatpod对象被调度在192.168.48.170上，再写一个配置清单定义一个Pod对象，通过labelSelector定义的标签选择器挑选对应的Pod对象。

```bash
root@s3:~/manifests/schudel# cat require-pod-affinity.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions: 
          - {key: app, operator: In, values: ["tomcat"]}
        topologyKey: kubernetes.io/hostname
root@s3:~/manifests/schudel# kubectl apply -f require-pod-affinity.yaml 
pod/myapp created

root@s3:~/manifests/schudel# kubectl get po -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES
myapp    1/1     Running   0          10s   10.200.78.143   192.168.48.170   <none>           <none>
tomcat   1/1     Running   0          17m   10.200.78.144   192.168.48.170   <none>           <none>

基于单一节点的Pod亲和性相对来说使用的情况会比较少，通常使用的是基于同一地区、区域、机架等拓扑位置约束。比如部署应用程序（myapp）和数据库（db）服务相关的Pod时，这两种Pod应该部署在同一区域上，可以加速通信的速度。
```

#### 3.1.2.2: Pod软亲和度

同理，有硬亲和度即有软亲和度，Pod也支持使用preferredDuringSchedulingIgnoredDuringExecuttion属性进行定义Pod的软亲和性，调度器会尽力满足亲和约束的调度，在满足不了约束条件时，也允许将该Pod调度到其他节点上运行。比如下面这一配置清单

```bash
root@s3:~/manifests/schudel# cat preferred-podaffinity.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: preerred-pod-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - {key: app, operator: In, values: ["cache"]}
              topologyKey: zone
          - weight: 20
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - {key: app, operator: In, values: ["db"]}
              topologyKey: zone
              
              
 root@s3:~/manifests/schudel# kubectl apply -f preferred-podaffinity.yaml 
deployment.apps/preerred-pod-affinity created
上述的清单配置当中，pod的软亲和调度需要将Pod调度到标签为app=cache并在区域zone当中，或者调度到app=db标签节点上的，但是我们的节点上并没有类似的标签，所以调度器会根据软亲和调度进行随机调度到node节点之上。如下：
root@s3:~/manifests/schudel# kubectl get po -o wide
NAME                                     READY   STATUS    RESTARTS   AGE   IP               NODE             NOMINATED NODE   READINESS GATES
preerred-pod-affinity-67dbd695f9-956tt   1/1     Running   0          79s   10.200.78.142    192.168.48.170   <none>           <none>
preerred-pod-affinity-67dbd695f9-9gr42   1/1     Running   0          79s   10.200.152.193   192.168.48.169   <none>           <none>
preerred-pod-affinity-67dbd695f9-gj58b   1/1     Running   0          79s   10.200.78.145    192.168.48.170   <none>           <none>


```

#### 3.1.2.3: pod反亲和度

podAffinity定义了Pod对象的亲和约束，而Pod对象的反亲和调度则是用podAntiAffinty属性进行定义，下面的配置清单中定义了由同一Deployment创建但是彼此基于节点位置互斥的Pod对象：



```bash
root@s3:~/manifests/schudel# cat podantiaffinity.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-antiaffinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - {key: app, operator: In, values: ["myapp"]}
            topologyKey: kubernetes.io/hostname
            
root@s3:~/manifests/schudel# kubectl apply -f podantiaffinity.yaml 
deployment.apps/pod-antiaffinity created


由于在配置清单中定义了强制性反亲和性，先前创建的pod app=myapp标签分别运行在node1和node2上面，因此3个pod处于pending的状态

root@s3:~/manifests/schudel# kubectl get pods -l app=myapp
NAME                                     READY   STATUS    RESTARTS   AGE
pod-antiaffinity-6f8fd55f88-q94wj        0/1     Pending   0          114s
pod-antiaffinity-6f8fd55f88-wsdjw        0/1     Pending   0          114s
pod-antiaffinity-6f8fd55f88-xhdng        0/1     Pending   0          114s
preerred-pod-affinity-67dbd695f9-7gtd8   1/1     Running   0          7m27s
preerred-pod-affinity-67dbd695f9-9v8td   1/1     Running   0          7m27s
preerred-pod-affinity-67dbd695f9-kng2v   1/1     Running   0          7m27s

把之前的deploy进行删除
再次查看处于running状态
root@s3:~/manifests/schudel# kubectl get pods -l app=myapp
NAME                                     READY   STATUS    RESTARTS   AGE
preerred-pod-affinity-67dbd695f9-7gtd8   1/1     Running   0          7m53s
preerred-pod-affinity-67dbd695f9-9v8td   1/1     Running   0          7m53s
preerred-pod-affinity-67dbd695f9-kng2v   1/1     Running   0          7m53s

```

### 3.1.3：污点和容忍度

污点的定义是在节点的nodeSpec，而容忍度的定义是在Pod中的podSpec，都属于键值型数据，两种方式都支持一个effect标记，语法格式为key=value: effect，其中key和value的用户和格式和资源注解类似，而effect是用来定义对Pod对象的排斥等级，主要包含以下3种类型：

- NoSchedule：不能容忍此污点的新Pod对象不能调度到该节点上，属于强制约束，节点现存的Pod对象不受影响。
- PreferNoSchedule：NoSchedule属于柔性约束，即不能容忍此污点的Pod对象尽量不要调度到该节点，不过无其他节点可以调度时也可以允许接受调度。
- NoExecute：不能容忍该污点的新Pod对象不能调度该节点上，强制约束，节点现存的Pod对象因为节点污点变动或Pod容忍度的变动导致无法匹配规则，Pod对象就会被从该节点上去除。

在Pod对象上定义容忍度时，其支持2中操作符：Equal和Exists

- Equal：等值比较，表示容忍度和污点必须在key、value、effect三者之上完全匹配。
- Exists：存在性判断，表示二者的key和effect必须完全匹配，而容忍度中的value字段使用空值。

#### 3.2.3.1：污点简介

* 污点（taints）用于node节点排斥pod调度，与亲和的作用是完全相反的既taint的node和pod是排斥调度关系

* 容忍(toleration)用于Pod容忍node节点的污点信息，既node有污点信息也会讲新的pod调度到node

* 污点的三种类型

  NoSchedule: 表示k8s将不会将pod调度到具有污点的node上

  #kubectl taint nodes 192.168.48.169 key1=value1:NoSchudule #设置污点

  node/192.168.48.169 tainted

  #kubectl describe node 192.168.48.169 #查看污点

  Taints:   key1=value1:NoSchedule

  #kubectl taint node 192.168.48.169 key1:NoSchedule- #取消污点

* PreferNoSchedule: 表示k8s将尽量避免将pod调度到具有该污点的Node上

* NoExecute: 表示k8s将不会将pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去

#### 3.2.3.2： 容忍简介：

* tolerations容忍

   定义pod的容忍度，可以调度至含有污点的node

* 基于operator的污点匹配

  如果operator是Exists,则容忍度不需要value而是直接匹配污点类型

  如果operator是Equal，则需要指定value并且value的值需要等于tolerations的key

#### 3.2.3.3：管理节点的污点

```bash
root@s3:~/manifests/schudel# kubectl taint --help
Usage:
  kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ...
KEY_N=VAL_N:TAINT_EFFECT_N [options]

#定义node上的污点
root@s3:~/manifests/schudel# kubectl taint node 192.168.48.169 node-type=production:NoSchedule
node/192.168.48.169 tainted

root@s3:/data# cat admin.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-pod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
root@s3:/data# kubectl apply -f admin.yaml 
deployment.apps/myapp-pod created

root@s3:/data# kubectl get po -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES
myapp-pod-ccd44fb7-69cfr   1/1     Running   0          10s   10.200.78.156   192.168.48.170   <none>           <none>
myapp-pod-ccd44fb7-pdkzh   1/1     Running   0          10s   10.200.78.152   192.168.48.170   <none>           <none>
myapp-pod-ccd44fb7-vtbmp   1/1     Running   0          10s   10.200.78.148   192.168.48.170 

node1已经被打不调度污点，创建的Po都调度到node2上



#node2打污点
root@s3:/data# kubectl taint node 192.168.48.170 node-type=dev:NoExecute
node/192.168.48.170 tainted
root@s3:/data# kubectl get po
NAME                       READY   STATUS    RESTARTS   AGE
myapp-pod-ccd44fb7-4g7xp   0/1     Pending   0          55s
myapp-pod-ccd44fb7-4h2q4   0/1     Pending   0          56s
myapp-pod-ccd44fb7-j6x59   0/1     Pending   0          56s
```

#### 3.2.3.4： pod对象的容忍度

Pod对象的容忍度可以通过spec.tolerations字段进行添加，同一的也有两种操作符：Equal和Exists方式。Equal等值方式如下

```bash
root@s4:~# kubectl explain pod.spec.tolerations


root@s3:/data# cat tolerations.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-pod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
      tolerations:
      - key: "node-type"
        operator: "Equal"
        value: "production"
        effect: "NoSchedule"
        
root@s3:/data# kubectl apply -f admin.yaml 
deployment.apps/myapp-pod configured

root@s3:/data# kubectl get po -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP               NODE             NOMINATED NODE   READINESS GATES
myapp-pod-58cdb8fcf4-lls5n   1/1     Running   0          4m28s   10.200.152.194   192.168.48.169   <none>           <none>
myapp-pod-58cdb8fcf4-m74fn   1/1     Running   0          4m37s   10.200.152.253   192.168.48.169   <none>           <none>
myapp-pod-58cdb8fcf4-zg9hj   1/1     Running   0          4m32s   10.200.152.254   192.168.48.169   <none>           <none>


#容忍key所有污点
root@s3:/data# cat tolerations.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-pod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
      tolerations:
      - key: "node-type"
        operator: "Exists"
        value: ""
        effect: ""

root@s3:/data# kubectl get po -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP               NODE             NOMINATED NODE   READINESS GATES
myapp-pod-86b55fd47f-8h8c2   1/1     Running   0          7s    10.200.152.198   192.168.48.169   <none>           <none>
myapp-pod-86b55fd47f-df5rg   1/1     Running   0          4s    10.200.78.155    192.168.48.170   <none>           <none>
myapp-pod-86b55fd47f-fr5vl   1/1     Running   0          10s   10.200.78.154    192.168.48.170   <none>           <none>

```

## 3.2: 核心指标与监控指标

### 3.2.1：metrics-server

https://github.com/kubernetes-sigs/metrics-server   #github

在最初的系统资源监控，是通过cAdvisor去收集单个节点以及相关Pod资源的指标数据，但是这一功能仅能够满足单个节点，在集群日益庞大的过程中，该功能就显得low爆了。于是将各个节点的指标数据进行汇聚并通过一个借口进行向外暴露传送是必要的。

Heapster群提供指标API和实现并进行监控，它是集群级别的监控和事件数据的聚合工具，但是一个完备的Heapster监控体系是需要进行数据存储的，为此其解决方案就是引入了Influxdb作为后端数据的持久存储，Grafana作为可视化的接口。原理就是Heapster从各个节点上的cAdvisor采集数据并存储到Influxdb中，再由Grafana展示。原理图如下：

![image-20220629105138498](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629105138498.png)

时代在变迁，陈旧的东西将会被淘汰，由于功能和系统发展的需求，Heapster无法满足k8s系统监控的需求，为此在Kubernetes 1.7版本以后引入了自定义指标(custom metrics API)，在1.8版本引入了资源指标（resource metrics API）。逐渐地Heapster用于提供核心指标API的功能也被聚合方式的指标API服务器metrics-server所替代。

**在新一代的Kubernetes指标监控体系当中主要由核心指标流水线和监控指标流水线组成：**

- 核心指标流水线：

是指由kubelet、、metrics-server以及由API server提供的api组成，它们可以为K8S系统提供核心指标，从而了解并操作集群内部组件和程序。其中相关的指标包括CPU的累积使用率、内存实时使用率，Pod资源占用率以及容器磁盘占用率等等。其中核心指标的获取原先是由heapster进行收集，但是在1.11版本之后已经被废弃，从而由新一代的metrics-server所代替对核心指标的汇聚。核心指标的收集是必要的。如下图：

![image-20220629105742805](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629105742805.png)



- 监控指标流水线：

用于从系统收集各种指标数据并提供给终端用户、存储系统以及HPA。它们包含核心指标以及许多非核心指标，其中由于非核心指标本身不能被Kubernetes所解析，此时就需要依赖于用户选择第三方解决方案。如下图：

![image-20220629105831253](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629105831253.png)



一个可以同时使用资源指标API和自定义指标API的组件是HPAv2，其实现了通过观察指标实现自动扩容和缩容。而目前资源指标API的实现主流是metrics-server。

自1.8版本后，容器的cpu和内存资源占用利用率都可以通过客户端指标API直接调用，从而获取资源使用情况，要知道的是API本身并不存储任何指标数据，仅仅提供资源占用率的实时监测数据。

资源指标和其他的API指标并没有啥区别，它是通过API Server的URL路径/apis/metrics.k8s.io/进行存取，只有在k8s集群内部署了metrics-server应用才能只用API，其简单的结构图如下



![image-20220629110042385](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629110042385.png)



Heapster。 Metrics Server 通过 Kubernetes 聚合 器（ kube- aggregator） 注册 到 主 API Server 之上， 而后 基于 kubelet 的 Summary API 收集 每个 节 点上 的 指标 数据， 并将 它们 存储 于 内存 中 然后 以 指标 API 格式 提供，如下图





![image-20220629110112632](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629110112632.png)



一般说来， Metrics Server 在 每个 集群 中 仅 会 运行 一个 实例， 启动 时， 它将 自动 初始化 与 各 节点 的 连接， 因此 出于 安全 方面 的 考虑， 它 需要 运行 于 普通 节点 而非 Master 主机 之上。 直接 使用 项目 本身 提供 的 资源 配置 清单 即 能 轻松 完成 metrics- server 的 部署。



#### 3.2.1.1: 部署metrics-server

```bash
root@s3:~# wget  https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
root@s3:~# kubectl apply -f components.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created

root@s3:~#
… 
  spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --kubelet-insecure-tls #添加这一条
        image: k8s.gcr.io/metrics-server/metrics-server:v0.4.2
        imagePullPolicy: IfNotPresent
………..
root@s3:~# kubectl apply -f components.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created


#查看podsmestrics-server拉取镜像失败
root@s3:~# kubectl get po -n kube-system
NAME                                       READY   STATUS             RESTARTS        AGE
calico-kube-controllers-647f956d86-8g4xf   1/1     Running            5 (119m ago)    5d19h
calico-node-hq7wz                          1/1     Running            19 (38h ago)    26d
calico-node-p7jtd                          1/1     Running            30 (119m ago)   26d
calico-node-tlvtj                          1/1     Running            23 (119m ago)   26d
calico-node-vvpwt                          1/1     Running            22 (119m ago)   26d
coredns-5785f8c645-89cl5                   1/1     Running            17 (119m ago)   26d
metrics-server-5856f7565c-w25hp            0/1     ErrImagePull       0               4m22s
#由于拉取的是国外源，这里不能翻墙
root@s3:~# kubectl describe po metrics-server-5856f7565c-w25hp -n kube-system
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  12m                   default-scheduler  Successfully assigned kube-system/metrics-server-5856f7565c-w25hp to 192.168.48.169
  Normal   Pulling    10m (x4 over 12m)     kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.1"
  Warning  Failed     10m (x4 over 12m)     kubelet            Failed to pull image "k8s.gcr.io/metrics-server/metrics-server:v0.6.1": rpc error: code = Unknown desc = Error response from daemon: Get "https://k8s.gcr.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Failed     10m (x4 over 12m)     kubelet            Error: ErrImagePull
  Warning  Failed     9m38s (x6 over 12m)   kubelet            Error: ImagePullBackOff

#把国外源替换国内源
root@s3:~# sed -i 's/k8s.gcr.io\/metrics-server/registry.cn-hangzhou.aliyuncs.com\/google_containers/g' components.yaml


root@s3:~# kubectl get po -n kube-system
NAME                                       READY   STATUS             RESTARTS         AGE
calico-kube-controllers-647f956d86-8g4xf   1/1     Running            5 (4h12m ago)    5d21h
calico-node-hq7wz                          1/1     Running            19 (40h ago)     26d
calico-node-p7jtd                          1/1     Running            30 (4h12m ago)   26d
calico-node-tlvtj                          1/1     Running            23 (4h12m ago)   26d
calico-node-vvpwt                          1/1     Running            22 (4h12m ago)   26d
coredns-5785f8c645-89cl5                   1/1     Running            17 (4h12m ago)   26d
metrics-server-5849f8bdfd-nck4x            1/1     Running            0                2m4s


#查看群组
root@s3:/data# kubectl api-versions
.....
metrics.k8s.io/v1beta1
.....
```

#### 3.2.1.1：查看node资源

```
root@s3:~# kubectl top nodes
NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%     
192.168.48.163   194m         19%    1200Mi          72%         
192.168.48.166   177m         17%    1429Mi          73%         
192.168.48.169   134m         13%    1009Mi          61%  
```

#### 3.2.1.2：查看pod资源

```
root@s3:/data# kubectl top pod myapp-pod-86b55fd47f-6qrmw -n default
NAME                         CPU(cores)   MEMORY(bytes)   
myapp-pod-86b55fd47f-6qrmw   0m           2Mi  
```

### 3.2.2：监控指标prometheus

#### 3.2.2.1: prometheus概述

除了前面的资源指标（如CPU、内存）以外，用户或管理员需要了解更多的指标数据，比如Kubernetes指标、容器指标、节点资源指标以及应用程序指标等等。自定义指标API允许请求任意的指标，其指标API的实现要指定相应的后端监视系统。而Prometheus是第一个开发了相应适配器的监控系统。这个适用于Prometheus的Kubernetes Customm Metrics Adapter是属于Github上的k8s-prometheus-adapter项目提供的。其原理图如下：

![image-20220629141944472](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629141944472.png)

要知道的是prometheus本身就是一监控系统，也分为server端和agent端，server端从被监控主机获取数据，而agent端需要部署一个node_exporter，主要用于数据采集和暴露节点的数据，那么 在获取Pod级别或者是mysql等多种应用的数据，也是需要部署相关的exporter。我们可以通过PromQL的方式对数据进行查询，但是由于本身prometheus属于第三方的 解决方案，原生的k8s系统并不能对Prometheus的自定义指标进行解析，就需要借助于k8s-prometheus-adapter将这些指标数据查询接口转换为标准的Kubernetes自定义指标。

Prometheus是一个开源的服务监控系统和时序数据库，其提供了通用的数据模型和快捷数据采集、存储和查询接口。它的核心组件Prometheus服务器定期从静态配置的监控目标或者基于服务发现自动配置的目标中进行拉取数据，新拉取到啊的 数据大于配置的内存缓存区时，数据就会持久化到存储设备当中。Prometheus组件架构图如下

![image-20220629142134722](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629142134722.png)

如上图，每个被监控的主机都可以通过专用的exporter程序提供输出监控数据的接口，并等待Prometheus服务器周期性的进行数据抓取。如果存在告警规则，则抓取到数据之后会根据规则进行计算，满足告警条件则会生成告警，并发送到Alertmanager完成告警的汇总和分发。当被监控的目标有主动推送数据的需求时，可以以Pushgateway组件进行接收并临时存储数据，然后等待Prometheus服务器完成数据的采集。

任何被监控的目标都需要事先纳入到监控系统中才能进行时序数据采集、存储、告警和展示，监控目标可以通过配置信息以静态形式指定，也可以让Prometheus通过服务发现的机制进行动态管理。下面是组件的一些解析：

- 监控代理程序：如node_exporter：收集主机的指标数据，如平均负载、CPU、内存、磁盘、网络等等多个维度的指标数据。
- kubelet（cAdvisor）：收集容器指标数据，也是K8S的核心指标收集，每个容器的相关指标数据包括：CPU使用率、限额、文件系统读写限额、内存使用率和限额、网络报文发送、接收、丢弃速率等等。
- API Server：收集API     Server的性能指标数据，包括控制队列的性能、请求速率和延迟时长等等
- etcd：收集etcd存储集群的相关指标数据
- kube-state-metrics：该组件可以派生出k8s相关的多个指标数据，主要是资源类型相关的计数器和元数据信息，包括制定类型的对象总数、资源限额、容器状态以及Pod资源标签系列等。

Prometheus 能够 直接 把 Kubernetes API Server 作为 服务 发现 系统 使用 进而 动态 发现 和 监控 集群 中的 所有 可被 监控 的 对象。 这里 需要 特别 说明 的 是， Pod 资源 需要 添加 下列 注解 信息 才 能被 Prometheus 系统 自动 发现 并 抓取 其 内建 的 指标 数据。

- prometheus. io/ scrape： 用于 标识     是否 需要 被 采集 指标 数据， 布尔 型 值， true 或 false。
- prometheus. io/ path： 抓取 指标     数据 时 使用 的 URL 路径， 一般 为/ metrics。
- prometheus. io/ port： 抓取 指标     数据 时 使 用的 套 接 字 端口， 如 8080。

另外， 仅 期望 Prometheus 为 后端 生成 自定义 指标 时 仅 部署 Prometheus 服务器 即可， 它 甚至 也不 需要 数据 持久 功能。 但 若要 配置 完整 功能 的 监控 系统， 管理员 还需 要在 每个 主机 上 部署 node_ exporter、 按 需 部署 其他 特有 类型 的 exporter 以及 Alertmanager。

#### 3.2.2.2：prometheus部署

由于官方的YAML部署方式需要使用到PVC，这里使用马哥提供的学习类型的部署，具体生产还是需要根据官方的建议进行。

https://github.com/iKubernetes/k8s-prom

```bash
root@s3:~# git clone https://github.com/iKubernetes/k8s-prom.git
root@s3:~# cd k8s-prom/
root@s3:~/k8s-prom# kubectl apply -f namespace.yaml 
namespace/prom created

```

#### 3.2.2.3: 部署node_exporter

```bash
root@s3:~/k8s-prom# cd node_exporter/
root@s3:~/k8s-prom/node_exporter# kubectl apply -f .
daemonset.apps/prometheus-node-exporter created
service/prometheus-node-exporter created
root@s3:~/k8s-prom/node_exporter# kubectl get po -n prom
NAME                             READY   STATUS    RESTARTS   AGE
prometheus-node-exporter-cgdms   1/1     Running   0          31s
prometheus-node-exporter-qcdv7   1/1     Running   0          31s
prometheus-node-exporter-smlhs   1/1     Running   0          31s

```

#### 3.2.2.4: 部署prometheus-server

```bash
root@s3:~/k8s-prom# cd prometheus/
root@s3:~/k8s-prom/prometheus# kubectl apply -f .
configmap/prometheus-config unchanged
deployment.apps/prometheus-server unchanged
clusterrole.rbac.authorization.k8s.io/prometheus created
serviceaccount/prometheus unchanged
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus unchanged
root@s3:~/k8s-prom/prometheus# kubectl get all -n prom
NAME                                     READY   STATUS    RESTARTS   AGE
pod/prometheus-node-exporter-cgdms       1/1     Running   0          10m
pod/prometheus-node-exporter-qcdv7       1/1     Running   0          10m
pod/prometheus-node-exporter-smlhs       1/1     Running   0          10m
pod/prometheus-server-56947f9447-jp4td   1/1     Running   0          33s





#浏览器测试
http://192.168.48.166:30090/
```

#### 3.2.2.5: 部署kube-sate-metrics

```bash
root@s3:~/k8s-prom# cd kube-state-metrics/
root@s3:~/k8s-prom/kube-state-metrics# ls
kube-state-metrics-deploy.yaml  kube-state-metrics-rbac.yaml  kube-state-metrics-svc.yaml
root@s3:~/k8s-prom/kube-state-metrics# kubectl apply -f .
deployment.apps/kube-state-metrics created
serviceaccount/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
service/kube-state-metrics created

root@s3:/data# kubectl get po -n prom
NAME                                  READY   STATUS    RESTARTS   AGE
kube-state-metrics-7b99db8cdb-rc74g   1/1     Running   0          3m13s
prometheus-node-exporter-cgdms        1/1     Running   0          105m
prometheus-node-exporter-qcdv7        1/1     Running   0          105m
prometheus-node-exporter-smlhs        1/1     Running   0          105m
prometheus-server-56947f9447-jp4td    1/1     Running   0          95m

```

#### 3.2.2.6: 制作证书

```
root@s3:~/k8s-prom# cat k8s-prometheus-adapter/custom-metrics-apiserver-deployment.yaml | grep secret
        secret:
          secretName: cm-adapter-serving-certs
root@s3:~/k8s-prom# cd /etc/kubernetes/ssl/
root@s3:/etc/kubernetes/ssl# (umask 077;openssl genrsa -out serving.key 2048)
Generating RSA private key, 2048 bit long modulus (2 primes)
...........................................................................................................+++++
.............................................................................................................+++++
e is 65537 (0x010001)
root@s3:/etc/kubernetes/ssl# openssl req -new -key serving.key -out serving.csr -subj "/CN=serving"
root@s3:/etc/kubernetes/ssl# openssl x509 -req -in serving.csr -CA ./ca.pem -CAkey ./ca-key.pem -CAcreateserial -out serving.crt -days 3650
Signature ok
subject=CN = serving
Getting CA Private Key
root@s3:/etc/kubernetes/ssl# kubectl create secret generic cm-adapter-serving-certs --from-file=serving.crt=./serving.crt --from-file=serving.key=./serving.key -n prom
secret/cm-adapter-serving-certs created

root@s3:/etc/kubernetes/ssl# kubectl get secrets -n prom
NAME                             TYPE                                  DATA   AGE
cm-adapter-serving-certs         Opaque                                2      23s
```

#### 3.2.2.7: 部署k8s-prometheus-adapter

如果由于历史版本更新原因这里自带的custom-metrics-apiserver-deployment.yaml和custom-metrics-config-map.yaml可能会有点问题，需要下载k8s-prometheus-adapter项目中的这2个文件

由于这里不是翻墙下不下来，把网址复制到浏览器上面 下面全选，复制到虚拟机中

https://raw.githubusercontent.com/DirectXMan12/k8s-prometheus-adapter/master/deploy/manifests/custom-metrics-apiserver-deployment.yaml



![image-20220629172713079](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629172713079.png)



https://raw.githubusercontent.com/DirectXMan12/k8s-prometheus-adapter/master/deploy/manifests/custom-metrics-config-map.yaml #也需要修改名称空间为prom



```bash
root@s3:~/k8s-prom/k8s-prometheus-adapter# mv custom-metrics-apiserver-deployment.yaml{,.bak}
#复制custom-metrics-apiserver-deployment进去
root@s3:~/k8s-prom/k8s-prometheus-adapter# vim custom-metrics-apiserver-deployment.yaml
:set paste
#复制custom-metrics-config
root@s3:~/k8s-prom/k8s-prometheus-adapter# vim custom-metrics-config-map.yaml
:set paste
root@s3:~/k8s-prom/k8s-prometheus-adapter# kubectl apply -f .
clusterrolebinding.rbac.authorization.k8s.io/custom-metrics:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/custom-metrics-auth-reader created
deployment.apps/custom-metrics-apiserver created
clusterrolebinding.rbac.authorization.k8s.io/custom-metrics-resource-reader created
serviceaccount/custom-metrics-apiserver created
service/custom-metrics-apiserver created
apiservice.apiregistration.k8s.io/v1beta1.custom.metrics.k8s.io created
clusterrole.rbac.authorization.k8s.io/custom-metrics-server-resources created
configmap/adapter-config created
clusterrole.rbac.authorization.k8s.io/custom-metrics-resource-reader created
clusterrolebinding.rbac.authorization.k8s.io/hpa-controller-custom-metrics created


root@s3:~/k8s-prom/k8s-prometheus-adapter# kubectl get po -n prom -w
NAME                                        READY   STATUS    RESTARTS   AGE
custom-metrics-apiserver-5cffcbc4d5-slk7t   1/1     Running   0          4s
kube-state-metrics-7b99db8cdb-rc74g         1/1     Running   0          73m
prometheus-node-exporter-cgdms              1/1     Running   0          175m
prometheus-node-exporter-qcdv7              1/1     Running   0          175m
prometheus-node-exporter-smlhs              1/1     Running   0          175m
prometheus-server-56947f9447-jp4td          1/1     Running   0          165m

root@s3:~/k8s-prom/k8s-prometheus-adapter# kubectl api-versions | grep custom
custom.metrics.k8s.io/v1beta1


root@s3:~/k8s-prom/k8s-prometheus-adapter# kubectl get svc -n prom
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
custom-metrics-apiserver   ClusterIP   10.100.28.141   <none>        443/TCP          19m
kube-state-metrics         ClusterIP   10.100.173.38   <none>        8080/TCP         146m
prometheus                 NodePort    10.100.168.58   <none>        9090:30090/TCP   168m
prometheus-node-exporter   ClusterIP   None            <none>        9100/TCP         176m

```

访问IP:30090，如下图：选择 需要查看的指标，点击Execute

![image-20220629175140931](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220629175140931.png)

#### 3.2.2.8: Grafana数据展示

```
root@s3:~/k8s-prom# cat grafana.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-grafana
  namespace: prom
spec:
  replicas: 1
  selector:
    matchLabels:
      task: monitoring
      k8s-app: grafana
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: grafana
    spec:
      containers:
      - name: grafana
        image: gcr.io/google_containers/heapster-grafana-amd64:v5.0.4
        ports:
        - containerPort: 3000
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/ssl/certs
          name: ca-certificates
          readOnly: true
        - mountPath: /var/lib/grafana
          name: grafana-storage
        env:
        #- name: INFLUXDB_HOST
        #  value: monitoring-influxdb
        - name: GF_SERVER_HTTP_PORT
          value: "3000"
          # The following env variables are required to make Grafana accessible via
          # the kubernetes api-server proxy. On production clusters, we recommend
          # removing these env variables, setup auth for grafana, and expose the grafana
          # service using a LoadBalancer or a public IP.
        - name: GF_AUTH_BASIC_ENABLED
          value: "false"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Admin
        - name: GF_SERVER_ROOT_URL
          # If you're only using the API Server proxy, set this value instead:
          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
          value: /
      volumes:
      - name: ca-certificates
        hostPath:
          path: /etc/ssl/certs
      - name: grafana-storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: monitoring-grafana
  name: monitoring-grafana
  namespace: prom
spec:
  # In a production setup, we recommend accessing Grafana through an external Loadbalancer
  # or through a public IP.
  # type: LoadBalancer
  # You could also use NodePort to expose the service at a randomly-generated port
  # type: NodePort
  ports:
  - port: 80
    targetPort: 3000
  selector:
    k8s-app: grafana
  type: NodePort

root@s3:~/k8s-prom# kubectl apply -f grafana.yaml 
deployment.apps/monitoring-grafana created
service/monitoring-grafana created



root@s3:~/k8s-prom# kubectl get po -n prom
NAME                                        READY   STATUS    RESTARTS   AGE
custom-metrics-apiserver-5cffcbc4d5-slk7t   1/1     Running   0          10m
kube-state-metrics-7b99db8cdb-rc74g         1/1     Running   0          83m
monitoring-grafana-6b4f4cd476-cxv5n         1/1     Running   0          7m12s
prometheus-node-exporter-cgdms              1/1     Running   0          3h6m
prometheus-node-exporter-qcdv7              1/1     Running   0          3h6m
prometheus-node-exporter-smlhs              1/1     Running   0          3h6m
prometheus-server-56947f9447-jp4td          1/1     Running   0          175m
root@s3:~/k8s-prom# kubectl get svc -n prom
NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
custom-metrics-apiserver   ClusterIP   10.100.28.141    <none>        443/TCP          29m
kube-state-metrics         ClusterIP   10.100.173.38    <none>        8080/TCP         157m
monitoring-grafana         NodePort    10.100.237.106   <none>        80:30573/TCP     7m25s
prometheus                 NodePort    10.100.168.58    <none>        9090:30090/TCP   179m
prometheus-node-exporter   ClusterIP   None             <none>        9100/TCP         3h6m


访问grafana的地址：IP:30573，默认是没有kubernetes的模板的，可以到grafana.com中去下载相关的kubernetes模板。
```

### 3.2.3: Horizontal Pod Autoscaler（HPA）POD 横向自动扩展

HPA 与 RC、Deployment 一样，也属于 Kubernetes 资源对象。通过追踪分析 RC 或 RS 控制的所有目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 的副本数。
HPA 有以下两种方式作为 Pod 负载的度量指标：

- CPU Utilization Percentage (CPU利用率百分比)
- 应用程序自定义的度量指标，比如服务在每秒内的相应的请求数（ TPS 或 QPS ）。

CPU Utilization Percentage 是一个算术平均值，即目标 Pod 所有副本自带的 CPU 利用率的平均值。一个 Pod 自身的 CPU 利用率是该 Pod 当前 CPU 的使用量除以它的Pod Request 的值，比如我们定义一个 Pod 的 Pod Request 为 0.4，而当前 Pod 的 CPU 使用量为 0.2，则它的 CPU 使用率为 50%，这样我们就可以算出来一个 RC 或 RS  控制的所有 Pod 副本的 CPU 利用率的算术平均值了。如果某一时刻CPU Utilization Percentage的值超过 80%，则意味着当前的 Pod 副本数很可能不足以支撑接下来更多的请求，需要进行动态扩容，而当请求高峰时段过去后，Pod 的 CPU 利用率又会降下来，此时对应的Pod副本数应该自动减少到一个合理的水平。

支持对象：DeploymentConfig、ReplicationController、Deployment、Replica Set

#### 3.2.3.1： 横向自动扩展

RS可以通过HPA来根据一些运行时指标实现自动伸缩，下面是一个简单的例子

两种方法：

1、通过yaml文件创建 deployment

```bash
root@s3:~/resoure# cat resource.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: http
        image: ikubernetes/myapp:v1
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: "50m"
            memory: "256Mi"
          limits:
            cpu: "50m"
            memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: myapp
root@s3:~/resoure# kubectl apply -f resource.yaml 
deployment.apps/myapp unchanged
service/myapp created

```

2.直接通过kubectl autoscale直接创建

```bash
root@s3:~/resoure# kubectl autoscale --help
Usage:
  kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS]
--max=MAXPODS [--cpu-percent=CPU] [options]
root@s3:~/resoure# kubectl autoscale deployment myapp --min=1 --max=8 --cpu-percent=60
horizontalpodautoscaler.autoscaling/myapp autoscaled
root@s3:~/resoure# kubectl get hpa
NAME    REFERENCE          TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
myapp   Deployment/myapp   <unknown>/60%   1         8         0          14s

```

#### 3.2.3.2: 压力测试

```bash
root@s3:~/resoure# kubectl patch svc myapp -p '{"spec":{"type":"NodePort"}}'
service/myapp patched
root@s3:~/resoure# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.100.0.1      <none>        443/TCP        27d
myapp        NodePort    10.100.91.184   <none>        80:30830/TCP   17m

root@s1:~# apt-get install apache2-utils
root@s1:~# ab -c 1000 -n 50000 http://192.168.48.166:30830/index.html

root@s3:~/resoure# kubectl describe hpa
Name:                                                  myapp
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Wed, 29 Jun 2022 22:57:52 +0800
Reference:                                             Deployment/myapp
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  101% (50m) / 60%
Min replicas:                                          1
Max replicas:                                          8
Deployment pods:                                       2 current / 4 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 4
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  71s   horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  10s   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target

root@s3:~/resoure# kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
myapp-68554d6674-87tdb   1/1     Running   0          26s
myapp-68554d6674-8shhd   1/1     Running   0          30m
myapp-68554d6674-mhnvw   1/1     Running   0          86s
myapp-68554d6674-z2wkb   1/1     Running   0          26s

```

#### 3.2.3.3: 配置文件

```bash
root@s3:~/resoure# cat hap-v2-demon.yaml 
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa-v2
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 55
  - type: Resource
    resource:
      name: memory
      targetAverageValue: 50Mi
root@s3:~/resoure# kubectl apply -f hap-v2-demon.yaml
horizontalpodautoscaler.autoscaling/myapp-hpa-v2 created

root@s3:~/resoure# kubectl get hpa
NAME           REFERENCE          TARGETS                MINPODS   MAXPODS   REPLICAS   AGE
myapp-hpa-v2   Deployment/myapp   3919872/50Mi, 0%/55%   1         10        1          84s


```

#### 3.2.3.4：测试

```bash
root@s1:~# ab -c 1000 -n 50000 http://192.168.48.166:30830/index.html
root@s3:~/resoure# kubectl describe hpa
Name:                                                  myapp-hpa-v2
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Wed, 29 Jun 2022 23:33:54 +0800
Reference:                                             Deployment/myapp
Metrics:                                               ( current / target )
  resource memory on pods:                             3387392 / 50Mi
  resource cpu on pods  (as a percentage of request):  100% (50m) / 55%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       4 current / 4 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  80s   horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  19s   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
root@s3:~/resoure# kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
myapp-68554d6674-274rq   1/1     Running   0          26s
myapp-68554d6674-2lxzw   1/1     Running   0          87s
myapp-68554d6674-8shhd   1/1     Running   0          60m
myapp-68554d6674-wqkk8   1/1     Running   0          26s


root@s3:~/resoure# kubectl describe hpa
Name:                                                  myapp-hpa-v2
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Wed, 29 Jun 2022 23:33:54 +0800
Reference:                                             Deployment/myapp
Metrics:                                               ( current / target )
  resource memory on pods:                             4313088 / 50Mi
  resource cpu on pods  (as a percentage of request):  0% (0) / 55%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 1 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  6m51s  horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  5m50s  horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  17s    horizontal-pod-autoscaler  New size: 1; reason: All metrics below target
root@s3:~/resoure# kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
myapp-68554d6674-8shhd   1/1     Running   0          66m

```

## 3.3：Helm程序包管理

### 3.3.1：Helm是什么

Helm 是 Deis 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。

Helm Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件。可以在你部署应用的时候自定义应用程序的一些 Metadata，以便于应用程序的分发。

对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。

对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序

### 3.3.2：Helm的概念和结构

每个成功的软件平台都有一个优秀的打包系统，比如 Debian、Ubuntu 的 apt，Redhat、Centos 的 yum。而 Helm 则是 Kubernetes 上的包管理器。

**思考？？**Helm 到底解决了什么问题？为什么 Kubernetes 需要 Helm？

Kubernetes 能够很好地组织和编排容器，但它缺少一个更高层次的应用打包工具，而 Helm 就是来干这件事的。

举个例子，我们需要部署一个MySQL服务，Kubernetes则需要部署以下对象：

```
① 为了能够让外界访问到MySQL，需要部署一个mysql的service；

②需要进行定义MySQL的密码，则需要部署一个Secret；

③Mysql的运行需要持久化的数据存储，此时还需要部署PVC；

④保证后端mysql的运行，还需要部署一个Deployment，以支持以上的对象。
```

针对以上对象，我们可以使用YAML文件进行定义并部署，但是仅仅对于单个的服务支持，如果应用需要由一个甚至几十个这样的服务组成，并且还需要考虑各种服务的依赖问题，可想而知，这样的组织管理应用的方式就显得繁琐。为此就诞生了一个工具Helm，就是为了解决Kubernetes这种应用部署繁重的现

### 3.3.3：基础概念

 Helm：客户端，主要负责管理本地的 Charts、repositories 以及与tiller服务器交互，发送Chart，实例安装、查询、卸载等操作；

  Tiller：安装在 k8s 集群中的服务端，是实际用来管理安装在 k8s 中应用的，就是将模板与 values 合并，当然实际开发过程中， 也可以安装在 k8s 集群之外 ；接收helm发来的Charts与Config，合并生产relase;

  Chart：是用来管理模板与默认 values 的项目，也可以认为是一个 package，可以发布到专门的 repository；

   Repository: Chart仓库，https/http服务器；

  Release：特定的Chart部署于目标集群上的一个实例；

  Release：使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。

- Helm 工作原理

 

这张图描述了 Helm 的几个关键组件 Helm（客户端）、Tiller（服务器）、Repository（Chart 软件仓库）、Chart（软件包）之间的关系。



![image-20220701104944239](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220701104944239.png)

- Chart Install 过程

 

  Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。

  Helm 将指定的 Chart 结构和 Values 信息通过 gRPC 传递给 Tiller。

  Tiller 根据 Chart 和 Values 生成一个 Release。

  Tiller 将 Release 发送给 Kubernetes 用于生成 Release。

- Chart Update 过程

 

  Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。

  Helm 将需要更新的 Release 的名称、Chart 结构和 Values 信息传递给 Tiller。

  Tiller 生成 Release 并更新指定名称的 Release 的 History。

  Tiller 将 Release 发送给 Kubernetes 用于更新 Release。

- Chart Rollback 过程

 

  Helm 将要回滚的 Release 的名称传递给 Tiller。

  Tiller 根据 Release 的名称查找 History。

  Tiller 从 History 中获取上一个 Release。

  Tiller 将上一个 Release 发送给 Kubernetes 用于替换当前 Release。

 

* Chart 处理依赖说明

#### 

Tiller 在处理 Chart 时，直接将 Chart 以及其依赖的所有 Charts 合并为一个 Release，同时传递给 Kubernetes。因此 Tiller 并不负责管理依赖之间的启动顺序。Chart 中的应用需要能够自行处理依赖关系。

### 3.3.4：部署Helm

官方文档：https://helm.sh/docs/intro/

此案例helm版本为3.7.0:https://github.com/helm/helm/releases/tag/v3.7.0

Helm的部署方式有两种：预编译的二进制程序和源码编译安装，这里使用二进制的方式进行安装

#### 3.3.4.1：版本区别helm v2 版本

helm v2 版本

包含两个组件，分别是 helm 客户端 和 Tiller 服务器，
helm 是一个命令行工具，用于本地开发及管理chart，chart仓库管理等
Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互

helm v3 版本
移除了Tiller helm直接和K8s交互
SA通过 kuberconfig 配置认证

设计原理

它是一个线程的方式

#### 3.3.4.2：下载helm

官网：https://helm.sh/zh/docs/intro/install/

```bash
root@s3:~/helm# wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz
root@s3:~/helm# ls
helm-v3.7.0-linux-amd64.tar.gz
root@s3:~/helm# tar -xf helm-v3.7.0-linux-amd64.tar.gz 
root@s3:~/helm# ls
helm-v3.7.0-linux-amd64.tar.gz  linux-amd64
root@s3:~/helm# cd linux-amd64/
root@s3:~/helm/linux-amd64# ls
helm  LICENSE  README.md
root@s3:~/helm/linux-amd64# mv helm /usr/bin/
root@s3:~/helm/linux-amd64# helm version
version.BuildInfo{Version:"v3.7.0", GitCommit:"eeac83883cb4014fe60267ec6373570374ce770b", GitTreeState:"clean", GoVersion:"go1.16.8"}

#设置环境变量kubeconfig来指定存有ApiServer的地址与token的配置文件地址，默认为~/.kube/config
root@s3:~/helm/linux-amd64# export KUBECONFIG=/root/.kube/config
#可知本地安装的helm版本为3.7.0，而init命令为helm2的命令，在Helm 3已经弃用，可使用helm env查看环境配置信息。另外，Helm 3 中，Tiller被移除了。
#添加仓库
root@s3:~/helm/linux-amd64# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
#更新仓库
root@s3:~/helm/linux-amd64# helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈

```



#### 3.3.4.3：helm的使用

官方可用的Chart列表： https://hub.kubeapps.com

```bash
helm常用命令：
- helm search:    搜索charts
- helm install:   安装charts
- helm list:      列出charts的所有版本
- helm pull：     下载图表到本地目录查看

用法:
  helm [command]

命令可用选项:
  completion  为指定的shell生成自动补全脚本（bash或zsh）
  create      创建一个新的charts
  delete      删除指定版本的release
  dependency  管理charts的依赖
  env helm    客户端环境信息 
  get         下载一个release
  help        关于任何命令的帮助
  history     release历史信息
  install     安装charts
  lint        检测包的存在问题
  list        列出release
  package     将chart目录进行打包
  plugin      add(增加), list（列出）, or remove（移除） Helm 插件
  repo        add(增加), list（列出）, remove（移除）, update（更新）, and index（索引） chart仓库
  show         显示图表的显示信息
  rollback    release版本回滚
  search      关键字搜索chart
  uninstall   卸载卸载版本
  status      查看release状态信息
  template    本地模板
  test        release测试
  upgrade     release更新
  verify      验证chart的签名和有效期
  version     打印客户端和服务端的版本信息
```



前面说过，Helm 可以像 yum 管理软件包一样管理 chart。 yum 的软件包存放在仓库中，同样的，Helm 也有仓库。

```
root@s3:~# helm repo list
NAME  	URL                                                   
stable	https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
```

与yum一样 ,helm也支持关键字搜索

```
root@s3:~# helm search repo mysql
NAME                         	CHART VERSION	APP VERSION	DESCRIPTION                                       
stable/mysql                 	0.3.5        	           	Fast, reliable, scalable, and easy to use open-...
stable/percona               	0.3.0        	           	free, fully compatible, enhanced, open source d...
stable/percona-xtradb-cluster	0.0.2        	5.7.19     	free, fully compatible, enhanced, open source d...
stable/gcloud-sqlproxy       	0.2.3        	           	Google Cloud SQL Proxy                            
stable/mariadb               	2.1.6        	10.1.31    	Fast, reliable, scalable, and easy to use open-...

```

安装chart 也很简单，执行如下命令可以安装MySQL

```bash
root@s3:~# helm install repo apphub/mysql
NAME: repo
LAST DEPLOYED: Fri Jul  1 14:55:17 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Please be patient while the chart is being deployed

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace default

Services:

  echo Master: repo-mysql.default.svc.cluster.local:3306
  echo Slave:  repo-mysql-slave.default.svc.cluster.local:3306

Administrator credentials:

  echo Username: root
  echo Password : $(kubectl get secret --namespace default repo-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run repo-mysql-client --rm --tty -i --restart='Never' --image  docker.io/bitnami/mysql:8.0.19-debian-10-r0 --namespace default --command -- bash

  2. To connect to master service (read/write):

      mysql -h repo-mysql.default.svc.cluster.local -uroot -p my_database

  3. To connect to slave service (read-only):

      mysql -h repo-mysql-slave.default.svc.cluster.local -uroot -p my_database

To upgrade this helm chart:

  1. Obtain the password as described on the 'Administrator credentials' section and set the 'root.password' parameter as shown below:

      ROOT_PASSWORD=$(kubectl get secret --namespace default repo-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode)
      helm upgrade repo bitnami/mysql --set root.password=$ROOT_PASSWORD

```

**出分为三部分：**

① chart 本次部署的描述信息：

NAME 是 release 的名字，repo 是起的名字

NAMESPACE 是 release 部署的 namespace，默认是 default，也可以通过 –namespace 指定。

STATUS 为 DEPLOYED，表示已经将 chart 部署到集群。



② 当前 release 包含的资源： Service、Secret 和 PersistentVolumeClaim，其名字都是 repo-mysql

③ NOTES 部分显示的是 release 的使用方法。比如如何访问 Service，如何获取数据库密码，以及如何连接数据库等。

**通过 `kubectl get` 可以查看组成 release 的各个对象：**

```bash
root@s3:~# kubectl get svc repo-mysql
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
repo-mysql   ClusterIP   10.100.11.178   <none>        3306/TCP   23m
root@s3:~# kubectl get svc repo-mysql-slave
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
repo-mysql-slave   ClusterIP   10.100.159.184   <none>        3306/TCP   23m
root@s3:~# kubectl get secret repo-mysql
NAME         TYPE     DATA   AGE
repo-mysql   Opaque   2      27m
root@s3:~# kubectl get pvc
NAME                       STATUS    VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-repo-mysql-master-0   Pending                                                                     27m
data-repo-mysql-slave-0    Pending                                                                     27m

```

由于我们还没有准备 PersistentVolume，当前 release 还不可用。

helm list 显示已经部署的 release，helm delete 可以删除 release。

```bash
root@s3:~# helm list
NAME     	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART      	APP VERSION
nginx-pod	default  	1       	2022-07-01 15:09:25.294742089 +0800 CST	deployed	nginx-5.1.5	1.16.1     
repo     	default  	1       	2022-07-01 14:55:17.635141453 +0800 CST	deployed	mysql-6.8.0	8.0.19    

root@s3:~# helm delete repo
release "repo" uninstalled
```

#### 3.3.4.4：chart目录结构

chart 是 Helm 的应用打包格式。chart 由一系列文件组成，这些文件描述了 Kubernetes 部署应用时所需要的资源，比如 Service、Deployment、PersistentVolumeClaim、Secret、ConfigMap 等。

单个的 chart 可以非常简单，只用于部署一个服务，比如 Memcached；chart 也可以很复杂，部署整个应用，比如包含 HTTP Servers、 Database、消息中间件、cache 等。

chart 将这些文件放置在预定义的目录结构中，通常整个 chart 被打成 tar 包，而且标注上版本信息，便于 Helm 部署。

以前面 MySQL chart 为例。一旦安装了某个 chart，我们就可以在 ~/.helm/cache/archive 中找到 chart 的 tar 包。

```bash
root@s3:~# cd .cache/helm/repository/
root@s3:~/.cache/helm/repository# ll
total 1732
drwxr-xr-x 3 root root    4096 7月   1 15:36 ./
drwxr-xr-x 3 root root    4096 7月   1 13:58 ../
-rw-r--r-- 1 root root    3902 7月   1 14:54 apphub-charts.txt
-rw-r--r-- 1 root root 1715990 7月   1 14:54 apphub-index.yaml
-rw-r--r-- 1 root root   17860 7月   1 14:55 mysql-6.8.0.tgz
-rw-r--r-- 1 root root    9552 7月   1 15:09 nginx-5.1.5.tgz
root@s3:~/.cache/helm/repository# tar xf mysql-6.8.0.tgz
root@s3:~/.cache/helm/repository# tree mysql
mysql
├── Chart.yaml
├── ci
│   └── values-production.yaml
├── files
│   └── docker-entrypoint-initdb.d
│       └── README.md
├── README.md
├── templates
│   ├── _helpers.tpl
│   ├── initialization-configmap.yaml
│   ├── master-configmap.yaml
│   ├── master-statefulset.yaml
│   ├── master-svc.yaml
│   ├── NOTES.txt
│   ├── secrets.yaml
│   ├── servicemonitor.yaml
│   ├── slave-configmap.yaml
│   ├── slave-statefulset.yaml
│   └── slave-svc.yaml
├── values-production.yaml
└── values.yaml

4 directories, 17 files

```

- Chart.yaml：YAML 文件，描述 chart 的概要信息。
- README.md：Markdown 格式的 README 文件，相当于 chart 的使用文档，此文件为可选。
- LICENSE：文本文件，描述 chart 的许可信息，此文件为可选。
- requirements.yaml ：chart 可能依赖其他的 chart，这些依赖关系可通过 requirements.yaml 指定。
- values.yaml：chart 支持在安装的时根据参数进行定制化配置，而 values.yaml 则提供了这些配置参数的默认值。
- templates目录：各类 Kubernetes 资源的配置模板都放置在这里。Helm 会将 values.yaml 中的参数值注入到模板中生成标准的 YAML 配置文件。
- templates/NOTES.txt：chart 的简易使用文档，chart 安装成功后会显示此文档内容。 与模板一样，可以在 NOTE.txt 中插入配置参数，Helm 会动态注入参数值。



# 四: kubernetes-日志收集

## 4.1: 日志收集简介：

日志收集的目的：

* 分布式日志数据统一收集，实现计中查询和管理
* 故障排查
* 安全信息和事件管理
* 报表统计及展示功能

日志收集的价值：

* 日志查询，问题排查，故障恢复，故障自愈
* 应用日志分析，错误报警
* 性能分析，用户行为分析

## 4.2：日志收集流程：



![image-20230201125329812](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230201125329812.png)



## 4.3: elasticsearch安装部署:

### 4.3.1：环境准备：

```bash
192.168.48.164 s5
192.168.48.165 s6
两台服务器同时安装
root@s4:/apps# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.16.0-amd64.deb
root@s5:/apps# dpkg -i elasticsearch-7.16.0-amd64.deb
s5
root@s5:/apps# grep -v '#' /etc/elasticsearch/elasticsearch.yml | grep -v "^$"
cluster.name: cluster-elk
node.name: node1
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 192.168.48.163
http.port: 9200
discovery.seed_hosts: ["192.168.48.164","192.168.48.165"]
cluster.initial_master_nodes: ["192.168.48.164","192.168.48.165"]
action.destructive_requires_name: true

s6
root@harbor:/usr/local/src# grep -v '#' /etc/elasticsearch/elasticsearch.yml |grep -v '^$'
cluster.name: cluster-elk
node.name: node2
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 192.168.48.164
http.port: 9200
discovery.seed_hosts: ["192.168.48.164","192.168.48.165"]
cluster.initial_master_nodes: ["192.168.48.164","192.168.48.165"]
action.destructive_requires_name: true
root@s4:/apps#systemctl start elasticsearch.service
```



![image-20230201142428334](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230201142428334.png)

#### 4.3.1.1: centos镜像制作:

```bash
# cat Dockerfile 
#自定义Centos 基础镜像
FROM centos:7.9.2009 


ADD filebeat-7.12.1-x86_64.rpm /tmp
RUN yum install -y /tmp/filebeat-7.12.1-x86_64.rpm vim wget tree  lrzsz gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel openssl openssl-devel iproute net-tools iotop &&  rm -rf /etc/localtime /tmp/filebeat-7.12.1-x86_64.rpm && ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  && useradd nginx -u 2022

# cat build-command.sh 
#!/bin/bash
docker build -t  harbor.magedu.net/base/magedu-centos-base:7.9.2009 .

docker push harbor.magedu.net/base/magedu-centos-base:7.9.2009

# bash build-command.sh
```



#### 4.3.1.2: JDK镜像制作：

```bash
# cat Dockerfile 
#JDK Base Image
FROM harbor.magedu.net/base/magedu-centos-base:7.9.2009 

ADD jdk-8u212-linux-x64.tar.gz /usr/local/src/
RUN ln -sv /usr/local/src/jdk1.8.0_212 /usr/local/jdk 
ADD profile /etc/profile

ENV JAVA_HOME /usr/local/jdk
ENV JRE_HOME $JAVA_HOME/jre
ENV CLASSPATH $JAVA_HOME/lib/:$JRE_HOME/lib/
ENV PATH $PATH:$JAVA_HOME/bin


# cat profile 
# /etc/profile

# System wide environment and startup programs, for login setup
# Functions and aliases go in /etc/bashrc

# It's NOT a good idea to change this file unless you know what you
# are doing. It's much better to create a custom.sh shell script in
# /etc/profile.d/ to make custom changes to your environment, as this
# will prevent the need for merging in future updates.

pathmunge () {
    case ":${PATH}:" in
        *:"$1":*)
            ;;
        *)
            if [ "$2" = "after" ] ; then
                PATH=$PATH:$1
            else
                PATH=$1:$PATH
            fi
    esac
}


if [ -x /usr/bin/id ]; then
    if [ -z "$EUID" ]; then
        # ksh workaround
        EUID=`/usr/bin/id -u`
        UID=`/usr/bin/id -ru`
    fi
    USER="`/usr/bin/id -un`"
    LOGNAME=$USER
    MAIL="/var/spool/mail/$USER"
fi

# Path manipulation
if [ "$EUID" = "0" ]; then
    pathmunge /usr/sbin
    pathmunge /usr/local/sbin
else
    pathmunge /usr/local/sbin after
    pathmunge /usr/sbin after
fi

HOSTNAME=`/usr/bin/hostname 2>/dev/null`
HISTSIZE=1000
if [ "$HISTCONTROL" = "ignorespace" ] ; then
    export HISTCONTROL=ignoreboth
else
    export HISTCONTROL=ignoredups
fi

export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL

# By default, we want umask to get set. This sets it for login shell
# Current threshold for system reserved uid/gids is 200
# You could check uidgid reservation validity in
# /usr/share/doc/setup-*/uidgid file
if [ $UID -gt 199 ] && [ "`/usr/bin/id -gn`" = "`/usr/bin/id -un`" ]; then
    umask 002
else
    umask 022
fi

for i in /etc/profile.d/*.sh /etc/profile.d/sh.local ; do
    if [ -r "$i" ]; then
        if [ "${-#*i}" != "$-" ]; then 
            . "$i"
        else
            . "$i" >/dev/null
        fi
    fi
done

unset i
unset -f pathmunge
export LANG=en_US.UTF-8
export HISTTIMEFORMAT="%F %T `whoami` "

export JAVA_HOME=/usr/local/jdk
export TOMCAT_HOME=/apps/tomcat
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$TOMCAT_HOME/bin:$PATH
export CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar


# cat build-command.sh 
#!/bin/bash
docker build -t harbor.magedu.net/bash/jdk-base:v8.212  .
sleep 1
docker push  harbor.magedu.net/bash/jdk-base:v8.212

```

#### 4.3.1.3: tomcat镜像制作：

```bash
# cat Dockerfile 
#Tomcat 8.5.43基础镜像
FROM harbor.magedu.net/base/jdk-base:v8.212

RUN mkdir /apps /data/tomcat/webapps /data/tomcat/logs -pv 
ADD apache-tomcat-8.5.43.tar.gz  /apps
RUN useradd tomcat -u 2050 && ln -sv /apps/apache-tomcat-8.5.43 /apps/tomcat && chown -R tomcat.tomcat /apps /data -R


# cat build-command.sh 
#!/bin/bash
docker build -t harbor.magedu.net/base/tomcat-base:v8.5.43  .
sleep 3
docker push  harbor.magedu.net/base/tomcat-base:v8.5.43


```



#### 4.3.1.4: tomcat-app1镜像业务制作：

```bash
 cat Dockerfile 
#tomcat web1
FROM harbor.magedu.net/base/tomcat-base:v8.5.43 

ADD catalina.sh /apps/tomcat/bin/catalina.sh
ADD server.xml /apps/tomcat/conf/server.xml
#ADD myapp/* /data/tomcat/webapps/myapp/
ADD app1.tar.gz /data/tomcat/webapps/myapp/
ADD run_tomcat.sh /apps/tomcat/bin/run_tomcat.sh
#ADD filebeat.yml /etc/filebeat/filebeat.yml 
RUN chown  -R nginx.nginx /data/ /apps/
#ADD filebeat-7.5.1-x86_64.rpm /tmp/
#RUN cd /tmp && yum localinstall -y filebeat-7.5.1-amd64.deb

EXPOSE 8080 8443

CMD ["/apps/tomcat/bin/run_tomcat.sh"]



# cat catalina.sh 
#!/bin/sh

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# -----------------------------------------------------------------------------
# Control Script for the CATALINA Server
#
# Environment Variable Prerequisites
#
#   Do not set the variables in this script. Instead put them into a script
#   setenv.sh in CATALINA_BASE/bin to keep your customizations separate.
#
#   CATALINA_HOME   May point at your Catalina "build" directory.
#
#   CATALINA_BASE   (Optional) Base directory for resolving dynamic portions
#                   of a Catalina installation.  If not present, resolves to
#                   the same directory that CATALINA_HOME points to.
#
#   CATALINA_OUT    (Optional) Full path to a file where stdout and stderr
#                   will be redirected.
#                   Default is $CATALINA_BASE/logs/catalina.out
#
#   CATALINA_OPTS   (Optional) Java runtime options used when the "start",
#                   "run" or "debug" command is executed.
#                   Include here and not in JAVA_OPTS all options, that should
#                   only be used by Tomcat itself, not by the stop process,
#                   the version command etc.
#                   Examples are heap size, GC logging, JMX ports etc.
#
#   CATALINA_TMPDIR (Optional) Directory path location of temporary directory
#                   the JVM should use (java.io.tmpdir).  Defaults to
#                   $CATALINA_BASE/temp.
#
#   JAVA_HOME       Must point at your Java Development Kit installation.
#                   Required to run the with the "debug" argument.
#
#   JRE_HOME        Must point at your Java Runtime installation.
#                   Defaults to JAVA_HOME if empty. If JRE_HOME and JAVA_HOME
#                   are both set, JRE_HOME is used.
#
#   JAVA_OPTS       (Optional) Java runtime options used when any command
#                   is executed.
#                   Include here and not in CATALINA_OPTS all options, that
#                   should be used by Tomcat and also by the stop process,
#                   the version command etc.
#                   Most options should go into CATALINA_OPTS.
#
#   JAVA_ENDORSED_DIRS (Optional) Lists of of colon separated directories
#                   containing some jars in order to allow replacement of APIs
#                   created outside of the JCP (i.e. DOM and SAX from W3C).
#                   It can also be used to update the XML parser implementation.
#                   Note that Java 9 no longer supports this feature.
#                   Defaults to $CATALINA_HOME/endorsed.
#
#   JPDA_TRANSPORT  (Optional) JPDA transport used when the "jpda start"
#                   command is executed. The default is "dt_socket".
#
#   JPDA_ADDRESS    (Optional) Java runtime options used when the "jpda start"
#                   command is executed. The default is localhost:8000.
#
#   JPDA_SUSPEND    (Optional) Java runtime options used when the "jpda start"
#                   command is executed. Specifies whether JVM should suspend
#                   execution immediately after startup. Default is "n".
#
#   JPDA_OPTS       (Optional) Java runtime options used when the "jpda start"
#                   command is executed. If used, JPDA_TRANSPORT, JPDA_ADDRESS,
#                   and JPDA_SUSPEND are ignored. Thus, all required jpda
#                   options MUST be specified. The default is:
#
#                   -agentlib:jdwp=transport=$JPDA_TRANSPORT,
#                       address=$JPDA_ADDRESS,server=y,suspend=$JPDA_SUSPEND
#
#   JSSE_OPTS       (Optional) Java runtime options used to control the TLS
#                   implementation when JSSE is used. Default is:
#                   "-Djdk.tls.ephemeralDHKeySize=2048"
#
#   CATALINA_PID    (Optional) Path of the file which should contains the pid
#                   of the catalina startup java process, when start (fork) is
#                   used
#
#   LOGGING_CONFIG  (Optional) Override Tomcat's logging config file
#                   Example (all one line)
#                   LOGGING_CONFIG="-Djava.util.logging.config.file=$CATALINA_BASE/conf/logging.properties"
#
#   LOGGING_MANAGER (Optional) Override Tomcat's logging manager
#                   Example (all one line)
#                   LOGGING_MANAGER="-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager"
#
#   USE_NOHUP       (Optional) If set to the string true the start command will
#                   use nohup so that the Tomcat process will ignore any hangup
#                   signals. Default is "false" unless running on HP-UX in which
#                   case the default is "true"
# -----------------------------------------------------------------------------

JAVA_OPTS="-server -Xms1g -Xmx1g -Xss512k -Xmn1g -XX:CMSInitiatingOccupancyFraction=65  -XX:+UseFastAccessorMethods -XX:+AggressiveOpts -XX:+UseBiasedLocking -XX:+DisableExplicitGC -XX:MaxTenuringThreshold=10 -XX:NewSize=2048M -XX:MaxNewSize=2048M -XX:NewRatio=2 -XX:PermSize=128m -XX:MaxPermSize=512m -XX:CMSFullGCsBeforeCompaction=5 -XX:+ExplicitGCInvokesConcurrent -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled"

# OS specific support.  $var _must_ be set to either true or false.
cygwin=false
darwin=false
os400=false
hpux=false
case "`uname`" in
CYGWIN*) cygwin=true;;
Darwin*) darwin=true;;
OS400*) os400=true;;
HP-UX*) hpux=true;;
esac

# resolve links - $0 may be a softlink
PRG="$0"

while [ -h "$PRG" ]; do
  ls=`ls -ld "$PRG"`
  link=`expr "$ls" : '.*-> \(.*\)$'`
  if expr "$link" : '/.*' > /dev/null; then
    PRG="$link"
  else
    PRG=`dirname "$PRG"`/"$link"
  fi
done

# Get standard environment variables
PRGDIR=`dirname "$PRG"`

# Only set CATALINA_HOME if not already set
[ -z "$CATALINA_HOME" ] && CATALINA_HOME=`cd "$PRGDIR/.." >/dev/null; pwd`

# Copy CATALINA_BASE from CATALINA_HOME if not already set
[ -z "$CATALINA_BASE" ] && CATALINA_BASE="$CATALINA_HOME"

# Ensure that any user defined CLASSPATH variables are not used on startup,
# but allow them to be specified in setenv.sh, in rare case when it is needed.
CLASSPATH=

if [ -r "$CATALINA_BASE/bin/setenv.sh" ]; then
  . "$CATALINA_BASE/bin/setenv.sh"
elif [ -r "$CATALINA_HOME/bin/setenv.sh" ]; then
  . "$CATALINA_HOME/bin/setenv.sh"
fi

# For Cygwin, ensure paths are in UNIX format before anything is touched
if $cygwin; then
  [ -n "$JAVA_HOME" ] && JAVA_HOME=`cygpath --unix "$JAVA_HOME"`
  [ -n "$JRE_HOME" ] && JRE_HOME=`cygpath --unix "$JRE_HOME"`
  [ -n "$CATALINA_HOME" ] && CATALINA_HOME=`cygpath --unix "$CATALINA_HOME"`
  [ -n "$CATALINA_BASE" ] && CATALINA_BASE=`cygpath --unix "$CATALINA_BASE"`
  [ -n "$CLASSPATH" ] && CLASSPATH=`cygpath --path --unix "$CLASSPATH"`
fi

# Ensure that neither CATALINA_HOME nor CATALINA_BASE contains a colon
# as this is used as the separator in the classpath and Java provides no
# mechanism for escaping if the same character appears in the path.
case $CATALINA_HOME in
  *:*) echo "Using CATALINA_HOME:   $CATALINA_HOME";
       echo "Unable to start as CATALINA_HOME contains a colon (:) character";
       exit 1;
esac
case $CATALINA_BASE in
  *:*) echo "Using CATALINA_BASE:   $CATALINA_BASE";
       echo "Unable to start as CATALINA_BASE contains a colon (:) character";
       exit 1;
esac

# For OS400
if $os400; then
  # Set job priority to standard for interactive (interactive - 6) by using
  # the interactive priority - 6, the helper threads that respond to requests
  # will be running at the same priority as interactive jobs.
  COMMAND='chgjob job('$JOBNAME') runpty(6)'
  system $COMMAND

  # Enable multi threading
  export QIBM_MULTI_THREADED=Y
fi

# Get standard Java environment variables
if $os400; then
  # -r will Only work on the os400 if the files are:
  # 1. owned by the user
  # 2. owned by the PRIMARY group of the user
  # this will not work if the user belongs in secondary groups
  . "$CATALINA_HOME"/bin/setclasspath.sh
else
  if [ -r "$CATALINA_HOME"/bin/setclasspath.sh ]; then
    . "$CATALINA_HOME"/bin/setclasspath.sh
  else
    echo "Cannot find $CATALINA_HOME/bin/setclasspath.sh"
    echo "This file is needed to run this program"
    exit 1
  fi
fi

# Add on extra jar files to CLASSPATH
if [ ! -z "$CLASSPATH" ] ; then
  CLASSPATH="$CLASSPATH":
fi
CLASSPATH="$CLASSPATH""$CATALINA_HOME"/bin/bootstrap.jar

if [ -z "$CATALINA_OUT" ] ; then
  CATALINA_OUT="$CATALINA_BASE"/logs/catalina.out
fi

if [ -z "$CATALINA_TMPDIR" ] ; then
  # Define the java.io.tmpdir to use for Catalina
  CATALINA_TMPDIR="$CATALINA_BASE"/temp
fi

# Add tomcat-juli.jar to classpath
# tomcat-juli.jar can be over-ridden per instance
if [ -r "$CATALINA_BASE/bin/tomcat-juli.jar" ] ; then
  CLASSPATH=$CLASSPATH:$CATALINA_BASE/bin/tomcat-juli.jar
else
  CLASSPATH=$CLASSPATH:$CATALINA_HOME/bin/tomcat-juli.jar
fi

# Bugzilla 37848: When no TTY is available, don't output to console
have_tty=0
if [ "`tty`" != "not a tty" ]; then
    have_tty=1
fi

# For Cygwin, switch paths to Windows format before running java
if $cygwin; then
  JAVA_HOME=`cygpath --absolute --windows "$JAVA_HOME"`
  JRE_HOME=`cygpath --absolute --windows "$JRE_HOME"`
  CATALINA_HOME=`cygpath --absolute --windows "$CATALINA_HOME"`
  CATALINA_BASE=`cygpath --absolute --windows "$CATALINA_BASE"`
  CATALINA_TMPDIR=`cygpath --absolute --windows "$CATALINA_TMPDIR"`
  CLASSPATH=`cygpath --path --windows "$CLASSPATH"`
  JAVA_ENDORSED_DIRS=`cygpath --path --windows "$JAVA_ENDORSED_DIRS"`
fi

if [ -z "$JSSE_OPTS" ] ; then
  JSSE_OPTS="-Djdk.tls.ephemeralDHKeySize=2048"
fi
JAVA_OPTS="$JAVA_OPTS $JSSE_OPTS"

# Register custom URL handlers
# Do this here so custom URL handles (specifically 'war:...') can be used in the security policy
JAVA_OPTS="$JAVA_OPTS -Djava.protocol.handler.pkgs=org.apache.catalina.webresources"

# Set juli LogManager config file if it is present and an override has not been issued
if [ -z "$LOGGING_CONFIG" ]; then
  if [ -r "$CATALINA_BASE"/conf/logging.properties ]; then
    LOGGING_CONFIG="-Djava.util.logging.config.file=$CATALINA_BASE/conf/logging.properties"
  else
    # Bugzilla 45585
    LOGGING_CONFIG="-Dnop"
  fi
fi

if [ -z "$LOGGING_MANAGER" ]; then
  LOGGING_MANAGER="-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager"
fi

# Java 9 no longer supports the java.endorsed.dirs
# system property. Only try to use it if
# JAVA_ENDORSED_DIRS was explicitly set
# or CATALINA_HOME/endorsed exists.
ENDORSED_PROP=ignore.endorsed.dirs
if [ -n "$JAVA_ENDORSED_DIRS" ]; then
    ENDORSED_PROP=java.endorsed.dirs
fi
if [ -d "$CATALINA_HOME/endorsed" ]; then
    ENDORSED_PROP=java.endorsed.dirs
fi

# Uncomment the following line to make the umask available when using the
# org.apache.catalina.security.SecurityListener
#JAVA_OPTS="$JAVA_OPTS -Dorg.apache.catalina.security.SecurityListener.UMASK=`umask`"

if [ -z "$USE_NOHUP" ]; then
    if $hpux; then
        USE_NOHUP="true"
    else
        USE_NOHUP="false"
    fi
fi
unset _NOHUP
if [ "$USE_NOHUP" = "true" ]; then
    _NOHUP=nohup
fi

# Add the JAVA 9 specific start-up parameters required by Tomcat
JDK_JAVA_OPTIONS="$JDK_JAVA_OPTIONS --add-opens=java.base/java.lang=ALL-UNNAMED"
JDK_JAVA_OPTIONS="$JDK_JAVA_OPTIONS --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED"
export JDK_JAVA_OPTIONS

# ----- Execute The Requested Command -----------------------------------------

# Bugzilla 37848: only output this if we have a TTY
if [ $have_tty -eq 1 ]; then
  echo "Using CATALINA_BASE:   $CATALINA_BASE"
  echo "Using CATALINA_HOME:   $CATALINA_HOME"
  echo "Using CATALINA_TMPDIR: $CATALINA_TMPDIR"
  if [ "$1" = "debug" ] ; then
    echo "Using JAVA_HOME:       $JAVA_HOME"
  else
    echo "Using JRE_HOME:        $JRE_HOME"
  fi
  echo "Using CLASSPATH:       $CLASSPATH"
  if [ ! -z "$CATALINA_PID" ]; then
    echo "Using CATALINA_PID:    $CATALINA_PID"
  fi
fi

if [ "$1" = "jpda" ] ; then
  if [ -z "$JPDA_TRANSPORT" ]; then
    JPDA_TRANSPORT="dt_socket"
  fi
  if [ -z "$JPDA_ADDRESS" ]; then
    JPDA_ADDRESS="localhost:8000"
  fi
  if [ -z "$JPDA_SUSPEND" ]; then
    JPDA_SUSPEND="n"
  fi
  if [ -z "$JPDA_OPTS" ]; then
    JPDA_OPTS="-agentlib:jdwp=transport=$JPDA_TRANSPORT,address=$JPDA_ADDRESS,server=y,suspend=$JPDA_SUSPEND"
  fi
  CATALINA_OPTS="$JPDA_OPTS $CATALINA_OPTS"
  shift
fi

if [ "$1" = "debug" ] ; then
  if $os400; then
    echo "Debug command not available on OS400"
    exit 1
  else
    shift
    if [ "$1" = "-security" ] ; then
      if [ $have_tty -eq 1 ]; then
        echo "Using Security Manager"
      fi
      shift
      exec "$_RUNJDB" "$LOGGING_CONFIG" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
        -D$ENDORSED_PROP="$JAVA_ENDORSED_DIRS" \
        -classpath "$CLASSPATH" \
        -sourcepath "$CATALINA_HOME"/../../java \
        -Djava.security.manager \
        -Djava.security.policy=="$CATALINA_BASE"/conf/catalina.policy \
        -Dcatalina.base="$CATALINA_BASE" \
        -Dcatalina.home="$CATALINA_HOME" \
        -Djava.io.tmpdir="$CATALINA_TMPDIR" \
        org.apache.catalina.startup.Bootstrap "$@" start
    else
      exec "$_RUNJDB" "$LOGGING_CONFIG" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
        -D$ENDORSED_PROP="$JAVA_ENDORSED_DIRS" \
        -classpath "$CLASSPATH" \
        -sourcepath "$CATALINA_HOME"/../../java \
        -Dcatalina.base="$CATALINA_BASE" \
        -Dcatalina.home="$CATALINA_HOME" \
        -Djava.io.tmpdir="$CATALINA_TMPDIR" \
        org.apache.catalina.startup.Bootstrap "$@" start
    fi
  fi

elif [ "$1" = "run" ]; then

  shift
  if [ "$1" = "-security" ] ; then
    if [ $have_tty -eq 1 ]; then
      echo "Using Security Manager"
    fi
    shift
    eval exec "\"$_RUNJAVA\"" "\"$LOGGING_CONFIG\"" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
      -classpath "\"$CLASSPATH\"" \
      -Djava.security.manager \
      -Djava.security.policy=="\"$CATALINA_BASE/conf/catalina.policy\"" \
      -Dcatalina.base="\"$CATALINA_BASE\"" \
      -Dcatalina.home="\"$CATALINA_HOME\"" \
      -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
      org.apache.catalina.startup.Bootstrap "$@" start
  else
    eval exec "\"$_RUNJAVA\"" "\"$LOGGING_CONFIG\"" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
      -classpath "\"$CLASSPATH\"" \
      -Dcatalina.base="\"$CATALINA_BASE\"" \
      -Dcatalina.home="\"$CATALINA_HOME\"" \
      -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
      org.apache.catalina.startup.Bootstrap "$@" start
  fi

elif [ "$1" = "start" ] ; then

  if [ ! -z "$CATALINA_PID" ]; then
    if [ -f "$CATALINA_PID" ]; then
      if [ -s "$CATALINA_PID" ]; then
        echo "Existing PID file found during start."
        if [ -r "$CATALINA_PID" ]; then
          PID=`cat "$CATALINA_PID"`
          ps -p $PID >/dev/null 2>&1
          if [ $? -eq 0 ] ; then
            echo "Tomcat appears to still be running with PID $PID. Start aborted."
            echo "If the following process is not a Tomcat process, remove the PID file and try again:"
            ps -f -p $PID
            exit 1
          else
            echo "Removing/clearing stale PID file."
            rm -f "$CATALINA_PID" >/dev/null 2>&1
            if [ $? != 0 ]; then
              if [ -w "$CATALINA_PID" ]; then
                cat /dev/null > "$CATALINA_PID"
              else
                echo "Unable to remove or clear stale PID file. Start aborted."
                exit 1
              fi
            fi
          fi
        else
          echo "Unable to read PID file. Start aborted."
          exit 1
        fi
      else
        rm -f "$CATALINA_PID" >/dev/null 2>&1
        if [ $? != 0 ]; then
          if [ ! -w "$CATALINA_PID" ]; then
            echo "Unable to remove or write to empty PID file. Start aborted."
            exit 1
          fi
        fi
      fi
    fi
  fi

  shift
  touch "$CATALINA_OUT"
  if [ "$1" = "-security" ] ; then
    if [ $have_tty -eq 1 ]; then
      echo "Using Security Manager"
    fi
    shift
    eval $_NOHUP "\"$_RUNJAVA\"" "\"$LOGGING_CONFIG\"" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
      -classpath "\"$CLASSPATH\"" \
      -Djava.security.manager \
      -Djava.security.policy=="\"$CATALINA_BASE/conf/catalina.policy\"" \
      -Dcatalina.base="\"$CATALINA_BASE\"" \
      -Dcatalina.home="\"$CATALINA_HOME\"" \
      -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
      org.apache.catalina.startup.Bootstrap "$@" start \
      >> "$CATALINA_OUT" 2>&1 "&"

  else
    eval $_NOHUP "\"$_RUNJAVA\"" "\"$LOGGING_CONFIG\"" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
      -classpath "\"$CLASSPATH\"" \
      -Dcatalina.base="\"$CATALINA_BASE\"" \
      -Dcatalina.home="\"$CATALINA_HOME\"" \
      -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
      org.apache.catalina.startup.Bootstrap "$@" start \
      >> "$CATALINA_OUT" 2>&1 "&"

  fi

  if [ ! -z "$CATALINA_PID" ]; then
    echo $! > "$CATALINA_PID"
  fi

  echo "Tomcat started."

elif [ "$1" = "stop" ] ; then

  shift

  SLEEP=5
  if [ ! -z "$1" ]; then
    echo $1 | grep "[^0-9]" >/dev/null 2>&1
    if [ $? -gt 0 ]; then
      SLEEP=$1
      shift
    fi
  fi

  FORCE=0
  if [ "$1" = "-force" ]; then
    shift
    FORCE=1
  fi

  if [ ! -z "$CATALINA_PID" ]; then
    if [ -f "$CATALINA_PID" ]; then
      if [ -s "$CATALINA_PID" ]; then
        kill -0 `cat "$CATALINA_PID"` >/dev/null 2>&1
        if [ $? -gt 0 ]; then
          echo "PID file found but no matching process was found. Stop aborted."
          exit 1
        fi
      else
        echo "PID file is empty and has been ignored."
      fi
    else
      echo "\$CATALINA_PID was set but the specified file does not exist. Is Tomcat running? Stop aborted."
      exit 1
    fi
  fi

  eval "\"$_RUNJAVA\"" $LOGGING_MANAGER $JAVA_OPTS \
    -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
    -classpath "\"$CLASSPATH\"" \
    -Dcatalina.base="\"$CATALINA_BASE\"" \
    -Dcatalina.home="\"$CATALINA_HOME\"" \
    -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
    org.apache.catalina.startup.Bootstrap "$@" stop

  # stop failed. Shutdown port disabled? Try a normal kill.
  if [ $? != 0 ]; then
    if [ ! -z "$CATALINA_PID" ]; then
      echo "The stop command failed. Attempting to signal the process to stop through OS signal."
      kill -15 `cat "$CATALINA_PID"` >/dev/null 2>&1
    fi
  fi

  if [ ! -z "$CATALINA_PID" ]; then
    if [ -f "$CATALINA_PID" ]; then
      while [ $SLEEP -ge 0 ]; do
        kill -0 `cat "$CATALINA_PID"` >/dev/null 2>&1
        if [ $? -gt 0 ]; then
          rm -f "$CATALINA_PID" >/dev/null 2>&1
          if [ $? != 0 ]; then
            if [ -w "$CATALINA_PID" ]; then
              cat /dev/null > "$CATALINA_PID"
              # If Tomcat has stopped don't try and force a stop with an empty PID file
              FORCE=0
            else
              echo "The PID file could not be removed or cleared."
            fi
          fi
          echo "Tomcat stopped."
          break
        fi
        if [ $SLEEP -gt 0 ]; then
          sleep 1
        fi
        if [ $SLEEP -eq 0 ]; then
          echo "Tomcat did not stop in time."
          if [ $FORCE -eq 0 ]; then
            echo "PID file was not removed."
          fi
          echo "To aid diagnostics a thread dump has been written to standard out."
          kill -3 `cat "$CATALINA_PID"`
        fi
        SLEEP=`expr $SLEEP - 1 `
      done
    fi
  fi

  KILL_SLEEP_INTERVAL=5
  if [ $FORCE -eq 1 ]; then
    if [ -z "$CATALINA_PID" ]; then
      echo "Kill failed: \$CATALINA_PID not set"
    else
      if [ -f "$CATALINA_PID" ]; then
        PID=`cat "$CATALINA_PID"`
        echo "Killing Tomcat with the PID: $PID"
        kill -9 $PID
        while [ $KILL_SLEEP_INTERVAL -ge 0 ]; do
            kill -0 `cat "$CATALINA_PID"` >/dev/null 2>&1
            if [ $? -gt 0 ]; then
                rm -f "$CATALINA_PID" >/dev/null 2>&1
                if [ $? != 0 ]; then
                    if [ -w "$CATALINA_PID" ]; then
                        cat /dev/null > "$CATALINA_PID"
                    else
                        echo "The PID file could not be removed."
                    fi
                fi
                echo "The Tomcat process has been killed."
                break
            fi
            if [ $KILL_SLEEP_INTERVAL -gt 0 ]; then
                sleep 1
            fi
            KILL_SLEEP_INTERVAL=`expr $KILL_SLEEP_INTERVAL - 1 `
        done
        if [ $KILL_SLEEP_INTERVAL -lt 0 ]; then
            echo "Tomcat has not been killed completely yet. The process might be waiting on some system call or might be UNINTERRUPTIBLE."
        fi
      fi
    fi
  fi

elif [ "$1" = "configtest" ] ; then

    eval "\"$_RUNJAVA\"" $LOGGING_MANAGER $JAVA_OPTS \
      -D$ENDORSED_PROP="\"$JAVA_ENDORSED_DIRS\"" \
      -classpath "\"$CLASSPATH\"" \
      -Dcatalina.base="\"$CATALINA_BASE\"" \
      -Dcatalina.home="\"$CATALINA_HOME\"" \
      -Djava.io.tmpdir="\"$CATALINA_TMPDIR\"" \
      org.apache.catalina.startup.Bootstrap configtest
    result=$?
    if [ $result -ne 0 ]; then
        echo "Configuration error detected!"
    fi
    exit $result

elif [ "$1" = "version" ] ; then

    "$_RUNJAVA"   \
      -classpath "$CATALINA_HOME/lib/catalina.jar" \
      org.apache.catalina.util.ServerInfo

else

  echo "Usage: catalina.sh ( commands ... )"
  echo "commands:"
  if $os400; then
    echo "  debug             Start Catalina in a debugger (not available on OS400)"
    echo "  debug -security   Debug Catalina with a security manager (not available on OS400)"
  else
    echo "  debug             Start Catalina in a debugger"
    echo "  debug -security   Debug Catalina with a security manager"
  fi
  echo "  jpda start        Start Catalina under JPDA debugger"
  echo "  run               Start Catalina in the current window"
  echo "  run -security     Start in the current window with security manager"
  echo "  start             Start Catalina in a separate window"
  echo "  start -security   Start in a separate window with security manager"
  echo "  stop              Stop Catalina, waiting up to 5 seconds for the process to end"
  echo "  stop n            Stop Catalina, waiting up to n seconds for the process to end"
  echo "  stop -force       Stop Catalina, wait up to 5 seconds and then use kill -KILL if still running"
  echo "  stop n -force     Stop Catalina, wait up to n seconds and then use kill -KILL if still running"
  echo "  configtest        Run a basic syntax check on server.xml - check exit code for result"
  echo "  version           What version of tomcat are you running?"
  echo "Note: Waiting for the process to end and use of the -force option require that \$CATALINA_PID is defined"
  exit 1

fi

# cat filebeat.yml 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /apps/tomcat/logs/catalina.out
  fields:
    type: tomcat-catalina
- type: log
  enabled: true
  paths:
    - /apps/tomcat/logs/localhost_access_log.*.txt 
  fields:
    type: tomcat-accesslog
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:

output.kafka:
  hosts: ["172.31.4.101:9092"]
  required_acks: 1
  topic: "magedu-n56-app1"
  compression: gzip
  max_message_bytes: 1000000
#output.redis:
#  hosts: ["172.31.2.105:6379"]
#  key: "k8s-magedu-app1"
#  db: 1
#  timeout: 5
#  password: "123456"


# cat run_tomcat.sh 
#!/bin/bash
#echo "nameserver 223.6.6.6" > /etc/resolv.conf
#echo "192.168.7.248 k8s-vip.example.com" >> /etc/hosts

#/usr/share/filebeat/bin/filebeat -e -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat &
su - nginx -c "/apps/tomcat/bin/catalina.sh start"
tail -f /etc/hosts

# cat server.xml 
<?xml version='1.0' encoding='utf-8'?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!-- Note:  A "Server" is not itself a "Container", so you may not
     define subcomponents such as "Valves" at this level.
     Documentation at /docs/config/server.html
 -->
<Server port="8005" shutdown="SHUTDOWN">
  <Listener className="org.apache.catalina.startup.VersionLoggerListener" />
  <!-- Security listener. Documentation at /docs/config/listeners.html
  <Listener className="org.apache.catalina.security.SecurityListener" />
  -->
  <!--APR library loader. Documentation at /docs/apr.html -->
  <Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" />
  <!-- Prevent memory leaks due to use of particular java/javax APIs-->
  <Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" />
  <Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" />
  <Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" />

  <!-- Global JNDI resources
       Documentation at /docs/jndi-resources-howto.html
  -->
  <GlobalNamingResources>
    <!-- Editable user database that can also be used by
         UserDatabaseRealm to authenticate users
    -->
    <Resource name="UserDatabase" auth="Container"
              type="org.apache.catalina.UserDatabase"
              description="User database that can be updated and saved"
              factory="org.apache.catalina.users.MemoryUserDatabaseFactory"
              pathname="conf/tomcat-users.xml" />
  </GlobalNamingResources>

  <!-- A "Service" is a collection of one or more "Connectors" that share
       a single "Container" Note:  A "Service" is not itself a "Container",
       so you may not define subcomponents such as "Valves" at this level.
       Documentation at /docs/config/service.html
   -->
  <Service name="Catalina">

    <!--The connectors can use a shared executor, you can define one or more named thread pools-->
    <!--
    <Executor name="tomcatThreadPool" namePrefix="catalina-exec-"
        maxThreads="150" minSpareThreads="4"/>
    -->


    <!-- A "Connector" represents an endpoint by which requests are received
         and responses are returned. Documentation at :
         Java HTTP Connector: /docs/config/http.html (blocking & non-blocking)
         Java AJP  Connector: /docs/config/ajp.html
         APR (HTTP/AJP) Connector: /docs/apr.html
         Define a non-SSL/TLS HTTP/1.1 Connector on port 8080
    -->
    <Connector port="8080" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
    <!-- A "Connector" using the shared thread pool-->
    <!--
    <Connector executor="tomcatThreadPool"
               port="8080" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
    -->
    <!-- Define a SSL/TLS HTTP/1.1 Connector on port 8443
         This connector uses the NIO implementation that requires the JSSE
         style configuration. When using the APR/native implementation, the
         OpenSSL style configuration is required as described in the APR/native
         documentation -->
    <!--
    <Connector port="8443" protocol="org.apache.coyote.http11.Http11NioProtocol"
               maxThreads="150" SSLEnabled="true" scheme="https" secure="true"
               clientAuth="false" sslProtocol="TLS" />
    -->

    <!-- Define an AJP 1.3 Connector on port 8009 -->
    <Connector port="8009" protocol="AJP/1.3" redirectPort="8443" />


    <!-- An Engine represents the entry point (within Catalina) that processes
         every request.  The Engine implementation for Tomcat stand alone
         analyzes the HTTP headers included with the request, and passes them
         on to the appropriate Host (virtual host).
         Documentation at /docs/config/engine.html -->

    <!-- You should set jvmRoute to support load-balancing via AJP ie :
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1">
    -->
    <Engine name="Catalina" defaultHost="localhost">

      <!--For clustering, please take a look at documentation at:
          /docs/cluster-howto.html  (simple how to)
          /docs/config/cluster.html (reference documentation) -->
      <!--
      <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster"/>
      -->

      <!-- Use the LockOutRealm to prevent attempts to guess user passwords
           via a brute-force attack -->
      <Realm className="org.apache.catalina.realm.LockOutRealm">
        <!-- This Realm uses the UserDatabase configured in the global JNDI
             resources under the key "UserDatabase".  Any edits
             that are performed against this UserDatabase are immediately
             available for use by the Realm.  -->
        <Realm className="org.apache.catalina.realm.UserDatabaseRealm"
               resourceName="UserDatabase"/>
      </Realm>

      <Host name="localhost"  appBase="/data/tomcat/webapps"  unpackWARs="false" autoDeploy="false">

        <!-- SingleSignOn valve, share authentication between web applications
             Documentation at: /docs/config/valve.html -->
        <!--
        <Valve className="org.apache.catalina.authenticator.SingleSignOn" />
        -->

        <!-- Access log processes all example.
             Documentation at: /docs/config/valve.html
             Note: The pattern used is equivalent to using pattern="common" -->
        <Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
               prefix="localhost_access_log" suffix=".txt"
               pattern="%h %l %u %t &quot;%r&quot; %s %b" />

      </Host>
    </Engine>
  </Service>
</Server>


# cat build-command.sh 
#!/bin/bash
TAG=$1
docker build -t  harbor.magedu.net/base/tomcat-app1:${TAG} .
sleep 3
docker push  harbor.magedu.net/base/tomcat-app1:${TAG}

# cat index.html 
<h1>magedu n56 app1 v1111</h1>
<h1>magedu n56 app1 v2222</h1>
#chmod 755 run_tomcat.sh
# bash build-command.sh v2
```

#### 4.3.1.5: k8s创建资源:

```bash
# cat tomcat-app1.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: magedu-tomcat-app1-deployment-label
  name: magedu-tomcat-app1-deployment
  namespace: magedu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: magedu-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: magedu-tomcat-app1-selector
    spec:
      containers:
      - name: magedu-tomcat-app1-container
        image: harbor.magedu.net/base/tomcat-app1:v2
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: magedu-tomcat-app1-service-label
  name: magedu-tomcat-app1-service
  namespace: magedu
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30092
  selector:
    app: magedu-tomcat-app1-selector

```



#### 4.3.1.6: filebeat收集镜像制作：

```bash
# cat filebeat.yml 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /apps/tomcat/logs/catalina.out
  fields:
    type: tomcat-catalina
- type: log
  enabled: true
  paths:
    - /apps/tomcat/logs/localhost_access_log.*.txt 
  fields:
    type: tomcat-accesslog
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:

#output.kafka:
#  hosts: ["172.31.4.101:9092"]
#  required_acks: 1
#  topic: "magedu-n56-app1"
#  compression: gzip
#  max_message_bytes: 1000000
output.redis:
  hosts: ["192.168.48.165:6379"]
  key: "k8s-magedu-app1"
  db: 1
  timeout: 5
  password: "123456"

# cat run_tomcat.sh 
#!/bin/bash
#echo "nameserver 223.6.6.6" > /etc/resolv.conf
#echo "192.168.7.248 k8s-vip.example.com" >> /etc/hosts

/usr/share/filebeat/bin/filebeat -e -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat &
su - nginx -c "/apps/tomcat/bin/catalina.sh start"
tail -f /etc/hosts

# cat Dockerfile 
#tomcat web1
FROM harbor.magedu.net/base/tomcat-base:v8.5.43 

ADD catalina.sh /apps/tomcat/bin/catalina.sh
ADD server.xml /apps/tomcat/conf/server.xml
ADD myapp/* /data/tomcat/webapps/myapp/
ADD app1.tar.gz /data/tomcat/webapps/myapp/
ADD run_tomcat.sh /apps/tomcat/bin/run_tomcat.sh
ADD filebeat.yml /etc/filebeat/filebeat.yml 
RUN chown  -R nginx.nginx /data/ /apps/
#ADD filebeat-7.5.1-x86_64.rpm /tmp/
#RUN cd /tmp && yum localinstall -y filebeat-7.5.1-amd64.deb

EXPOSE 8080 8443

CMD ["/apps/tomcat/bin/run_tomcat.sh"]

# bash build-command.sh v3

```



#### 4.3.1.7: redis安装：

```bash
192.168.48.165
#apt install redis
#vim /etc/redis/redis.conf
bind 0.0.0.0
port 6379
requirepass 123456
#systemctl start redis
```

#### 4.3.1.8: deployment部署-收集日志

```bash
cat tomcat-app1.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: magedu-tomcat-app1-deployment-label
  name: magedu-tomcat-app1-deployment
  namespace: magedu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: magedu-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: magedu-tomcat-app1-selector
    spec:
      containers:
      - name: magedu-tomcat-app1-container
        image: harbor.magedu.net/base/tomcat-app1:v3
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: magedu-tomcat-app1-service-label
  name: magedu-tomcat-app1-service
  namespace: magedu
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30092
  selector:
    app: magedu-tomcat-app1-selector

```

#### 4.3.1.9: redis验证：

```bash
root@k8s-node2:~# redis-cli 
127.0.0.1:6379> auth 123456
OK
127.0.0.1:6379> select 1
OK
127.0.0.1:6379[1]> KEYS *
1) "k8s-magedu-app1"
127.0.0.1:6379[1]> type k8s-magedu-app1
list
127.0.0.1:6379[1]> LLEN k8s-magedu-app1
(integer) 61
127.0.0.1:6379[1]> lrange k8s-magedu-app1 0 -1

```

#### 4.3.2.0: logstash安装：

```bash
#wget https://artifacts.elastic.co/downloads/logstash/logstash-7.16.0-amd64.deb
# dpkg -i logstash-7.16.0-amd64.deb
# pwd
/etc/logstash/conf.d

# cat redis-to-es.conf 
input {
	redis {
	key => "k8s-magedu-app1"
        host => "192.168.48.165"
        password => "123456"
        port => 6379
        db => 1
        data_type => "list"
	}
}

output {
	if [fields][type] == "tomcat-accesslog" {
        elasticsearch {
		hosts => ["http://192.168.48.164:9200","http://127.0.0.1:9200"]
  		index => "magedu-app1-accesslog-%{+YYYY.MM.dd}"
	}
	
	}

	if [fields][type] == "tomcat-catalina" {
	elasticsearch {
		hosts => ["http://192.168.48.164:9200","http://127.0.0.1:9200"]
		index => "magedu-app1-catalinalog-%{+YYYY.MM.dd}"
	}
	}
}
#systemctl start logstash
```



#### 4.3.2.1: es验证：

![image-20230202154305876](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230202154305876.png)



# 五： k8s运维示例：

## 5.1：手动调整pod数量：

kubectl scale 对运行在k8s环境中的pod数量进行扩容（增加）或缩容（减少）

```bash
当前pod数量
root@s3:~# kubectl get deployment -n test
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat-app1   1/1     1            1           33d
tomcat-app2   1/1     1            1           33d

#查看命令使用帮助
root@s3:~# kubectl --help | grep scale
  scale         Set a new size for a deployment, replica set, or replication controller
  autoscale     Auto-scale a deployment, replica set, stateful set, or replication controller
root@s3:~# kubectl scale --help
#执行扩容/缩容
root@s3:~# kubectl scale deployment/tomcat-app1 --replicas=3 -n test
deployment.apps/tomcat-app1 scaled

#验证手动扩容结果
root@s3:~# kubectl get deployment -n test
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat-app1   3/3     1            1           33d
tomcat-app2   1/1     1            1           33d
```



## 5.2：HPA自动伸缩pod数量：

kubectl autoscale自动控制在k8s集群中运行的pod数量（水平自动伸缩），需要提前设置pod范围及触发条件

k8s从1.1版本开始增加了名称HPA(Horizontal Pod Wutoscaler)的控制器，用于实现基于pod中资源（CPU/Memory）利用率进行对pod的自动扩缩容功能的实现，早期的版本只能基于heapster组件实现对CPU利用率做为触发条件，但是在k8s 1.11版本开始使用Metrices Server完成数据采集，然后将采集到的数据通过API（Aggregated API,汇总API），例如metrice.k8s.io、custom.metrics.k8s.io、external.metrics.k8s.io,然后再把数据提供给HPA控制器进行查询，以实现基于某个资源利用率对pod进行扩缩容的目的

```bash
控制器管理默认每隔15s（可以通过-horizontal-pod-autoscaler-sync-period修改）查询metrics的资源使用情况
支持一下三种metrics指标类型：
  -> 预定义metrics(比如pod的CPU)以利用率的方式计算
  -> 自定义的pod metrics，以原始值（raw value）的方式计算
  -> 自定义的object metrics
支持两种metrics查询方式:
  -> Heapster
  -> 自定义的REST API
支持多metrics
```



![image-20230202165356654](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230202165356654.png)



https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/

```
期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)]

例如：当前指标为200m 目标设定值为100m，那么由于200.0/100.0 == 2.0 副本数量将会翻倍，如果当前指标为50m、副本数量将会减半，因为50.0/100.0 == 0.5 ,如果计算出的扩缩比例接近1.0（根据--horizontal-pod-autoscaler-tolerance参数全局配置的容忍值，默认为0.1）,将会放弃本次扩缩
```



![image-20230202171529633](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230202171529633.png)



### 5.2.1：准备metrics-server

使用metrics-server作为HPA数据源

https://github.com/kubernetes-sigs/metrics-server

#### 5.2.1.1: 创建metrics-server服务：

```bash
# cat components-v0.4.4.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: system:aggregated-metrics-reader
rules:
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    k8s-app: metrics-server
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        image: harbor.magedu.net/base/metrics-server:v0.4.4
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          periodSeconds: 10
        name: metrics-server
        ports:
        - containerPort: 4443
          name: https
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          periodSeconds: 10
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      volumes:
      - emptyDir: {}
        name: tmp-dir
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  labels:
    k8s-app: metrics-server
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
  version: v1beta1
  versionPriority: 100
# kubectl apply -f components-v0.4.4.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created

```

#### 5.2.1.2：验证metrics-server pod:



![image-20230202175135931](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230202175135931.png)

验证metrics-server是否采集到node数据：

```bash
root@s3:/apps/metrics# kubectl top node
NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
192.168.48.163   164m         16%    969Mi           76%       
192.168.48.166   276m         27%    1291Mi          66%       
192.168.48.169   303m         30%    882Mi           53%       
192.168.48.170   267m         26%    1031Mi          62% 
```

验证metrics-server是否采集到pod数据：

```bash
root@s3:/apps/metrics# kubectl top pod
NAME     CPU(cores)   MEMORY(bytes)   
tomcat   0m           2Mi 
```

#### 5.2.1.3: 修改controller-manager启动参数:

```bash
root@s3:/apps/metrics# kube-controller-manager --help | grep horizontal-pod-autoscaler-sync-period
      --horizontal-pod-autoscaler-sync-period duration                 The period for syncing the number of pods in horizontal pod autoscaler. (default 15s)
#定义Pod数量水平伸缩的间隔周期,默认15秒

root@s3:/apps/metrics# kube-controller-manager --help | grep horizontal-pod-autoscaler-cpu-initialization
-period
      --horizontal-pod-autoscaler-cpu-initialization-period duration   The period after pod start when CPU samples might be skipped. (default 5m0s)
      #用于设置pod的初始话时间，在此时间内的pod,cpu资源指标将不会被采纳，默认为5分钟
      root@s3:/apps/metrics# kube-controller-manager --help | grep horizontal-pod-autoscaler-initial-readiness-delay
      --horizontal-pod-autoscaler-initial-readiness-delay duration     The period after pod start during which readiness changes will be treated as initial readiness. (default 30s)
      #用于设置pod准备时间，再次时间内的pod统统被认为未就绪及不采集数据，默认为30秒
```

#### 5.2.1.4：通过命令配置扩缩容：

```bash
root@s3:/apps/metrics# kubectl autoscale deployment magedu-tomcat-app1-deployment --min=2 --max=10 --cpu-percent=80 -n magedu
horizontalpodautoscaler.autoscaling/magedu-tomcat-app1-deployment autoscaled

验证信息：
root@s3:/apps/metrics# kubectl describe deployment magedu-tomcat-app1-deployment -n magedu
desired 最终期望处于READY状态的副本数
updated 当前完成更新的副本数
total 总计副本数
available 当前可用的副本数
unavailable 不可以用的副本数
```



![image-20230202180537638](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230202180537638.png)



### 5.2.2：yaml文件中定义扩缩容配置：

定义在tomcat服务中的yaml中

```bash
# cat tomcat-app1.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: magedu-tomcat-app1-deployment-label
  name: magedu-tomcat-app1-deployment
  namespace: magedu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: magedu-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: magedu-tomcat-app1-selector
    spec:
      containers:
      - name: magedu-tomcat-app1-container
        image: harbor.magedu.net/base/tomcat-app1:v3
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
        resources:
          limits:
            cpu: 500m
            memory: "512Mi"
          requests:
            cpu: 200m
            memory: "250Mi"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: magedu-tomcat-app1-service-label
  name: magedu-tomcat-app1-service
  namespace: magedu
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30092
  selector:
    app: magedu-tomcat-app1-selector


#定义HPA控制器
# cat hpa.yaml 
apiVersion: autoscaling/v1  #定义API版本
kind: HorizontalPodAutoscaler #对象类型
metadata: #定义对象元数据
  namespace: magedu #创建后略属于的namespace
  name: magedu-tomcat-app1-podautoscaler #对象名称
  labels: #标签
    app: magedu-tomcat-app1 #自定义的label名称
spec: #定义对象具体信息
  scaleTargetRef: #定义水平伸缩的目标对象,deployment,replicationcontroller/replicaset
    apiVersion: apps/v1 #目标API版本
    kind: Deployment #目标对象类型为deployment
    name: magedu-tomcat-app1-deployment #deployment具体名称
  minReplicas: 2 #最小pod数
  maxReplicas: 5 #最大pod数
  targetCPUUtilizationPercentage: 30 #cpu使用率30
#  metrics: #调用metrics数据定义
#  - type: Resource #类型资源
#    resource: #定义资源
#      name: cpu #资源名称为cpu
#      targetAverageUtilization: 30 #cpu使用率
#  - type: Resource #类型为资源
#    resource: #定义资源
#      name: memory #资源名称为memory
#      targetAverageValue: 500Mi #memory使用率


```



#### 5.2.2.1: 验证HPA日志：

空闲一段时间，验证是否会对容器扩缩容



```bash
root@s3:~# kubectl get po -n magedu
NAME                                             READY   STATUS    RESTARTS   AGE
magedu-tomcat-app1-deployment-656b5d4984-kgwpk   1/1     Running   0          15h
magedu-tomcat-app1-deployment-656b5d4984-tdtk9   1/1     Running   0          15h
root@s3:~# kubectl describe horizontalpodautoscalers.autoscaling magedu-tomcat-app1-podautoscaler -n magedu

Events:
  Type     Reason             Age   From                       Message
  ----     ------             ----  ----                       -------
  Normal   SuccessfulRescale  15h   horizontal-pod-autoscaler  New size: 2; reason: All metrics below target
  Warning  FailedGetScale     10m   horizontal-pod-autoscaler  Unauthorized

```





![image-20230203100104582](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230203100104582.png)



## 5.3: 业务镜像版本升级及回滚：

在指定的deployment中通过kubectl set image 指定新版本的镜像:tag来实现更新代码的目的。

构建三个不同版本的nginx镜像，第一次使用v1版本，后组逐渐升级到v2与v3，测试镜像版本升级与回滚操作

```bash
deployment控制器支持两种更新策略：默认为滚动更新
1.滚动更新（rolling update）:
滚动更新事默认的更新策略，滚动更新是基于新版本镜像创建新版本pod，然后删除一部分旧版本pod，然后在创建新版本pod，在删除一部分旧版本pod，直到旧版本Pod删除完成，滚动更新优势实是在升级过程当中不会导致服务不可用，缺点是升级过程中会导致两个版本在短时间内会并存
具体升级过程是在执行更新操作后k8s会创建一个新版本的ReplicaSet控制器，在删除旧版本的Replicaset控制器下的POD的同时会在新版本的ReplicaSet控制器下创建信的pod，直到旧版本的pod全部删除后在把旧版本的ReplicaSet控制器也回收掉

在执行滚动更新的同时，为了保证服务的可用性，当前控制器内不可用的pod(pod需要拉取镜像执行创建和执行探针探测期间是不可用的)不能超出一定范围，因为需要至少保留一定数量的pod以保证服务可以被客户端正常访问，可以通过以下参数指定：#kubectl explain deployment.spec.strategy
 deployment.spec.strategy.rollingUpdate.maxSurge #指定在升级期间pod总数可以超出定义好的期望的pod数的个数或者百分比，默认为25%，如果设置为10%，如果当前10个pod，那么升级时最多将创建1个pod及额外还有10%的pod临时会超出当前（replicas）指定的副本数限制
 deployment.spec.strategy.rollingUpdate.maxUnavailable #指定在升级期间最大不可用的pod数，可以是整数或者当前pod的百分比，默认是25%,加入当前是100个pod，那么升级时最多可以有25个（25%）pod不可以用既还要75个（75%）pod是可以用的
 #注意：以上俩个值不能同时为0，如果maxUnavailable最大不可用pod为0，maxSurge超出pod数也为0，那么将会导致pod无法进行滚动更新
 
 2.重建更新（recreate）
 先删除现有的pod，然后基于新版本的镜像重建，优势是同时有一个版本在线，不会产生多版本在线问题，缺点是pod删除后到pod重建成功中间的时间会导致服务无法访问，因此较少使用
```



### 5.3.1：升级到镜像到指定版本：

```bash
#镜像更新命令格式为：
# kubectl set image deploy myapp-app1 http=tomcat:8.5.2


```



### 5.3.2：查看历史版本信息：

```bash
root@s3:/apps/nginx# kubectl rollout history deployment myapp-app1
deployment.apps/myapp-app1 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>

```



### 5.3.3: 回滚到上一个版本：

```bash
# kubectl rollout undo deployment myapp-app1 
deployment.apps/myapp-app1 rolled back
```



### 5.3.4: 回滚到指定版本：

```bash
# kubectl rollout undo deployment myapp-app1 --to-revision=1


回滚和的版本号：
# kubectl rollout history deployment myapp-app1 
deployment.apps/myapp-app1 
REVISION  CHANGE-CAUSE
3         <none>
4         <none>
5         <none>
```

### 5.3.5: 配置主机为封锁状态且不参与调度：

```bash
# kubectl --help | grep cordon
  cordon        Mark node as unschedulable #标记为警戒，既不参加pod调度
  uncordon      Mark node as schedulable #去掉警戒，既参加pod调度
  
  #设置192.168.48.169不参加调度
  # kubectl cordon 192.168.48.169
node/192.168.48.169 cordoned
root@s3:/apps/nginx# kubectl get node
NAME             STATUS                     ROLES    AGE    VERSION
192.168.48.163   Ready,SchedulingDisabled   master   74d    v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   248d   v1.22.2
192.168.48.169   Ready,SchedulingDisabled   node     248d   v1.22.2
192.168.48.170   Ready                      node     248d   v1.22.2

#设置192.168.48.169参加调度
# kubectl uncordon 192.168.48.169
node/192.168.48.169 uncordoned
root@s3:/apps/nginx# kubectl get node
NAME             STATUS                     ROLES    AGE    VERSION
192.168.48.163   Ready,SchedulingDisabled   master   74d    v1.22.2
192.168.48.166   Ready,SchedulingDisabled   master   248d   v1.22.2
192.168.48.169   Ready                      node     248d   v1.22.2
192.168.48.170   Ready                      node     248d   v1.22.2

```





# 六： 持续集成与部署：



![image-20230203140259620](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230203140259620.png)



## 6.1：jenkins环境准备：



```bash
#apt install openjdk-11-jdk daemon
#wget https://mirrors.tuna.tsinghua.edu.cn/jenkins/debian-stable/jenkins_2.319.3_all.deb
#dpkg -i jenkins_2.319.3_all.deb
```

## 6.2：gitlab环境准备：

```bash
#docker pull gitlab/gitlab-ce:latest
#docker run -id -p 9980:80 -p 9922:22 -v /home/gitlab/etc:/etc/gitlab  -v /home/gitlab/log:/var/log/gitlab -v /home/gitlab/opt:/var/opt/gitlab --restart always --privileged=true --name gitlab gitlab/gitlab-ce:latest
## docker exec -it gitlab /bin/bash
# 修改gitlab.rb
vi /etc/gitlab/gitlab.rb
## 加入如下
# gitlab访问地址，可以写域名。如果端口不写的话默认为80端口
external_url 'http://192.168.48.164'
# ssh主机ip
gitlab_rails['gitlab_ssh_host'] = '192.168.48.164'
# ssh连接端口
gitlab_rails['gitlab_shell_ssh_port'] = 9922


# 让配置生效
gitlab-ctl reconfigure

# 修改http和ssh配置
vi /opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml
  gitlab:
    ## Web server settings (note: host is the FQDN, do not include http://)
    host: 192.168.48.164
    port: 9980
    https: false

# 重启
gitlab-ctl restart
# 退出容器
exit


# 机器配置要大于4g，否则很容易启动不了，报502
http://ip:3000/

#修改root密码
# 进入容器内部
docker exec -it gitlab /bin/bash

# 进入控制台
gitlab-rails console -e production

# 查询id为1的用户，id为1的用户是超级管理员
user = User.where(id:1).first
# 修改密码为123@321Qaz
user.password='123@321Qaz'
# 保存
user.save!
# 退出
exit
```

